# SPSA Curve Reconstruction Loss

## Overview

Implemented efficient differentiable wrapper for XRD curve generation using **SPSA (Simultaneous Perturbation Stochastic Approximation)** gradient estimation.

### Key Innovation: 2 simulations per sample (vs 14 for naive FD)

**Problem:** We want gradient âˆ‚L/âˆ‚Î¸ where L is curve loss, but full XRD simulator is not differentiable.

**Solution:** SPSA estimates J^T v using only 2 simulations:
- J = âˆ‚y/âˆ‚Î¸ (Jacobian: curve from params)
- v = âˆ‚L/âˆ‚y (incoming gradient from PyTorch)
- Trick: Ï†(Î¸) = v^T y(Î¸)  â†’  âˆ‡Ï† = J^T v

## How SPSA Works

### Algorithm

1. **Random perturbation:** Î” ~ Rademacher({-1, +1}^7)
2. **Two evaluations:**
   - Ï†â‚Š = v^T y(Î¸ + cÂ·Î”)
   - Ï†â‚‹ = v^T y(Î¸ - cÂ·Î”)
3. **Gradient estimate:** âˆ‡Ï† â‰ˆ [(Ï†â‚Š - Ï†â‚‹)/(2c)] Â· Î”

### Performance Comparison

| Method           | Simulations per Sample | For Batch=32 | Speedup |
|------------------|------------------------|--------------|---------|
| Naive FD         | 2P = 14 (P=7 params)   | 448 sims     | 1Ã—      |
| **SPSA**         | **2 (constant!)**      | **64 sims**  | **7Ã—**  |

## Usage

### 1. Curve Reconstruction Loss (Validation)

```python
from xrd_pytorch import CurveReconstructionLoss

# Create loss function
criterion = CurveReconstructionLoss(
    c_rel=1e-3,           # SPSA step size
    crop_start=40,        # Must match dataset!
    crop_end=701,         # Must match dataset!
    reduction='mean'
)

# During validation (no gradients needed)
with torch.no_grad():
    loss = criterion(curve_input, params_pred)

# During training (with SPSA gradients)
loss = criterion(curve_input, params_pred)  # 2 sims per sample
loss.backward()  # Gradients via SPSA
```

### 2. Hybrid Loss (Params + Curve)

```python
from xrd_pytorch import HybridLoss

# Create hybrid loss
criterion = HybridLoss(
    alpha=1.0,    # Weight for parameter loss (fast)
    beta=0.1,     # Weight for curve loss (2 sims per sample)
    c_rel=1e-3    # SPSA step size
)

# Training step
loss_total, loss_params, loss_curve = criterion(
    curve_input,    # [B, 1, L]
    params_true,    # [B, 7]
    params_pred     # [B, 7]
)

loss_total.backward()  # Gradients for both losses
optimizer.step()
```

### 3. Direct SPSA Curve Generation

```python
from xrd_pytorch import xrd_curve_spsa

# Generate curves with SPSA gradients
params_pred = model(curve_input)  # [B, 7]
curves_reconstructed = xrd_curve_spsa(
    params_pred,
    c_rel=1e-3,
    seed=42  # For reproducibility
)

# curves_reconstructed has gradients wrt params_pred!
```

## Recommended Training Strategy

### Option 1: Validation Metric Only (RECOMMENDED)

```python
# Training: Fast parameter loss
loss_params = MSE(params_pred, params_true)
loss_params.backward()

# Validation: Curve reconstruction quality
with torch.no_grad():
    curve_pred = xrd_curve_spsa(params_pred)
    curve_error = MAE(curve_input, curve_pred)

# Early stopping based on curve_error!
```

**Why this is best:**
- âœ… Fast training (no simulations in backward pass)
- âœ… Validation optimizes for what we care about (curve quality)
- âœ… Early stopping prevents "good params, bad curve" problem

### Option 2: Hybrid Loss (EXPERIMENTAL)

```python
# Combined loss with small Î²
criterion = HybridLoss(alpha=1.0, beta=0.1)
loss = criterion(curve_input, params_true, params_pred)
loss.backward()
```

**When to use:**
- When parameter loss alone gives "good params, bad curve"
- When you have computational budget (2 sims per sample)
- Start with Î²=0.1, increase to Î²=0.5 if needed

### Option 3: Curriculum Learning

```python
# Epoch 0-50: Only parameter loss (fast warmup)
if epoch < 50:
    beta = 0.0
# Epoch 50-100: Gradually add curve loss
elif epoch < 100:
    beta = (epoch - 50) / 50 * 0.1  # 0 â†’ 0.1
# Epoch 100+: Full hybrid loss
else:
    beta = 0.1

criterion = HybridLoss(alpha=1.0, beta=beta)
```

## Hyperparameter Tuning

### c_rel (SPSA step size)

**Default: 1e-3** (0.1% of [0,1] range)

- **Too small (1e-5):** Noisy gradients, slow convergence
- **Too large (1e-2):** Biased gradients, parameter clipping artifacts
- **Optimal range:** 1e-4 to 5e-3

### alpha / beta (loss weights)

**Recommended starting points:**

| Strategy              | alpha | beta | Speed         | Quality |
|-----------------------|-------|------|---------------|---------|
| **Validation only**   | 1.0   | 0.0  | âš¡ Fastest    | Good    |
| **Gentle hybrid**     | 1.0   | 0.1  | Fast          | Better  |
| **Balanced**          | 1.0   | 0.5  | Medium        | Best    |
| **Curve-focused**     | 0.2   | 0.8  | ðŸŒ Slow       | Best+   |

### seed (reproducibility)

- Set `seed=epoch` during training for different perturbations each epoch
- Set `seed=42` during validation for reproducibility
- Set `seed=None` for random (fastest, no seed generation overhead)

## CRITICAL: Preprocessing Alignment

**SPSA wrapper MUST use EXACT SAME preprocessing as dataset!**

Current implementation matches `NormalizedXRDDataset`:
1. **Crop:** `R_vseZ[40:701]` (661 points)
2. **Log:** `np.log10(curve + 1e-12)`
3. **Normalize:** `(log_curve - min) / (max - min)`

If you change dataset preprocessing, update `simulate_curve_normalized()` in `xrd_pytorch.py`!

## Performance Tips

### 1. Batch Size vs Curve Loss

For batch=32:
- **Params loss only:** 0 sims per step â†’ ~100 steps/sec
- **Hybrid (Î²=0.1):** 64 sims per step â†’ ~2 steps/sec

**Recommendation:** Use smaller batch for hybrid training (batch=8-16)

### 2. Selective Application

```python
# Apply curve loss only to 50% of batches
if np.random.rand() < 0.5:
    beta = 0.1
else:
    beta = 0.0

# Or: apply only to difficult samples (high param error)
param_errors = torch.abs(params_pred - params_true).mean(dim=1)
beta_per_sample = (param_errors > threshold).float() * 0.1
```

### 3. Caching (TODO)

Future optimization: Cache simulator objects to avoid re-initialization:

```python
# Current: Creates new simulator for each curve
simulator = HRXRDSimulator(crystal, film, geometry)

# Future: Reuse simulator
_simulator_cache = HRXRDSimulator(...)  # Global cache
```

**Expected speedup:** 20-30% faster simulation

## Test Results

All tests passing (see `xrd_pytorch.py`):

```
âœ“ Forward pass OK
  - Input: [2, 7] params
  - Output: [2, 661] curves
  - Range: [0.0, 1.0]

âœ“ Backward pass OK
  - Gradients: [2, 7]
  - All 14 gradients nonzero
  - Range: [-2.46, +2.46]

âœ“ Curve reconstruction loss OK
  - Loss: 0.3107
  - Gradients computed via SPSA

âœ“ Hybrid loss OK
  - Param loss: 0.1830
  - Curve loss: 0.3189
  - Total: 0.2148
```

## Next Steps

1. **Quick test (1000 samples):**
   - Train baseline (params only)
   - Train hybrid (alpha=1.0, beta=0.1)
   - Compare validation curve error

2. **If hybrid wins:** Train on 100k dataset

3. **If baseline wins:** Use curve loss as validation metric only

## References

- SPSA algorithm: Spall, J.C. (1992). "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation"
- Our implementation: 2 sims per sample, Rademacher perturbations, adaptive step size
