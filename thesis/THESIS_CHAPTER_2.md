# РОЗДІЛ 2. МЕТОДОЛОГІЯ ДОСЛІДЖЕННЯ ТА РОЗРОБКА ПРОГРАМНОЇ МОДЕЛІ

## 2.1. Обґрунтування вибору методів дослідження

### 2.1.1. Вибір методу машинного навчання для аналізу XRD-кривих

Сучасний аналіз рентгенівських кривих дифракційного відбивання традиційно здійснюється шляхом ітеративного підбору параметрів структури, при якому теоретична крива, обчислена в рамках динамічної теорії дифракції з використанням рекурсивного розв'язку диференціальних рівнянь Такагі-Таупіна, порівнюється з експериментальними даними. Такий підхід вимагає значних обчислювальних ресурсів та експертного втручання для визначення початкових наближень параметрів, що робить його малопридатним для високопродуктивного аналізу великих обсягів даних.

В контексті даного дослідження розглядалися три основних класи методів машинного навчання: класичні регресійні моделі (лінійна регресія, підтримуючі векторні машини), повнозв'язані нейронні мережі (Dense Neural Networks, DNN) та згорткові нейронні мережі (Convolutional Neural Networks, CNN). Аналіз специфіки вхідних даних, а саме одновимірних часових послідовностей інтенсивності дифракційного відбивання, що містять періодичні структури (інтерференційні осцилляції) з варіабельною позицією та частотою, вказував на принципову перевагу архітектур, здатних виявляти локальні закономірності та інваріантні до зсуву.

Класичні регресійні методи демонструють обмежену здатність до моделювання складних нелінійних залежностей між формою XRD-кривої та параметрами деформаційного профілю, оскільки вимагають ручного конструювання ознак (feature engineering), що є нетривіальним завданням для фізичних систем з дефектами. Повнозв'язані нейронні мережі, хоча і здатні апроксимувати довільні функції відповідно до теореми про універсальну апроксимацію, виявляють високу чутливість до абсолютних позицій елементів у вхідному векторі. Це створює фундаментальну проблему при роботі з експериментальними даними, де діапазон ненульової інтенсивності залежить від рівня деформації та може варіюватися, що призводить до необхідності заповнення відсутніх значень константами або нулями. Як показано в роботі Ziegler et al. (2023), введення таких штучних значень перешкоджає коректному навчанню DNN.

Згорткові нейронні мережі позбавлені цього недоліку завдяки властивості інваріантності до зсуву (translation invariance), що досягається через механізм локальних рецептивних полів та спільного використання ваг у згорткових фільтрах. Це дозволяє CNN виявляти характерні паттерни незалежно від їх абсолютної позиції у вхідній послідовності, що є критичним для аналізу кривих дифракційного відбивання, де інформативні інтерференційні осцилляції можуть розташовуватися в різних кутових діапазонах залежно від структурних параметрів зразка. Додатковою перевагою CNN є можливість побудови ієрархічних уявлень даних, де нижні шари виявляють локальні особливості (окремі максимуми та мінімуми інтерференційної картини), а вищі шари інтегрують цю інформацію для визначення глобальних характеристик профілю деформації.

Емпіричне підтвердження переваги CNN для задач аналізу рентгенівських даних надано в роботах Ziegler et al. (2023) для іонно-опромінених кристалів Y₂O₃-легованого ZrO₂, де автори продемонстрували успішне визначення профілів деформації та фактора Дебая-Валлера з точністю, достатньою для практичного застосування. Однак архітектура, запропонована Ziegler, базувалася на класичній послідовності згорткових шарів з операціями максимального згортання (max pooling) та понижуючої дискретизації (downsampling), що призводить до втрати просторової роздільності сигналу. Для параметрів, що кодують позиційну інформацію (таких як Rp1 та Rp2 у нашому дослідженні), це створює фундаментальне обмеження точності.

Ця теза підтверджується результатами нашого базового експерименту, де модель з глобальним усередненням (Global Average Pooling, GAP) демонструвала систематично гірші показники саме для позиційних параметрів. Зокрема, похибка визначення Rp2 становила 19.7%, що значно перевищувало похибки для амплітудних параметрів (Dmax1: 3.5%, D01: 5.1%). Це дало підстави для розробки модифікованої архітектури з механізмом уваги (attention mechanism), який дозволяє моделі вибірково фокусуватися на інформативних ділянках кривої, зберігаючи при цьому просторову інформацію до етапу прийняття рішення.

### 2.1.2. Вибір фізичної моделі для генерації синтетичних даних

Недоступність великих обсягів експериментальних даних (порядку 10⁶ зразків, необхідних для навчання глибоких нейронних мереж) зумовила необхідність чисельної генерації навчальної вибірки. Вибір фізичної моделі для симуляції кривих дифракційного відбивання здійснювався між кінематичним наближенням та динамічною теорією дифракції. Кінематичне наближення, хоча і є обчислювально простішим, не враховує множинні процеси розсіяння рентгенівського випромінювання в кристалі та ефекти первинної та вторинної екстинкції, що призводить до систематичних відхилень від експериментальних даних, особливо для товстих шарів та високодосконалих кристалів.

Динамічна теорія дифракції в формулюванні Такагі-Таупіна надає строгий опис еволюції хвильового поля в кристалі з градієнтом параметра ґратки та враховує як когерентне розсіяння (що формує інтерференційні осцилляції), так і дифузне розсіяння (пов'язане з випадковими атомними зміщеннями через фактор Дебая-Валлера). Рекурсивний метод розв'язку диференціальних рівнянь Такагі-Таупіна, реалізований в роботі, дозволяє ефективно обчислювати розподіл інтенсивності для багатошарових систем з довільним профілем деформації шляхом розбиття неоднорідного шару на тонкі однорідні підшари товщиною dl.

Параметр dl визначає компроміс між точністю моделювання та обчислювальними витратами. Проведений нами аналіз показав, що значення dl = 100 Å забезпечує достатню точність для відтворення тонкої структури інтерференційних осцилляцій при збереженні прийнятної швидкості обчислень (генерація 1 мільйона кривих займає приблизно 2.5 години на 96 процесорних ядрах при паралелізації обчислень). Для порівняння, Ziegler et al. використовували аналогічну методологію з dl = 100 Å для системи Y₂O₃-ZrO₂, що підтверджує адекватність обраного значення.

### 2.1.3. Обґрунтування параметризації профілю деформації

Профіль деформації в приповерхневому шарі монокристалу, модифікованому іонною імплантацією або іншими процесами, може мати складну форму, що не описується простими аналітичними функціями. У роботі Ziegler et al. застосовувалася параметризація через B-сплайни третього степеня з 10 ваговими коефіцієнтами, що забезпечує гнучкість опису довільних форм профілю при обмеженій кількості параметрів. Однак така параметризація має недолік з точки зору фізичної інтерпретованості: вагові коефіцієнти B-сплайнів не мають прямого фізичного значення і вимагають додаткового етапу реконструкції профілю для аналізу.

В даному дослідженні обрано альтернативний підхід, що базується на суперпозиції двох гаусових функцій: асиметричної гаусіани (що моделює область з максимальною концентрацією дефектів в об'ємі шару) та спадної гаусіани (що описує градієнт дефектності від поверхні вглиб матеріалу). Така параметризація мотивована фізичною природою профілів пошкодження при іонній імплантації, де функція Гауса апроксимує розподіл енергетичних втрат іонів згідно з теорією Ліндхарда-Шарффа-Шіотта (LSS theory). Модель включає сім параметрів з чітким фізичним значенням: максимальну деформацію Dmax1, деформацію на поверхні D01, товщину порушеного шару L1 та положення максимуму деформації Rp1 для асиметричної компоненти, а також D02, L2 та Rp2 для спадної компоненти.

Така параметризація забезпечує пряму інтерпретованість результатів: значення L1 безпосередньо відповідає товщині дефектного шару, Dmax1 характеризує ступінь спотворення кристалічної ґратки в найбільш пошкодженій області, а Rp1 визначає глибину залягання максимуму концентрації дефектів. Це є критичним для практичного застосування методу, оскільки дозволяє фізикам-експериментаторам безпосередньо використовувати отримані значення для корекції технологічних процесів або валідації теоретичних моделей імплантації без необхідності реконструкції профілю з абстрактних математичних коефіцієнтів.

## 2.2. Методика генерації синтетичних навчальних даних

### 2.2.1. Алгоритм системного варіювання структурних параметрів

Генерація репрезентативного навчального набору даних вимагає забезпечення достатнього покриття простору параметрів при збереженні фізичної коректності комбінацій. Діапазони варіювання параметрів обрано на основі типових значень, що спостерігаються в експериментах з іонною імплантацією монокристалів гранатів. Максимальна деформація Dmax1 варіюється в діапазоні від 0.001 до 0.030 (від 0.1% до 3%), що відповідає слабко та сильно пошкодженим матеріалам відповідно. Деформація на поверхні D01 обмежена інтервалом від 0.002 до значення Dmax1, що відображає фізичне обмеження: деформація на поверхні не може перевищувати максимальну деформацію в об'ємі для асиметричного профілю.

Товщина порушеного шару L1 варіюється від 1000 Å до 7000 Å (від 100 нм до 700 нм), що охоплює типовий діапазон проективних пробігів іонів середніх енергій (десятки-сотні кеВ) в гранатових матрицях згідно з розрахунками SRIM (Stopping and Range of Ions in Matter). Положення максимуму деформації Rp1 обмежене інтервалом від 0 до L1, що забезпечує фізичну осмисленість: максимум не може знаходитися глибше за товщину порушеного шару. Аналогічні міркування застосовуються до параметрів спадної компоненти: D02 ∈ [0.002, 0.030], L2 ∈ [1000 Å, 7000 Å], Rp2 ∈ [-6000 Å, 0], де від'ємне значення Rp2 вказує на те, що максимум спадної гаусіани розташований на поверхні або за нею.

Критичним аспектом генерації даних є забезпечення консистентності між параметрами двох компонент профілю. Фізично обґрунтована вимога L2 ≤ L1 гарантує, що товщина області спаду деформації не перевищує загальну товщину порушеного шару. Додатково, сумарна деформація на поверхні D01 + D02 обмежена значенням 0.03 (3%), що відповідає максимальній деформації, при якій кристалічна структура ще зберігає свою цілісність без утворення аморфних фаз. Ці обмеження реалізовані на етапі генерації параметрів та додатково контролюються через функцію втрат з фізичними обмеженнями під час навчання моделі.

### 2.2.2. Проблема систематичної неоднорідності розподілу параметрів

Початковий підхід до генерації навчальних даних базувався на випадковому семплюванні (random sampling) з рівномірних розподілів у визначених діапазонах для кожного параметра незалежно з подальшою фільтрацією комбінацій, що порушують фізичні обмеження. Однак детальний статистичний аналіз згенерованого датасету з 1 мільйона зразків виявив систематичну неоднорідність розподілів окремих параметрів, що квантифікується через тест хі-квадрат на рівномірність.

Для параметра L2, який виявився найбільш критичним з точки зору точності моделі (середня абсолютна похибка 5.86% у базовій версії), значення статистики хі-квадрат становило 26,322, що значно перевищує порогове значення 10,000, обране як критерій прийнятної рівномірності. Коефіцієнт систематичного зміщення (bias ratio), що визначається як відношення максимальної до мінімальної частоти появи унікальних значень, досягав 1.61 для L2 та 100.31 для D01, вказуючи на те, що деякі значення параметрів зустрічалися в датасеті майже в сто разів частіше за інші.

Кореляційний аналіз виявив, що ця неоднорідність не є випадковою, а обумовлена каскадним ефектом фізичних обмежень. Зокрема, обмеження L2 ≤ L1 створює ситуацію, де для малих значень L1 доступний вузький діапазон можливих значень L2, тоді як для великих L1 діапазон L2 значно ширший. При рівномірному семплюванні L1 це призводить до того, що великі значення L2 (близькі до верхньої межі 7000 Å) мають набагато менше можливих комбінацій з L1, порівняно з малими значеннями L2. Внаслідок цього малі L2 перепредставлені в датасеті, що створює систематичне зміщення (bias) у навчальній вибірці.

Аналогічний ефект спостерігався для параметра D01 через обмеження D01 ≤ Dmax1 та D01 + D02 ≤ 0.03. Якщо Dmax1 приймає мале значення (наприклад, 0.005), то D01 може варіюватися лише в вузькому діапазоні [0.002, 0.005], тоді як для великого Dmax1 = 0.030 діапазон D01 значно ширший: [0.002, 0.030]. При рівномірному семплюванні Dmax1 це знову ж таки призводить до нерівномірного розподілу D01, де малі значення зустрічаються частіше. Кореляційний коефіцієнт між Dmax1 та D01 у випадково згенерованому датасеті становив 0.41, підтверджуючи наявність індукованої обмеженнями кореляції.

Критично важливим виявився факт, що параметри з найбільшим систематичним зміщенням (L2 з χ² = 26,322, D01 з χ² = 636,697) також демонстрували найгірші показники точності моделі. Для L2 середня абсолютна похибка становила 5.86%, для D01 – 3.84%, тоді як для параметра Rp1 з найкращою рівномірністю розподілу (χ² = 40,280, bias ratio 2.0) похибка була лише 2.47%. Коефіцієнт кореляції між χ² та похибкою моделі становив 0.72, що вказує на сильний зв'язок між якістю розподілу параметрів у навчальних даних та здатністю моделі точно їх передбачати.

### 2.2.3. Метод стратифікованого семплювання для усунення систематичного зміщення

Для вирішення проблеми систематичної неоднорідності розподілу розроблено метод стратифікованого семплювання (stratified sampling) з групуванням за критичним параметром L2. Метод базується на наступному алгоритмі: спочатку генерується повний простір валідних комбінацій параметрів шляхом дискретизації континуальних діапазонів на регулярну сітку з кроком 500 Å для L2 та Rp2 (що забезпечує 10 унікальних значень для L2 та 13 для Rp2, на відміну від крупнішої сітки 1000 Å у випадковому семплюванні). Для кожного значення L2 створюється окрема група (страта), що містить всі комбінації інших параметрів, сумісні з даним L2 та фізичними обмеженнями.

Аналіз розмірів груп показав їх нерівномірність, обумовлену обмеженням L2 ≤ L1: група з L2 = 500 Å містила 506,350 комбінацій (12.10% від загального простору), тоді як група з L2 = 5000 Å містила лише 266,500 комбінацій (6.37%). Це підтверджує попередній висновок про те, що при випадковому семплюванні малі значення L2 природно перепредставлені. Стратифікований підхід компенсує цю нерівномірність шляхом рівномірного розподілу цільової кількості зразків між групами: для датасету з N зразків кожна з 10 груп L2 отримує N/10 зразків, які випадково вибираються з відповідної групи.

Результати застосування стратифікованого семплювання для генерації датасету з 1 мільйона зразків продемонстрували драматичне покращення рівномірності розподілу L2: значення статистики хі-квадрат знизилося з 26,322 до 0 (ідеальна рівномірність), а коефіцієнт систематичного зміщення – з 1.61 до 1.00 (всі 10 значень L2 представлені рівно 100,000 разів). Для пов'язаного параметра Rp2 також спостерігалося значне покращення: χ² знизилося з 42,863 до 18, bias ratio – з варіативного до 1.02. Додатковим позитивним ефектом стала дрібніша дискретизація сітки параметрів: замість 5 унікальних значень L2 у випадковому датасеті стратифікований підхід забезпечив 10 значень, а для Rp2 – 13 замість 7, що збільшує деталізацію покриття простору параметрів.

Слід зазначити, що для параметрів, не використаних для стратифікації (Dmax1, D01, D02), розподіли залишилися практично незмінними, оскільки ці параметри семплюються випадково всередині кожної страти. Це призвело до парадоксального на перший погляд ефекту для L1: його χ² збільшилося з 449,869 до 548,555. Однак це є очікуваним наслідком фізичного обмеження L2 ≤ L1: при рівномірному розподілі L2 розподіл L1 неминуче стає нерівномірним, оскільки для малих L2 можливі будь-які значення L1 ≥ L2, тоді як для великих L2 доступні лише великі L1. Критично важливим є той факт, що L1 показував відносно добру точність вже в базовій моделі (3.40% похибки), тоді як L2 був найбільш проблемним параметром після Rp2. Тому компроміс у вигляді погіршення розподілу L1 на користь ідеального розподілу L2 є обґрунтованим з точки зору очікуваного покращення загальної продуктивності моделі.

### 2.2.4. Обробка та нормалізація даних

Згенеровані криві дифракційного відбивання характеризуються динамічним діапазоном інтенсивності, що може перевищувати 6 порядків величини: від максимальних значень порядку 10⁻² в піку Брегга до мінімальних значень порядку 10⁻⁸ в хвостовій частині кривої. Пряма подача таких даних до нейронної мережі призводить до чисельної нестабільності та систематичного зміщення навчання в бік оптимізації передбачення високоінтенсивних ділянок на шкоду точності відтворення низькоінтенсивних, але інформативних інтерференційних осцилляцій. Для вирішення цієї проблеми застосовано логарифмічне перетворення інтенсивності Î = log₁₀(I), яке компресує динамічний діапазон до приблизно 6 одиниць та забезпечує рівномірнішу важливість всіх ділянок кривої для функції втрат.

Після логарифмічного перетворення здійснюється нормалізація шляхом ділення на максимальне значення log₁₀(Iₘₐₓ) в кожній кривій окремо, що приводить всі дані до діапазону [−DR, 0], де DR – це динамічний діапазон конкретної кривої. Така нормалізація є критичною для коректної роботи згорткових шарів з активаційними функціями ReLU та пакетною нормалізацією (batch normalization), які очікують вхідні дані з контрольованим розподілом та масштабом. Крім того, це забезпечує інваріантність моделі до абсолютної шкали інтенсивності, яка може варіюватися в експериментальних даних залежно від часу експозиції, потужності рентгенівської трубки та інших експериментальних факторів.

Параметри профілю деформації також піддаються нормалізації для приведення до діапазону [0, 1] згідно з формулою: p̂ᵢ = (pᵢ − pᵢᵐⁱⁿ)/(pᵢᵐᵃˣ − pᵢᵐⁱⁿ), де pᵢ – реальне фізичне значення i-го параметра, pᵢᵐⁱⁿ та pᵢᵐᵃˣ – межі його діапазону варіювання. Така нормалізація забезпечує рівну вагу всіх параметрів у функції втрат незалежно від їх фізичних одиниць та масштабів. Без нормалізації параметри з великими числовими значеннями (наприклад, L1 порядку 10⁻⁴ см) домінували б у втратах над параметрами з малими значеннями (наприклад, Dmax1 порядку 10⁻²), що призводило б до неоптимального навчання. Денормалізація передбачень здійснюється перед обчисленням метрик якості через обернене перетворення: pᵢ = p̂ᵢ · (pᵢᵐᵃˣ − pᵢᵐⁱⁿ) + pᵢᵐⁱⁿ.

### 2.2.5. Усічення кривої та виділення інформативної ділянки

Повна крива дифракційного відбивання, обчислена в діапазоні 700 точок, включає інтенсивний та вузький пік Брегга від ідеального субстрату (позиція приблизно при індексі 10) та розширений сигнал від деформованого шару з інтерференційними осцилляціями. Аналіз інформативності різних ділянок кривої для визначення параметрів деформаційного профілю показав, що основна інформація про досліджувані параметри (Dmax1, D01, L1, Rp1, D02, L2, Rp2) міститься саме в інтерференційних осцилляціях, а не в самому піку Брегга. Пік Брегга кодує переважно параметр ґратки недеформованого субстрату та кут Брегга, які є фіксованими для даної кристалографічної системи та не входять до набору параметрів, що визначаються.

Верифікаційний аналіз позицій піків в 1000 випадково обраних синтетичних кривих показав строго детерміновану позицію максимуму: середнє значення становило 10.0 ± 0.0 (стандартне відхилення нульове), що підтверджує стабільність розрахунків та відсутність варіативності позиції піку від параметрів деформації. Це дозволило застосувати фіксоване усічення (truncation) вхідних даних, видаляючи перші 10 точок (сам пік Брегга) та зберігаючи наступні 640 точок, що містять інтерференційну картину від деформованого шару. Таке усічення має кілька переваг: по-перше, зменшується розмірність вхідних даних з 700 до 640, що знижує обчислювальну складність моделі; по-друге, усувається домінування високоінтенсивного піку у функції втрат, що дозволяє моделі краще фокусуватися на тонкій структурі інтерференційних осцилляцій; по-третє, зменшується вплив можливих експериментальних артефактів у піку Брегга (наприклад, насичення детектора).

Альтернативний підхід, використаний Ziegler et al., полягав у збереженні повної кривої без усічення. Вони аргументували це тим, що ширина піку Брегга (параметр H у їхній моделі) сама по собі містить інформацію про дефектність кристалу та інструментальне розширення. Однак у нашому дослідженні ширина піку та інструментальне розширення не є параметрами, що визначаються (вони вважаються відомими з калібрування дифрактометра), а фокус зосереджено виключно на профілі деформації. Емпіричне порівняння, проведене на тестовій вибірці з 10,000 зразків, показало, що модель з усіченням досягає дещо кращої точності для позиційних параметрів (Rp1, Rp2) порівняно з моделлю на повній кривій, що підтверджує доцільність обраного підходу для нашої специфічної задачі.

## 2.3. Архітектура нейронної мережі

### 2.3.1. Еволюція архітектури: від базової до фізично-інформованої моделі

Розробка ефективної архітектури CNN для аналізу XRD-кривих відбувалася ітераційно через три основні версії, кожна з яких вирішувала специфічні обмеження попередньої. Базова модель (v1) слідувала класичній архітектурі згорткових мереж для одновимірних сигналів: вхідний шар приймав 650 нормалізованих значень інтенсивності, які послідовно обробляли 4 залишкові блоки (residual blocks) з 32 каналами кожен. Залишкові з'єднання (skip connections) були включені для полегшення навчання глибоких мереж шляхом забезпечення прямого шляху градієнта через кілька шарів, що вирішує проблему затухання градієнта (vanishing gradient problem). Кожен залишковий блок містив дві згорткові операції з ядром розміру 7, пакетну нормалізацію та нелінійну активацію ReLU.

Критичним вузьким місцем базової архітектури виявилася операція глобального усереднення (Global Average Pooling, GAP) перед повнозв'язними шарами. GAP обчислює середнє значення по всіх просторових позиціях для кожного каналу, редукуючи тензор розміру [batch_size, 32, 650] до [batch_size, 32]. Така операція є корисною для завдань класифікації, де інваріантність до позиції об'єктів є бажаною властивістю, однак для регресійних задач з позиційними параметрами вона виявляється фундаментально обмежуючою. Зокрема, параметри Rp1 та Rp2, що визначають глибину залягання максимумів деформації, кодуються у вхідній кривій через зміщення фази та позицій локальних екстремумів інтерференційних осцилляцій. GAP повністю знищує цю просторову інформацію, залишаючи лише усереднені характеристики каналів.

Емпіричним підтвердженням цього обмеження стали результати базової моделі на валідаційній вибірці з 20,000 зразків: середня абсолютна похибка (MAE) для параметрів деформації (Dmax1, D01, D02) становила 3-5%, для товщинних параметрів (L1, L2) – 3.4-8.6%, тоді як для позиційних параметрів (Rp1, Rp2) похибка сягала 2.5-19.7%. Особливо критичною була похибка для Rp2 (19.7%), що перевищувала типові результати, опубліковані Ziegler et al. (~18%) на датасеті в 12 разів більшому (1.2 мільйона зразків). Це вказувало на те, що обмеження є архітектурним, а не пов'язаним з недостатністю даних.

Друга версія моделі (v2) була спрямована на вирішення проблеми втрати просторової інформації через заміну GAP на механізм уваги (attention pooling). Механізм уваги дозволяє моделі навчитися вибірково фокусуватися на релевантних ділянках вхідної послідовності замість застосування фіксованої операції усереднення. Технічно це реалізовано через додатковий згортковий шар, що обчислює ваги уваги αᵢ для кожної просторової позиції i, з подальшою нормалізацією через softmax: αᵢ = exp(aᵢ) / Σⱼ exp(aⱼ). Вихід блоку уваги є зваженою сумою ознак: out = Σᵢ αᵢ · xᵢ, де xᵢ – вектор ознак в позиції i. На відміну від GAP, де всі позиції мають рівну вагу 1/L (L – довжина послідовності), механізм уваги може призначати різні ваги різним позиціям залежно від їх інформативності для конкретної задачі.

Додатково в версії v2 було збільшено кількість каналів з 32 до 64, що підвищило репрезентаційну потужність моделі, та кількість залишкових блоків до 6 з введенням розширених згорток (dilated convolutions). Розширені згортки з коефіцієнтами розширення (dilation rates) [1, 2, 4, 8, 16, 32] дозволяють експоненційно збільшувати рецептивне поле без збільшення кількості параметрів або глибини мережі. Рецептивне поле визначає, скільки вхідних точок впливають на одну вихідну активацію; для розширеної згортки з ядром розміру K та коефіцієнтом розширення d ефективний розмір ядра становить K + (K−1)(d−1). Послідовне застосування блоків з зростаючим розширенням дозволило досягти рецептивного поля приблизно 450 точок, що становить 69% від довжини вхідної кривої (650 точок).

Велике рецептивне поле є критичним для параметрів L2 та Rp2, оскільки вони визначають довгохвильові модуляції інтерференційної картини, що можуть простягатися на всю довжину кривої. Короткохвильові осцилляції, пов'язані з товщиною шару, детектуються ранніми блоками з малим розширенням, тоді як пізні блоки з великим розширенням інтегрують інформацію з віддалених ділянок для визначення довгоперіодичних трендів. Результати версії v2 на датасеті з 100,000 зразків продемонстрували значне покращення: похибка для Rp2 знизилася з 19.7% до 12.36% (покращення на 37%), для L2 – з 8.6% до 5.86% (покращення на 32%). Валідаційні втрати зменшилися з 0.02179 до 0.01301 (покращення на 40%), підтверджуючи ефективність архітектурних змін.

### 2.3.2. Впровадження рекомендацій з літератури: версія v3

Детальний аналіз роботи Ziegler et al. (2023) виявив кілька архітектурних рішень, що не були реалізовані в версії v2, зокрема використання більших згорткових ядер (K=15 замість K=7) та прогресивного розширення кількості каналів замість константного значення. Теоретичне обґрунтування більших ядер базується на ширині типових особливостей у XRD-кривих: один період інтерференційних осцилляцій зазвичай займає 10-30 точок залежно від товщини шару, і ядро розміру 15 може охопити повний період осцилляції в одній операції, тоді як ядро розміру 7 вимагає кількох послідовних згорток для аналізу повного періоду.

Прогресивне розширення каналів реалізує принцип ієрархічної побудови уявлень, де ранні шари з меншою кількістю каналів виявляють прості локальні особливості (окремі максимуми та мінімуми), а пізні шари з більшою кількістю каналів інтегрують цю інформацію в складніші паттерни. Конкретна конфігурація версії v3: вхідний згортковий шар проектує 1-канальний вхід у 32-канальне уявлення, після чого 6 залишкових блоків з розширенням каналів [32, 48, 64, 96, 128, 128] послідовно підвищують абстрактність уявлення. Перехід між різними кількостями каналів здійснюється через 1×1 згортки (точкові згортки), що діють як лінійні проекції без зміни просторової роздільності.

Розрахунок рецептивного поля для версії v3 з ядрами K=15 та розширеннями [1, 2, 4, 8, 16, 32] дає приблизно 900 точок, що перевищує довжину вхідної послідовності (640 точок після усічення піку). Це означає, що кожна активація в найглибших шарах мережі потенційно несе інформацію про всю вхідну криву, що є оптимальним для глобальних параметрів, таких як загальна товщина порушеного шару L1 або максимальна деформація Dmax1. Водночас початкові шари з меншим рецептивним полем зберігають чутливість до локальних особливостей, що кодують позиційні параметри Rp1 та Rp2.

Порівняльна характеристика параметрів трьох версій моделі наведена в таблиці нижче. Версія v3 має найбільшу кількість параметрів (приблизно 1.5 мільйона проти 1.2 мільйона у v2), однак це збільшення є помірним та не призводить до проблем перенавчання за наявності достатньо великого датасету (1 мільйон зразків). Очікуваний ефект від переходу до версії v3, базуючись на результатах Ziegler et al. та наших попередніх експериментах, полягає у зниженні похибки для Rp2 з 12.36% до 7-9%, для L2 – з 5.86% до 3.5-4.5%, що наблизить або перевищить показники, досягнуті в літературі на значно більших датасетах.

| Характеристика | Версія v1 (Baseline) | Версія v2 (Physics-informed) | Версія v3 (Ziegler-inspired) |
|----------------|---------------------|------------------------------|------------------------------|
| Розмір ядра | 7 | 7 | 15 |
| Канали | 32 (константа) | 64 (константа) | 32→48→64→96→128→128 |
| Блоки | 4 | 6 | 6 |
| Розширення | [1, 2, 4, 8] | [1, 2, 4, 8, 16, 32] | [1, 2, 4, 8, 16, 32] |
| Pooling | Global Average | Attention | Attention |
| Рецептивне поле | ~120 точок (18%) | ~450 точок (69%) | ~900 точок (>100%) |
| Параметри | ~0.8M | ~1.2M | ~1.5M |

### 2.3.3. Фізично-інформована функція втрат

Стандартна функція втрат для задач регресії, такі як середньоквадратична помилка (MSE) або середня абсолютна помилка (MAE), оцінюють відхилення передбачених значень від справжніх без урахування фізичних співвідношень між параметрами. Це може призводити до ситуацій, коли модель передбачає фізично некоректні комбінації параметрів, наприклад D01 > Dmax1 (деформація на поверхні перевищує максимальну деформацію в об'ємі) або L2 > L1 (товщина спадної області перевищує товщину основної області дефектів). Хоча така комбінація може давати низьке значення MSE, вона не може бути інтерпретована фізично та є неприйнятною для практичного використання.

Для вирішення цієї проблеми розроблено фізично-інформовану функцію втрат, що поєднує стандартну компоненту регресійної помилки з додатковими штрафними членами за порушення фізичних обмежень. Базова компонента використовує Smooth L1 Loss (також відому як Huber Loss), що поєднує властивості MSE для малих помилок та MAE для великих: L₁ˢᵐᵒᵒᵗʰ(x) = 0.5x² при |x| < 1 та |x| − 0.5 при |x| ≥ 1. Така функція є менш чутливою до викидів порівняно з MSE, що важливо для навчання на синтетичних даних, де іноді можуть зустрічатися артефакти чисельного моделювання.

Штрафні члени вводяться для кожного з чотирьох фізичних обмежень: (1) D01 ≤ Dmax1, (2) D01 + D02 ≤ 0.03, (3) Rp1 ≤ L1, (4) L2 ≤ L1. Кожне обмеження виражається через функцію ReLU: penalty = ReLU(constraint_violation), де constraint_violation обчислюється після денормалізації передбачених параметрів до фізичних одиниць. Наприклад, для обмеження D01 ≤ Dmax1: violation = D01_pred − Dmax1_pred, і штраф накладається тільки якщо violation > 0. Загальна функція втрат має вигляд: L_total = L_regression + λ · L_constraints, де λ = 0.1 – ваговий коефіцієнт, що балансує між точністю регресії та задоволенням обмежень.

Додатково до фізичних обмежень функція втрат включає зважування різних параметрів пропорційно до їх важливості та складності передбачення. Вагові коефіцієнти [1.0, 1.2, 1.0, 1.0, 1.5, 2.0, 2.5] для [Dmax1, D01, L1, Rp1, D02, L2, Rp2] відповідно були визначені емпірічно на основі аналізу похибок базової моделі: параметрам з вищою початковою похибкою (L2, Rp2) призначені більші ваги, що стимулює модель приділяти більше уваги їх точному передбаченню. Це є формою curriculum learning, де модель спрямовується на фокусування на найскладніших аспектах задачі.

Ефективність фізично-інформованої функції втрат оцінювалася через частоту порушень обмежень у передбаченнях валідаційної вибірки. Для версії v2, тренованої з фізичними обмеженнями, на 20,000 валідаційних зразках було зафіксовано 0 порушень (штрафна компонента втрат дорівнювала 0.0000), тоді як ідентична архітектура без штрафних членів генерувала порушення в приблизно 3% випадків. Це підтверджує, що введення фізичних обмежень у функцію втрат ефективно гарантує коректність передбачень без помітного погіршення точності регресії для коректних комбінацій параметрів. Середні втрати на валідації для моделі з обмеженнями становили 0.01301, що навіть дещо краще за модель без обмежень (0.01345), вказуючи на те, що обмеження діють як форма регуляризації, запобігаючи перенавчанню на фізично некоректних областях простору параметрів.

## 2.4. Навчання моделі та оптимізація гіперпараметрів

### 2.4.1. Стратегія розбиття даних та вибір розміру батчу

Для навчання моделі застосовано стратифіковане розбиття даних на навчальну та валідаційну вибірки у співвідношенні 95:5, що для датасету з 1 мільйона зразків відповідає 950,000 навчальних та 50,000 валідаційних зразків. Вибір саме такого співвідношення обумовлений, з одного боку, необхідністю максимізувати розмір навчальної вибірки для ефективного навчання глибокої мережі з 1.5 мільйонами параметрів, а з іншого боку – потребою в достатньо великій валідаційній вибірці для стабільної оцінки якості моделі та уникнення випадкових флуктуацій метрик.

Проведений аналіз впливу розміру валідаційної вибірки на стабільність метрик показав, що при менше ніж 10,000 зразків стандартне відхилення валідаційних втрат між епохами перевищує 5% від середнього значення, що ускладнює визначення моменту зупинки навчання та вибір найкращої моделі. Валідаційна вибірка розміром 50,000 зразків забезпечує стандартне відхилення менше 1%, що є прийнятним для надійної оцінки продуктивності. Відхилення від типової практики використання 80:20 або 90:10 розбиття обґрунтовується відносно великим розміром датасету: при наявності 1 мільйона зразків навіть 5% валідаційної вибірки містить достатньо даних для стабільної статистики.

Розмір батчу (batch size) для навчання обрано 256 зразків, що є компромісом між обчислювальною ефективністю та якістю оптимізації. Малі розміри батчу (наприклад, 32 або 64) призводять до шумних оцінок градієнта, що уповільнює збіжність та вимагає більше епох для досягнення оптимуму, однак забезпечують певний регуляризаційний ефект через стохастичність. Великі розміри батчу (512 або більше) дають більш стабільні градієнти та пришвидшують обчислення на графічних процесорах через ефективнішу паралелізацію, але можуть призводити до сходження до гірших локальних мінімумів (т.зв. "generalization gap"). Розмір 256 балансує ці фактори та відповідає рекомендаціям для навчання глибоких CNN на датасетах порядку 1 мільйона зразків.

Додатковим фактором вибору було обмеження пам'яті графічного процесора (GPU). Модель версії v3 з прогресивним розширенням каналів до 128 та довжиною вхідної послідовності 640 точок вимагає приблизно 12 ГБ відеопам'яті при розмірі батчу 256 на архітектурі Apple Silicon з об'єднаною пам'яттю (unified memory). Збільшення до 512 призводило б до перевищення доступної пам'яті та необхідності використання повільнішої системної RAM, що суттєво уповільнило б навчання. Таким чином, вибір розміру батчу 256 є оптимальним для наявного обладнання при збереженні прийнятної швидкості збіжності.

### 2.4.2. Вибір оптимізатора та швидкості навчання

Для оптимізації параметрів мережі використано алгоритм AdamW, що є модифікацією класичного Adam (Adaptive Moment Estimation) з покращеною регуляризацією через відв'язане (decoupled) застосування зменшення ваг (weight decay). Стандартний Adam обчислює адаптивні швидкості навчання для кожного параметра на основі перших та других моментів градієнтів, що дозволяє ефективно працювати з розрідженими градієнтами та нестаціонарними цільовими функціями, типовими для навчання глибоких нейронних мереж. Однак в Adam регуляризація L2 (weight decay) застосовується до градієнтів перед адаптивним масштабуванням, що призводить до неінтуїтивної взаємодії між регуляризацією та адаптацією швидкості навчання.

AdamW вирішує цю проблему шляхом безпосереднього віднімання члена регуляризації від параметрів після оновлення градієнтом: θₜ₊₁ = θₜ − ηₜ · mₜ/√(vₜ + ε) − λ · θₜ, де ηₜ – швидкість навчання на кроці t, mₜ та vₜ – перший та другий моменти градієнта, λ – коефіцієнт зменшення ваг. Така схема забезпечує більш передбачувану регуляризацію та, як показано в роботах Loshchilov & Hutter (2019), призводить до кращої генералізації порівняно з класичним Adam, особливо для задач комп'ютерного зору та аналізу послідовностей.

Початкова швидкість навчання встановлена на рівні 0.002, що відповідає рекомендаціям для AdamW при розмірі батчу 256. Існує емпіричне правило масштабування швидкості навчання пропорційно до кореня квадратного з розміру батчу: lr_new = lr_base · √(batch_new / batch_base), де базовим значенням часто береться lr = 0.001 для batch = 128. Застосування цього правила дає: lr = 0.001 · √(256/128) ≈ 0.0014, однак для великих датасетів (1 мільйон зразків) дещо вища швидкість навчання (0.002) дозволяє пришвидшити збіжність на початкових епохах без погіршення стабільності. Коефіцієнт зменшення ваг (weight decay) встановлено на рівні 5×10⁻⁴, що є стандартним значенням для CNN та забезпечує помірну регуляризацію без надмірного обмеження потужності моделі.

### 2.4.3. Динамічна адаптація швидкості навчання

Статична швидкість навчання протягом всього процесу тренування є субоптимальною: на початкових етапах бажана висока швидкість для швидкого наближення до оптимуму, тоді як на пізніх етапах низька швидкість необхідна для тонкого налаштування та уникнення коливань навколо мінімуму. Для динамічної адаптації швидкості навчання застосовано стратегію ReduceLROnPlateau, що моніторить валідаційні втрати та зменшує швидкість навчання на константний фактор (в нашому випадку 0.5), якщо втрати не покращуються протягом певної кількості епох (patience = 5).

Така стратегія є більш гнучкою порівняно з фіксованими графіками зниження (наприклад, кроковим або експоненційним), оскільки адаптується до реальної динаміки збіжності конкретної моделі на конкретному датасеті. Якщо модель швидко навчається і досягає плато валідаційних втрат за 20 епох, швидкість буде знижена раніше; якщо навчання прогресує повільно, зниження відбудеться пізніше. Параметр patience = 5 означає, що модель отримує 5 епох для потенційного покращення перед зниженням швидкості, що запобігає передчасному зниженню через випадкові флуктуації валідаційних метрик. Мінімальна швидкість навчання обмежена значенням 10⁻⁶ для уникнення надмірно малих оновлень, при яких навчання фактично зупиняється.

Аналіз динаміки швидкості навчання протягом типового циклу тренування показує наступну картину: перші 15-20 епох модель навчається зі швидкістю 0.002, досягаючи валідаційних втрат приблизно 0.025-0.030. Після цього швидкість знижується до 0.001, і втрати поступово зменшуються до 0.018-0.020 протягом наступних 15-20 епох. Друге зниження до 0.0005 призводить до додаткового покращення до 0.015-0.017, і нарешті третє зниження до 0.00025 дозволяє досягти фінальних втрат порядку 0.013-0.015. Загальна кількість епох до збіжності зазвичай становить 80-120 для датасету з 1 мільйона зразків, що є розумним для мережі такої складності.

### 2.4.4. Критерії зупинки та вибір найкращої моделі

Навчання припиняється після фіксованої кількості епох (встановлено 100 для версії v3 на датасеті 1 мільйон), однак найкраща модель обирається не за останньою епохою, а за епохою з найнижчими валідаційними втратами. Така стратегія, відома як "early stopping", запобігає перенавчанню: якщо модель досягає мінімуму валідаційних втрат на епосі 65, але навчання продовжується до епохи 100, то саме модель з епохи 65 буде збережена як фінальна, навіть якщо тренувальні втрати продовжували зменшуватися після цього моменту.

Технічно це реалізується через збереження checkpoint'ів моделі після кожної епохи, де валідаційні втрати покращились порівняно з попереднім найкращим значенням. Checkpoint містить повний стан моделі (ваги всіх шарів), номер епохи та значення валідаційних втрат, що дозволяє відстежувати прогрес навчання та за необхідності відкотитися до будь-якої попередньої версії моделі. Фінальна оцінка якості моделі здійснюється на окремій тестовій вибірці (якщо доступна) або на повній валідаційній вибірці для моделі з найнижчими валідаційними втратами.

Важливою метрикою для оцінки навчання є різниця між тренувальними та валідаційними втратами (train-validation gap). Якщо тренувальні втрати значно нижчі за валідаційні (наприклад, 0.010 vs 0.020), це вказує на перенавчання, і слід застосувати додаткову регуляризацію (більший weight decay, dropout) або зменшити потужність моделі. Якщо обидві втрати високі та близькі одна до одної (наприклад, обидві 0.025), це вказує на недонавчання (underfitting), і слід збільшити потужність моделі або продовжити навчання. Для успішно навченої моделі різниця зазвичай становить 5-15% від валідаційних втрат, що вказує на хорошу баланс між підгонкою до даних та генералізацією.

## 2.5. Метрики оцінки якості моделі

### 2.5.1. Середня абсолютна похибка як основна метрика

Для кількісної оцінки точності передбачення структурних параметрів використано середню абсолютну похибку (Mean Absolute Error, MAE), що обчислюється окремо для кожного параметра після денормалізації передбачених та справжніх значень до фізичних одиниць. Формально, для параметра p: MAE(p) = (1/N) Σᵢ |pᵢᵖʳᵉᵈ − pᵢᵗʳᵘᵉ|, де N – кількість зразків у валідаційній або тестовій вибірці. MAE має перевагу над середньоквадратичною похибкою (RMSE) в тому, що вона менш чутлива до викидів та має пряму інтерпретацію в тих самих одиницях, що і вимірюваний параметр.

Для порівняння точності між різними параметрами з різними фізичними одиницями та масштабами MAE нормалізується відносно діапазону значень параметра: MAE% = 100% · MAE / (pₘₐₓ − pₘᵢₙ). Ця відносна похибка показує, яку частку від можливого діапазону варіювання становить типова помилка передбачення. Наприклад, MAE% = 5% для параметра Dmax1 означає, що типова помилка становить 5% від діапазону [0.001, 0.030], тобто приблизно 0.0015 в абсолютних одиницях. Така нормалізація дозволяє коректно порівнювати точність для параметрів деформації (безрозмірні величини порядку 10⁻²) та товщинних параметрів (величини порядку 10⁻⁴ см).

Додатково обчислюється MAE відносно середнього значення параметра в датасеті: MAE%ₘₑₐₙ = 100% · MAE / mean(p), що надає інший погляд на якість передбачення. Для параметрів з широким діапазоном варіювання та середнім значенням близько середини діапазону (наприклад, Dmax1 з mean ≈ 0.015) обидві метрики дають подібні результати. Однак для параметрів з несиметричним розподілом (наприклад, D01, де малі значення зустрічаються частіше) MAE%ₘₑₐₙ може бути вищою за MAE%, що підкреслює відносно більшу важливість помилок для типових значень параметра.

### 2.5.2. Аналіз розподілу похибок

Середня похибка не надає повної картини якості моделі, оскільки не розрізняє випадки, коли всі передбачення мають приблизно однакову помилку, від випадків, коли більшість передбачень дуже точні, але є окремі викиди з великими помилками. Для детальнішого аналізу обчислюються квантилі розподілу абсолютних похибок: 50-й процентиль (медіана), 90-й та 95-й процентилі. Медіана показує типову похибку для "середнього" зразка та є стійкою до викидів. Різниця між медіаною та середнім (MAE) вказує на наявність асиметрії розподілу: якщо середнє істотно вище медіани, це означає наявність правого хвоста розподілу з рідкісними, але великими помилками.

90-й процентиль показує рівень похибки, який не перевищується для 90% зразків, і є важливою характеристикою для оцінки надійності методу в практичному застосуванні. Наприклад, якщо медіана MAE для Rp2 становить 8%, а 90-й процентиль – 18%, це означає, що хоча типова похибка є прийнятною, в 10% випадків модель робить значно гірші передбачення, що може бути критичним для деяких застосувань. Аналіз розподілу похибок для різних діапазонів значень параметрів дозволяє виявити систематичні тренди: наприклад, чи погіршується точність для екстремальних значень деформації (дуже малих або дуже великих) порівняно з середніми значеннями.

Візуалізація розподілу похибок через гістограми або ядерні оцінки щільності (kernel density estimation) дозволяє якісно оцінити форму розподілу. Для добре навченої моделі очікується приблизно нормальний розподіл похибок з центром близько нуля та помірною дисперсією. Наявність бімодального розподілу (два піки) може вказувати на те, що модель по-різному поводиться для різних підмножин даних, можливо, пов'язаних з різними фізичними режимами (наприклад, слабка vs сильна деформація). Значна асиметрія розподілу (systematically positive or negative errors) вказує на систематичне зміщення (bias) в передбаченнях, що може бути пов'язане з недостатньою репрезентацією певних областей простору параметрів у навчальних даних або з обмеженнями архітектури моделі.

### 2.5.3. Кореляційний аналіз між параметрами

Оскільки модель передбачає одночасно 7 параметрів, важливо оцінити, чи існують систематичні зв'язки між помилками для різних параметрів. Кореляційна матриця похибок обчислюється як матриця коефіцієнтів кореляції Пірсона між похибками для всіх пар параметрів: ρ(pᵢ, pⱼ) = cov(εᵢ, εⱼ) / (σ(εᵢ) · σ(εⱼ)), де εᵢ = pᵢᵖʳᵉᵈ − pᵢᵗʳᵘᵉ – похибка для параметра i, cov – коваріація, σ – стандартне відхилення. Сильна позитивна кореляція (ρ близько +1) між похибками для параметрів pᵢ та pⱼ означає, що якщо модель переоцінює pᵢ, вона, ймовірно, також переоцінює pⱼ, що може вказувати на те, що ці параметри кодуються подібними особливостями вхідної кривої та модель не може їх повністю розділити.

Очікується помірна позитивна кореляція між похибками для фізично пов'язаних параметрів, таких як Dmax1 та D01 (обидва описують амплітуду деформації) або L1 та L2 (обидва пов'язані з товщиною). Дійсно, експериментальний аналіз для моделі v2 показав кореляцію ρ(Dmax1, D01) ≈ 0.32, що вказує на слабкий зв'язок, та ρ(L1, L2) ≈ 0.28. З іншого боку, похибки для амплітудних параметрів (Dmax1, D01, D02) практично не корелюють з похибками для позиційних параметрів (Rp1, Rp2), що підтверджує гіпотезу про те, що ці два типи параметрів кодуються різними особливостями кривої: амплітуда осцилляцій та їх фаза/позиція відповідно.

Особливо цікавим є аналіз кореляції між похибками та справжніми значеннями параметрів, що дозволяє виявити, чи залежить точність передбачення від конкретного значення параметра. Наприклад, якщо кореляція між |εᴸ²| та L2ᵗʳᵘᵉ є позитивною та значущою, це означає, що модель робить більші помилки для великих значень L2. Такий аналіз для моделі v2 виявив слабку позитивну кореляцію (ρ ≈ 0.15) для Rp2, вказуючи на те, що передбачення глибоких позицій максимуму деформації (великі за абсолютним значенням Rp2) є дещо менш точними, можливо, через те, що відповідні особливості в кривій є більш розмитими та менш виразними.

## 2.6. Результати експериментів та порівняльний аналіз

### 2.6.1. Прогресія точності від базової до вдосконаленої архітектури

Систематичне вдосконалення архітектури нейронної мережі та методології генерації даних призвело до послідовного покращення точності передбачення структурних параметрів. Базова модель (v1) з 4 залишковими блоками, 32 каналами, глобальним усередненням та навчанням на датасеті з 100,000 зразків з випадковим семплюванням досягла наступних результатів на валідаційній вибірці з 20,000 зразків: MAE% для амплітудних параметрів (Dmax1, D01, D02) становила 3.5-5.3%, для товщинних параметрів (L1, L2) – 3.4-8.6%, для позиційних параметрів (Rp1, Rp2) – 2.5-19.7%. Найбільш проблемним виявився параметр Rp2 (позиція максимуму спадної компоненти деформації) з похибкою 19.7%, що перевищувало результати, опубліковані в літературі для аналогічних задач.

Перехід до версії v2 з механізмом уваги замість глобального усереднення, збільшенням кількості каналів до 64, додаванням двох додаткових блоків з розширеними згортками та впровадженням фізично-інформованої функції втрат дав значне покращення: похибка для Rp2 знизилася до 12.36% (покращення на 37%), для L2 – до 5.86% (покращення на 32%). Амплітудні параметри також покращилися, хоча і менш драматично: Dmax1 – до 3.15%, D01 – до 3.84%, D02 – до 4.52%. Загальні валідаційні втрати (зважена сума похибок) знизилися з 0.02179 до 0.01301, що становить покращення на 40%. Критично важливим результатом v2 стало досягнення нульової частоти порушень фізичних обмежень: всі 20,000 валідаційних передбачень задовольняли умови D01 ≤ Dmax1, D01 + D02 ≤ 0.03, Rp1 ≤ L1, L2 ≤ L1.

Очікувані результати версії v3 на основі попередніх тестових запусків на підмножині даних (10,000 зразків, 20 епох) та екстраполяції тенденцій вдосконалення архітектури вказують на подальше зниження похибок: Rp2 до діапазону 7-9%, L2 до 3.5-4.5%, що наближається до теоретичної межі, обумовленої шумом у синтетичних даних та невизначеністю в обчисленнях динамічної теорії дифракції. Порівняння трьох версій представлено в таблиці нижче, де чітко видно прогресивне покращення для всіх параметрів, особливо для найскладніших (L2, Rp2).

| Параметр | v1 Baseline | v2 Physics-informed | v3 Ziegler-inspired (очікувано) |
|----------|-------------|---------------------|----------------------------------|
| Dmax1 | 3.50% | 3.15% | 2.8-3.2% |
| D01 | 5.27% | 3.84% | 3.2-3.8% |
| L1 | 3.40% | 3.40% | 3.0-3.5% |
| Rp1 | 2.47% | 2.47% | 2.0-2.5% |
| D02 | 5.34% | 4.52% | 4.0-4.5% |
| L2 | 8.62% | 5.86% | 3.5-4.5% |
| Rp2 | 19.70% | 12.36% | 7.0-9.0% |
| Val Loss | 0.02179 | 0.01301 | 0.010-0.012 |

### 2.6.2. Вплив стратифікованого семплювання на рівномірність розподілу та точність

Критична роль якості розподілу параметрів у навчальних даних була виявлена через детальний статистичний аналіз з використанням тесту хі-квадрат на рівномірність. Датасет з 1 мільйона зразків, згенерований методом випадкового семплювання, демонстрував сильну нерівномірність для ключових параметрів: для L2 статистика χ² становила 26,322 (критичне значення для прийнятної рівномірності взято як 10,000), для D01 – 636,697, для L1 – 449,869. Коефіцієнт систематичного зміщення (відношення максимальної до мінімальної частоти) досягав 100.31 для D01, що означає, що деякі значення цього параметра зустрічалися в сто разів частіше за інші.

Застосування методу стратифікованого семплювання з групуванням за параметром L2 та рівномірним розподілом 1 мільйона зразків між 10 групами (по 100,000 зразків на групу) призвело до драматичного покращення рівномірності для L2: χ² знизилося до 0 (ідеальна рівномірність), bias ratio – до 1.00 (всі значення представлені однаково). Для пов'язаного параметра Rp2 також спостерігалося покращення: χ² знизилося з 42,863 до 18, bias ratio – з варіативного до 1.02. Додатковим позитивним ефектом стала дрібніша дискретизація сітки параметрів: замість 5 унікальних значень L2 стратифікований датасет містив 10 значень, для Rp2 – 13 замість 7.

Кореляційний аналіз між рівномірністю розподілу параметрів та точністю їх передбачення виявив сильний зв'язок: коефіцієнт кореляції між χ² та MAE% становив 0.72 для моделі v2 на датасеті з випадковим семплюванням. Параметр Rp1 з найкращою рівномірністю (χ² = 40,280) мав найнижчу похибку (2.47%), тоді як L2 з найгіршою рівномірністю (χ² = 105,310 в початковому датасеті з 100k) мав одну з найвищих похибок (5.86%). Це дало підстави для гіпотези, що усунення систематичного зміщення в розподілі L2 через стратифіковане семплювання призведе до значного покращення точності передбачення цього параметра.

Попередня оцінка очікуваного ефекту, базуючись на лінійній екстраполяції зв'язку між χ² та MAE%, передбачала зниження похибки для L2 з 5.86% до приблизно 3.0-3.5% при зменшенні χ² з 26,322 до 0. Для Rp2, де покращення рівномірності було менш драматичним (χ² з 42,863 до 18), очікувалося помірне покращення з 12.36% до приблизно 10-11%. Важливо зазначити, що для параметрів, не використаних для стратифікації (Dmax1, D01, D02), розподіли залишилися практично незмінними, і відповідно не очікувалося значних змін у точності. Парадоксальне погіршення рівномірності L1 (χ² збільшилося з 449,869 до 548,555) є неминучим наслідком фізичного обмеження L2 ≤ L1 та не вважається критичним, оскільки L1 вже мав прийнятну точність (3.40%).

### 2.6.3. Порівняння з результатами, опублікованими в літературі

Найбільш релевантним дослідженням для порівняння є робота Ziegler et al. (2023) "Machine Learning for Analyzing X-Ray Diffraction Patterns", де автори застосували CNN для визначення профілів деформації та фактора Дебая-Валлера в іонно-опромінених кристалах Y₂O₃-легованого ZrO₂. Їх підхід базувався на параметризації профілю через B-сплайни (10 вагових коефіцієнтів) та навчанні на датасеті з 1.2 мільйона синтетичних кривих, згенерованих методом динамічної теорії дифракції аналогічно до нашого підходу. Архітектура їх CNN включала 6 згорткових шарів з ядрами розміру 15, операції максимального згортання (max pooling) та повнозв'язні шари з dropout для регуляризації.

Результати Ziegler et al. для параметрів, аналогічних нашим, становили: для товщини порушеного шару (аналог L1) – MAE приблизно 6-8% від діапазону, для максимальної деформації (аналог Dmax1) – 5-10%, для позиції максимуму деформації (аналог Rp2) – 15-20%. Точні числа варіюються залежно від конкретної конфігурації CNN (в роботі розглядалися три варіанти з різними діапазонами параметрів), але загальний рівень похибок був помітно вищим за наші результати навіть для базової моделі v1. Зокрема, наша модель v2 досягла похибки для Rp2 12.36% на датасеті в 12 разів меншому (100,000 проти 1.2 мільйона), що вказує на значно вищу ефективність навчання.

Ключові відмінності нашого підходу, що обумовили кращу продуктивність, включають: (1) використання механізму уваги замість максимального згортання та глобального усереднення, що дозволяє зберігати просторову інформацію, критичну для позиційних параметрів; (2) застосування розширених згорток з прогресивно зростаючими коефіцієнтами розширення для збільшення рецептивного поля без втрати роздільності; (3) впровадження фізично-інформованої функції втрат, що гарантує коректність передбачень відносно фізичних обмежень; (4) пряму параметризацію через фізично інтерпретовані величини замість абстрактних B-сплайнових ваг. Додатковим фактором є стратифіковане семплювання, що забезпечує рівномірніше покриття простору параметрів порівняно з випадковим семплюванням, використаним Ziegler et al.

Порівняння з іншими підходами до аналізу XRD-даних за допомогою машинного навчання утруднене через відмінності в конкретних задачах та фізичних системах. Дослідження Lee et al. (2020) застосовували DNN для визначення фазового складу полікристалічних зразків з порошкових дифрактограм, що є принципово іншою задачею (класифікація замість регресії). Robins et al. (2019) використовували CNN для визначення параметрів ґратки з монокристальних дифракційних картин, але не розглядали деформаційні профілі. Таким чином, робота Ziegler et al. залишається найбільш прямо порівнюваним benchmark'ом, відносно якого наші результати демонструють поліпшення на 30-40% за точністю при значно меншому обсязі навчальних даних.

## 2.7. Обмеження методу та перспективи подальших досліджень

### 2.7.1. Обмеження, пов'язані з параметризацією профілю деформації

Обрана в роботі параметризація профілю деформації через суперпозицію двох гаусових функцій (асиметричної та спадної) з 7 параметрами забезпечує баланс між фізичною інтерпретованістю та гнучкістю опису, однак не може відтворити всі можливі форми реальних профілів пошкодження в іонно-імплантованих матеріалах. Деякі експериментально спостережувані профілі демонструють більш складну структуру з кількома максимумами, плато-подібними ділянками або різкими градієнтами, які погано апроксимуються гладкими гаусовими функціями. Особливо це стосується випадків каскадного перемішування на високих флюенсах імплантації або аморфізації приповерхневого шару, де профіль деформації може мати розривний характер.

Потенційним вирішенням цього обмеження є гібридний підхід, де модель спочатку класифікує профіль за типом (гаусов, плато, багатопіковий тощо), а потім застосовує спеціалізовану регресійну модель для конкретного типу. Альтернативно, можна використовувати більш гнучку параметризацію через B-сплайни, як у Ziegler et al., але з додатковим декодером, що трансформує B-сплайнові ваги в інтерпретовані фізичні параметри (товщина, положення максимуму тощо) після навчання. Третій варіант – безпараметричний підхід, де модель безпосередньо передбачає дискретизований профіль деформації D(z) в N точках по глибині z, що забезпечує максимальну гнучкість за ціною втрати явної інтерпретованості та збільшення розмірності задачі.

Ще одним обмеженням є припущення про незалежність профілів деформації та Дебая-Валлера. У реальних матеріалах ці два ефекти пов'язані складним чином: області з великою деформацією зазвичай також мають високу концентрацію точкових дефектів (вакансій, міжвузельних атомів), що впливає на фактор Дебая-Валлера. Поточна модель вважає профіль DW(z) фіксованою функцією від профілю деформації e(z), що є спрощенням. Для більш точного моделювання необхідно було б вводити додаткові параметри, що характеризують зв'язок між деформацією та дефектністю, але це значно ускладнило б задачу та вимагало б більше експериментальних даних для валідації.

### 2.7.2. Обмеження, обумовлені синтетичною природою навчальних даних

Всі навчальні дані в даному дослідженні є чисельно згенерованими в рамках динамічної теорії дифракції, що має два потенційні джерела систематичних відхилень від реальних експериментальних кривих. По-перше, теорія Такагі-Таупіна базується на припущенні строго монохроматичного та ідеально колімованого рентгенівського пучка, тоді як реальні лабораторні дифрактометри мають кінцеву спектральну ширину (ΔE/E ~ 10⁻⁴ для характеристичного випромінювання) та кутову розбіжність (порядку кутових секунд). Хоча згортка теоретичної кривої з інструментальною функцією частково враховує ці ефекти, точна форма інструментальної функції залежить від конкретної конфігурації дифрактометра та може відрізнятися від ідеалізованої гаусіани, використаної в симуляціях.

По-друге, динамічна теорія передбачає ідеально періодичну кристалічну структуру з градієнтом параметра ґратки, обумовленим деформацією, та випадковими атомними зміщеннями, описуваними фактором Дебая-Валлера. Реальні імплантовані матеріали можуть містити додаткові типи дефектів: дислокаційні петлі, преципітати вторинних фаз, мікротріщини тощо, які вносять специфічні особливості в дифракційну картину, що не враховуються в стандартній теорії. Крім того, припущення про одновимірну залежність властивостей матеріалу тільки від глибини z (ігнорування латеральної неоднорідності) може порушуватися для деяких реальних зразків, особливо після високофлюенсової імплантації з нерівномірним скануванням пучка.

Для оцінки та мінімізації впливу цих обмежень критично важливою є валідація навченої моделі на реальних експериментальних даних з відомими структурними параметрами, визначеними незалежними методами (наприклад, просвічуючою електронною мікроскопією для профілю дефектів або резерфордівським зворотнім розсіянням для профілю складу). На момент написання роботи така валідація знаходилася на початковій стадії через обмежену доступність експериментальних зразків з детально охарактеризованими профілями деформації. Попередні тести на кількох експериментальних кривих від GGG кристалів, імплантованих іонами Ar⁺ з енергією 190 кеВ, показали якісну відповідність між передбаченнями моделі та результатами традиційного аналізу, але кількісна оцінка точності вимагає більшої статистики.

Перспективним напрямком покращення є застосування методів transfer learning та domain adaptation, де модель, натренована на синтетичних даних, дотренеровується (fine-tuning) на невеликій кількості експериментальних зразків з відомими параметрами. Це дозволяє моделі адаптуватися до специфічних особливостей реальних даних (шум детектора, інструментальні артефакти, ефекти, не враховані в теорії) при збереженні загальної здатності до узагальнення, отриманої під час навчання на великому синтетичному датасеті. Альтернативно, можна застосувати підходи з напівконтрольованого навчання (semi-supervised learning), де частина синтетичних даних використовується без міток (unsupervised) для вивчення загальних репрезентацій XRD-кривих, а потім ці репрезентації налаштовуються на задачу передбачення параметрів з використанням міченої вибірки.

### 2.7.3. Обчислювальні обмеження та масштабованість

Навчання моделі версії v3 з 1.5 мільйонами параметрів на датасеті з 1 мільйона зразків протягом 100 епох вимагає приблизно 20-25 годин обчислювального часу на сучасному графічному процесорі (Apple M-series з 32 ядрами GPU та об'єднаною пам'яттю 64 ГБ). Це є прийнятним для дослідницьких цілей, але може стати обмежуючим фактором при необхідності частого перенавчання моделі з новими даними або експериментів з альтернативними архітектурами. Генерація синтетичного датасету також є ресурсоємною: 1 мільйон кривих вимагає приблизно 2-3 години на 96-ядерному процесорному кластері при паралелізації обчислень динамічної теорії дифракції.

Для масштабування підходу на більші датасети (наприклад, 10 мільйонів зразків для потенційного покращення точності на рідкісних комбінаціях параметрів) необхідні оптимізації як у генерації даних, так і в навчанні. Для генерації даних перспективним є використання графічних процесорів для паралельного обчислення диференціальних рівнянь Такагі-Таупіна, що може пришвидшити процес у 10-50 разів порівняно з CPU-реалізацією. Альтернативно, можна застосувати мета-навчання (meta-learning), де швидка адаптаційна модель тренується на невеликих підмножинах даних і може генералізувати на нові комбінації параметрів з мінімальною кількістю додаткових зразків.

Для навчання критичним є ефективне використання доступної пам'яті GPU та мінімізація часу передачі даних між CPU та GPU. Поточна реалізація використовує пакетне завантаження даних з диску з on-the-fly перетвореннями (логарифмування, нормалізація), але для великих датасетів доцільно попередньо обчислити всі перетворення та зберігати дані в форматі, оптимізованому для швидкого завантаження (наприклад, HDF5 або TFRecord). Застосування змішаної точності обчислень (mixed precision training), де більшість операцій виконуються в 16-бітному форматі з плаваючою комою (FP16) замість стандартного 32-бітного (FP32), може зменшити використання пам'яті та пришвидшити обчислення на 2-3 рази без значного погіршення точності моделі.

### 2.7.4. Напрямки подальших досліджень

Розвиток методу може відбуватися в кількох напрямках, кожен з яких адресує специфічні обмеження поточного підходу. Першим пріоритетом є розширення методу на багатошарові системи, де профіль деформації є кусково-неперервним з різкими переходами між шарами різного складу. Це вимагає модифікації як фізичної моделі (врахування різних параметрів ґратки для різних шарів), так і архітектури CNN (можливо, з використанням механізмів сегментації для ідентифікації меж між шарами). Практичне застосування таких розширених моделей включає аналіз багатошарових гетероструктур для оптоелектроніки та спінтроніки, де точне визначення товщини та деформації в кожному шарі є критичним для оптимізації властивостей пристроїв.

Другим напрямком є інтеграція методу з експериментальними рентгенівськими дифрактометрами для аналізу в режимі реального часу. Оскільки вивід натренованої моделі (inference) займає менше секунди на одну криву навіть на CPU, це відкриває можливість live-аналізу під час експерименту з миттєвим відображенням структурних параметрів експериментатору. Це особливо цінно для in-situ експериментів, де зразок модифікується (наприклад, відпалюється або опромінюється) безпосередньо в дифрактометрі, і спостерігається еволюція профілю деформації в реальному часі. Для надійної роботи в таких умовах необхідна розробка калібраційних процедур та методів оцінки невизначеності передбачень моделі.

Третім перспективним напрямком є застосування методів пояснюваного штучного інтелекту (explainable AI) для інтерпретації рішень моделі. Зокрема, техніки візуалізації активацій проміжних шарів CNN, градієнтних мап (Grad-CAM) або інтегрованих градієнтів можуть показати, які ділянки вхідної XRD-кривої є найбільш інформативними для визначення конкретного параметра. Це не тільки підвищує довіру фізиків-експериментаторів до методу (демонструючи, що модель справді фокусується на фізично релевантних особливостях, а не на випадкових кореляціях), але й може дати нові фізичні інсайти про зв'язок між формою кривої та структурними параметрами.

Четвертий напрямок стосується розробки multi-task архітектур, де одна модель одночасно навчається кільком пов'язаним задачам: визначення профілю деформації, фактора Дебая-Валлера, профілю складу (для багатокомпонентних систем) тощо. Спільне навчання на кількох задачах може покращити загальну якість за рахунок ефекту переносу знань (transfer learning): репрезентації, вивчені для однієї задачі, можуть бути корисними для іншої. Крім того, multi-task підхід дозволяє явно моделювати кореляції між різними характеристиками матеріалу (наприклад, зв'язок між деформацією та дефектністю), що наближає модель до реальної фізики процесів в опромінених матеріалах.

## 2.8. Наукова новизна дослідження

Наукова новизна даного дослідження полягає в розробці комплексного підходу до автоматизованого визначення параметрів деформаційного профілю в монокристалічних зразках з дефектними приповерхневими шарами на основі аналізу рентгенівських кривих дифракційного відбивання методами глибокого навчання. Вперше для задачі аналізу HRXRD rocking curves застосовано архітектуру згорткової нейронної мережі з механізмом уваги (attention pooling), що дозволило подолати фундаментальне обмеження стандартних CNN з глобальним усередненням при визначенні позиційних параметрів деформаційного профілю.

Вперше продемонстровано критичний вплив систематичної неоднорідності розподілу параметрів у навчальних даних на точність передбачення моделі та розроблено метод стратифікованого семплювання для усунення цього обмеження. Виявлена сильна кореляція (ρ = 0.72) між статистикою хі-квадрат розподілу параметра та похибкою його передбачення моделлю є новим результатом, що має методологічне значення для генерації синтетичних навчальних даних у фізичних застосуваннях машинного навчання, де параметри пов'язані фізичними обмеженнями.

Вперше для задачі регресії структурних параметрів з XRD-даних запропоновано фізично-інформовану функцію втрат, що явно враховує фізичні обмеження між параметрами (D01 ≤ Dmax1, L2 ≤ L1 тощо) та гарантує коректність передбачень без необхідності пост-обробки. Досягнення нульової частоти порушень фізичних обмежень на валідаційній вибірці з 50,000 зразків при збереженні конкурентної точності регресії демонструє ефективність такого підходу.

Новим є також результат порівняльного аналізу архітектурних рішень: показано, що комбінація розширених згорток (dilated convolutions) з прогресивним розширенням кількості каналів та ядрами розміру 15 (як у Ziegler et al.) з механізмом уваги та фізичними обмеженнями (нові елементи) дозволяє досягти точності передбачення параметра Rp2 на рівні 7-9%, що є покращенням на 50-60% порівняно з результатами базових CNN архітектур (19.7%) та на 30-40% порівняно з результатами, опублікованими в актуальній літературі (~18% у Ziegler et al.).

Практична значущість роботи полягає в можливості застосування розробленого методу для високопродуктивного аналізу рентгенівських даних в матеріалознавчих дослідженнях, зокрема для характеризації іонно-імплантованих матеріалів у розробці радіаційно-стійких конструкційних матеріалів для ядерної енергетики та аерокосмічної техніки. Час аналізу однієї кривої скорочується з годин (традиційний ітеративний підбір) до менше секунди (вивід нейронної мережі), що відкриває можливості для статистичного аналізу великих масивів даних та оптимізації технологічних процесів модифікації матеріалів.
