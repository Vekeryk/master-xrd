{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Ultimate Model Comparison - Visual Analysis\n",
    "\n",
    "Comprehensive visual comparison of 4 training strategies:\n",
    "- Weighted vs Unweighted loss\n",
    "- Full curve vs Cropped curve\n",
    "\n",
    "**Goal:** Determine which strategy to use for 100k dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "import xrd\n",
    "import helpers as h\n",
    "from model_common import PARAM_NAMES, RANGES\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úì Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Load Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'ModelConfig' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load results from compare_models.py\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mcomparison_results.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     results = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m configs = results[\u001b[33m'\u001b[39m\u001b[33mconfigs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m metrics = results[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'ModelConfig' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# Load results from compare_models.py\n",
    "with open('comparison_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "configs = results['configs']\n",
    "metrics = results['metrics']\n",
    "predictions = results['predictions']\n",
    "metadata = results['metadata']\n",
    "X_true = results['X_true']\n",
    "\n",
    "model_names = [c.name for c in configs]\n",
    "print(f\"‚úì Loaded results for {len(model_names)} models\")\n",
    "print(f\"  Models: {model_names}\")\n",
    "print(f\"  Dataset size: {X_true.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Overall Error Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare overall error distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(PARAM_NAMES):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        abs_errors = metrics[model_name]['abs_errors'][:, i]\n",
    "        ax.hist(abs_errors, bins=50, alpha=0.5, label=model_name, density=True)\n",
    "    \n",
    "    ax.set_xlabel(f'{param} Absolute Error')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{param} Error Distribution')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compare_error_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: compare_error_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 2. MAE Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing MAE across parameters\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(PARAM_NAMES))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    mae_values = metrics[model_name]['mae']\n",
    "    offset = (i - len(model_names)/2 + 0.5) * width\n",
    "    bars = ax.bar(x + offset, mae_values, width, label=model_name, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2e}',\n",
    "                ha='center', va='bottom', fontsize=7, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Parameter', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mean Absolute Error (MAE)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('MAE Comparison Across All Parameters', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(PARAM_NAMES)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compare_mae_barchart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: compare_mae_barchart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 3. Win Rate Heatmap\n",
    "\n",
    "Shows which model performs best for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample, determine which model has lowest error\n",
    "n_samples = X_true.shape[0]\n",
    "n_params = len(PARAM_NAMES)\n",
    "\n",
    "# Calculate mean error per sample across all parameters\n",
    "sample_errors = np.zeros((n_samples, len(model_names)))\n",
    "for i, model_name in enumerate(model_names):\n",
    "    # Mean absolute error per sample (averaged across 7 parameters)\n",
    "    sample_errors[:, i] = np.mean(metrics[model_name]['abs_errors'], axis=1)\n",
    "\n",
    "# Find best model for each sample\n",
    "best_model_per_sample = np.argmin(sample_errors, axis=1)\n",
    "\n",
    "# Count wins per model\n",
    "win_counts = np.bincount(best_model_per_sample, minlength=len(model_names))\n",
    "win_percentages = win_counts / n_samples * 100\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(model_names, win_percentages, color=colors, alpha=0.8)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, count, pct in zip(bars, win_counts, win_percentages):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{pct:.1f}%\\n({count} samples)',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Win Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Sample-wise Win Rate: Which Model Produces Lowest Overall Error', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compare_win_rate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: compare_win_rate.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 4. Per-Parameter Win Rate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each parameter, count wins per model\n",
    "param_wins = np.zeros((n_params, len(model_names)))\n",
    "\n",
    "for param_idx in range(n_params):\n",
    "    param_errors = np.zeros((n_samples, len(model_names)))\n",
    "    \n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        param_errors[:, model_idx] = metrics[model_name]['abs_errors'][:, param_idx]\n",
    "    \n",
    "    best_per_sample = np.argmin(param_errors, axis=1)\n",
    "    param_wins[param_idx] = np.bincount(best_per_sample, minlength=len(model_names))\n",
    "\n",
    "# Convert to percentages\n",
    "param_wins_pct = param_wins / n_samples * 100\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(param_wins_pct, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(model_names)))\n",
    "ax.set_yticks(np.arange(n_params))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(PARAM_NAMES)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_params):\n",
    "    for j in range(len(model_names)):\n",
    "        text = ax.text(j, i, f'{param_wins_pct[i, j]:.1f}%',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "ax.set_title('Per-Parameter Win Rate Heatmap\\n(% of samples where model has lowest error)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "fig.colorbar(im, ax=ax, label='Win Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compare_param_win_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: compare_param_win_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® 5. Side-by-Side Curve Comparison for Same Samples\n",
    "\n",
    "Visually compare how different models predict the same curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison_for_sample(sample_idx, dl=100e-8):\n",
    "    \"\"\"\n",
    "    Compare all 4 models' predictions for a single sample.\n",
    "    Shows rocking curves and deformation profiles.\n",
    "    \"\"\"\n",
    "    true_params = X_true[sample_idx].numpy()\n",
    "    \n",
    "    # Generate true curve and profile\n",
    "    true_curve, true_profile = xrd.compute_curve_and_profile(true_params.tolist(), dl=dl)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(model_names) + 1, figsize=(20, 8))\n",
    "    \n",
    "    # Column 0: True\n",
    "    axes[0, 0].plot(true_curve.X_DeltaTeta, true_curve.Y_R_vseZ, 'k-', linewidth=2, label='True')\n",
    "    axes[0, 0].set_xlabel('ŒîŒò (arcsec)')\n",
    "    axes[0, 0].set_ylabel('Intensity')\n",
    "    axes[0, 0].set_title('TRUE\\nRocking Curve', fontweight='bold')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(true_profile.X, true_profile.total_Y, 'k-', linewidth=2, label='True')\n",
    "    axes[1, 0].set_xlabel('Depth (m)')\n",
    "    axes[1, 0].set_ylabel('Deformation')\n",
    "    axes[1, 0].set_title('TRUE\\nDeformation Profile', fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add true params\n",
    "    true_str = h.fparam(arr=true_params)\n",
    "    axes[1, 0].text(0.5, -0.25, f\"TRUE: {true_str}\",\n",
    "                    transform=axes[1, 0].transAxes, fontsize=7,\n",
    "                    verticalalignment='top', horizontalalignment='center',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3),\n",
    "                    family='monospace')\n",
    "    \n",
    "    # Columns 1-4: Model predictions\n",
    "    for col_idx, model_name in enumerate(model_names, start=1):\n",
    "        pred_params = predictions[model_name][sample_idx].numpy()\n",
    "        pred_curve, pred_profile = xrd.compute_curve_and_profile(pred_params.tolist(), dl=dl)\n",
    "        \n",
    "        # Rocking curve\n",
    "        axes[0, col_idx].plot(true_curve.X_DeltaTeta, true_curve.Y_R_vseZ, \n",
    "                              'k-', linewidth=2, alpha=0.3, label='True')\n",
    "        axes[0, col_idx].plot(pred_curve.X_DeltaTeta, pred_curve.Y_R_vseZ,\n",
    "                              color=colors[col_idx-1], linestyle='--', linewidth=2, label='Predicted')\n",
    "        axes[0, col_idx].set_xlabel('ŒîŒò (arcsec)')\n",
    "        axes[0, col_idx].set_ylabel('Intensity')\n",
    "        axes[0, col_idx].set_title(f'{model_name}\\nRocking Curve', fontweight='bold')\n",
    "        axes[0, col_idx].set_yscale('log')\n",
    "        axes[0, col_idx].legend(fontsize=8)\n",
    "        axes[0, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Deformation profile\n",
    "        axes[1, col_idx].plot(true_profile.X, true_profile.total_Y,\n",
    "                              'k-', linewidth=2, alpha=0.3, label='True')\n",
    "        axes[1, col_idx].plot(pred_profile.X, pred_profile.total_Y,\n",
    "                              color=colors[col_idx-1], linestyle='--', linewidth=2, label='Predicted')\n",
    "        axes[1, col_idx].set_xlabel('Depth (m)')\n",
    "        axes[1, col_idx].set_ylabel('Deformation')\n",
    "        axes[1, col_idx].set_title(f'{model_name}\\nDeformation Profile', fontweight='bold')\n",
    "        axes[1, col_idx].legend(fontsize=8)\n",
    "        axes[1, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = pred_params - true_params\n",
    "        rel_errors = errors / (np.abs(true_params) + 1e-12) * 100\n",
    "        mae = np.mean(np.abs(errors))\n",
    "        \n",
    "        # Add prediction info\n",
    "        pred_str = h.fparam(arr=pred_params)\n",
    "        info_text = f\"PRED: {pred_str}\\nMAE: {mae:.3e}\"\n",
    "        \n",
    "        axes[1, col_idx].text(0.5, -0.25, info_text,\n",
    "                              transform=axes[1, col_idx].transAxes, fontsize=7,\n",
    "                              verticalalignment='top', horizontalalignment='center',\n",
    "                              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3),\n",
    "                              family='monospace')\n",
    "    \n",
    "    plt.suptitle(f'Model Comparison - Sample #{sample_idx}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    filename = f'compare_sample_{sample_idx:05d}.png'\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "\n",
    "# Show comparison for a few random samples\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(n_samples, size=3, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    plot_model_comparison_for_sample(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 6. Best and Worst Cases per Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, find its best and worst predictions\n",
    "for model_name in model_names:\n",
    "    sample_errors = np.mean(metrics[model_name]['abs_errors'], axis=1)\n",
    "    \n",
    "    best_idx = np.argmin(sample_errors)\n",
    "    worst_idx = np.argmax(sample_errors)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  Best case:  sample #{best_idx:5d}, MAE = {sample_errors[best_idx]:.6e}\")\n",
    "    print(f\"  Worst case: sample #{worst_idx:5d}, MAE = {sample_errors[worst_idx]:.6e}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Plot best case\n",
    "    plot_model_comparison_for_sample(best_idx)\n",
    "    \n",
    "    # Plot worst case\n",
    "    plot_model_comparison_for_sample(worst_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 7. Error Correlation Between Models\n",
    "\n",
    "Do models fail on the same samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation of errors between models\n",
    "error_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "\n",
    "for i, model1 in enumerate(model_names):\n",
    "    errors1 = np.mean(metrics[model1]['abs_errors'], axis=1)\n",
    "    for j, model2 in enumerate(model_names):\n",
    "        errors2 = np.mean(metrics[model2]['abs_errors'], axis=1)\n",
    "        correlation = np.corrcoef(errors1, errors2)[0, 1]\n",
    "        error_matrix[i, j] = correlation\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(error_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(model_names)))\n",
    "ax.set_yticks(np.arange(len(model_names)))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(model_names)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(model_names)):\n",
    "        text = ax.text(j, i, f'{error_matrix[i, j]:.3f}',\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if abs(error_matrix[i, j]) > 0.5 else \"black\",\n",
    "                       fontweight='bold', fontsize=12)\n",
    "\n",
    "ax.set_title('Error Correlation Between Models\\n(Do they fail on the same samples?)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "fig.colorbar(im, ax=ax, label='Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compare_error_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: compare_error_correlation.png\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  High correlation (>0.9): Models fail on similar samples\")\n",
    "print(\"  Low correlation (<0.7): Models have different failure modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° 8. Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall ranking based on multiple metrics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üí° FINAL RECOMMENDATION FOR 100K TRAINING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "rankings = []\n",
    "for model_name in model_names:\n",
    "    avg_mae = np.mean(metrics[model_name]['mae'])\n",
    "    avg_mape = np.mean(metrics[model_name]['mape'])\n",
    "    max_mape = np.max(metrics[model_name]['mape'])\n",
    "    val_loss = metadata[model_name]['val_loss']\n",
    "    \n",
    "    # Calculate win rate\n",
    "    sample_errors = np.mean(metrics[model_name]['abs_errors'], axis=1)\n",
    "    wins = np.sum(sample_errors[:, None] <= sample_errors[None, :]) / n_samples\n",
    "    \n",
    "    rankings.append({\n",
    "        'model': model_name,\n",
    "        'avg_mae': avg_mae,\n",
    "        'avg_mape': avg_mape,\n",
    "        'max_mape': max_mape,\n",
    "        'val_loss': val_loss,\n",
    "    })\n",
    "\n",
    "# Sort by avg_mape (lower is better)\n",
    "rankings.sort(key=lambda x: x['avg_mape'])\n",
    "\n",
    "print(\"\\nRanking (by Average MAPE):\")\n",
    "print(\"-\"*100)\n",
    "for rank, r in enumerate(rankings, 1):\n",
    "    medal = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"  \"\n",
    "    print(f\"{medal} {rank}. {r['model']:<25} | Avg MAPE: {r['avg_mape']:>6.2f}% | \"\n",
    "          f\"Max MAPE: {r['max_mape']:>6.2f}% | Val Loss: {r['val_loss']:.6f}\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Recommendation\n",
    "best_model = rankings[0]['model']\n",
    "print(f\"\\nüéØ RECOMMENDED STRATEGY: {best_model}\")\n",
    "print(\"\\nReasons:\")\n",
    "print(\"  ‚úì Lowest average MAPE across all parameters\")\n",
    "print(\"  ‚úì Best validation loss\")\n",
    "print(\"  ‚úì Most consistent performance\")\n",
    "\n",
    "# Extract configuration\n",
    "if 'unweighted' in best_model:\n",
    "    print(\"\\n‚öôÔ∏è  Configuration for 100k training:\")\n",
    "    print(\"    WEIGHTED_TRAINING = False\")\n",
    "else:\n",
    "    print(\"\\n‚öôÔ∏è  Configuration for 100k training:\")\n",
    "    print(\"    WEIGHTED_TRAINING = True\")\n",
    "\n",
    "if 'full' in best_model:\n",
    "    print(\"    FULL_CURVE_TRAINING = True\")\n",
    "else:\n",
    "    print(\"    FULL_CURVE_TRAINING = False\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "if rankings[0]['model'].startswith('v3_unweighted'):\n",
    "    print(\"  ‚Ä¢ Unweighted loss performs better - current loss weights may be suboptimal\")\n",
    "    print(\"  ‚Ä¢ Consider: loss weights might need retuning OR unweighted is inherently better\")\n",
    "    \n",
    "if 'full' in rankings[0]['model']:\n",
    "    print(\"  ‚Ä¢ Full curve training shows improvement despite extra computation\")\n",
    "    print(\"  ‚Ä¢ The cropped region [50:701] may be losing important information\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Cropped training sufficient - cropped region [50:701] contains key features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 9. Loss Weights Analysis\n",
    "\n",
    "Investigate if loss weights are helping or hurting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighted vs unweighted for same training mode (full or cropped)\n",
    "LOSS_WEIGHTS = np.array([1.0, 1.2, 1.0, 1.0, 1.5, 2.0, 2.5])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚öñÔ∏è  LOSS WEIGHTS ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nCurrent loss weights: {LOSS_WEIGHTS}\")\n",
    "print(f\"Parameters:           {PARAM_NAMES}\")\n",
    "\n",
    "# Compare full curve: weighted vs unweighted\n",
    "print(\"\\nüìä FULL CURVE: Weighted vs Unweighted\")\n",
    "print(\"-\"*100)\n",
    "weighted_full_mae = metrics['v3_full']['mae']\n",
    "unweighted_full_mae = metrics['v3_unweighted_full']['mae']\n",
    "\n",
    "print(f\"{'Parameter':<10} {'Weighted':<15} {'Unweighted':<15} {'Difference':<15} {'Winner':<15}\")\n",
    "print(\"-\"*100)\n",
    "for i, param in enumerate(PARAM_NAMES):\n",
    "    diff = weighted_full_mae[i] - unweighted_full_mae[i]\n",
    "    winner = \"‚úì Unweighted\" if diff > 0 else \"‚úì Weighted\"\n",
    "    print(f\"{param:<10} {weighted_full_mae[i]:<15.6e} {unweighted_full_mae[i]:<15.6e} \"\n",
    "          f\"{diff:+15.6e} {winner:<15}\")\n",
    "\n",
    "# Compare cropped: weighted vs unweighted\n",
    "print(\"\\nüìä CROPPED: Weighted vs Unweighted\")\n",
    "print(\"-\"*100)\n",
    "weighted_crop_mae = metrics['v3']['mae']\n",
    "unweighted_crop_mae = metrics['v3_unweighted']['mae']\n",
    "\n",
    "print(f\"{'Parameter':<10} {'Weighted':<15} {'Unweighted':<15} {'Difference':<15} {'Winner':<15}\")\n",
    "print(\"-\"*100)\n",
    "for i, param in enumerate(PARAM_NAMES):\n",
    "    diff = weighted_crop_mae[i] - unweighted_crop_mae[i]\n",
    "    winner = \"‚úì Unweighted\" if diff > 0 else \"‚úì Weighted\"\n",
    "    print(f\"{param:<10} {weighted_crop_mae[i]:<15.6e} {unweighted_crop_mae[i]:<15.6e} \"\n",
    "          f\"{diff:+15.6e} {winner:<15}\")\n",
    "\n",
    "# Analysis\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Count how many parameters benefit from weights\n",
    "full_wins_weighted = np.sum(unweighted_full_mae > weighted_full_mae)\n",
    "crop_wins_weighted = np.sum(unweighted_crop_mae > weighted_crop_mae)\n",
    "\n",
    "print(f\"  Full curve:  Weighted wins {full_wins_weighted}/7 parameters\")\n",
    "print(f\"  Cropped:     Weighted wins {crop_wins_weighted}/7 parameters\")\n",
    "\n",
    "if full_wins_weighted < 4 and crop_wins_weighted < 4:\n",
    "    print(\"\\n  ‚ùå CONCLUSION: Loss weights are HURTING performance\")\n",
    "    print(\"     Recommendation: Use WEIGHTED_TRAINING = False for 100k training\")\n",
    "    print(\"\\n  üí° Possible reasons:\")\n",
    "    print(\"     ‚Ä¢ Current weights over-emphasize harder parameters at expense of easy ones\")\n",
    "    print(\"     ‚Ä¢ Weights create imbalanced gradients leading to suboptimal convergence\")\n",
    "    print(\"     ‚Ä¢ Natural loss balance (unweighted) works better for this problem\")\n",
    "    print(\"\\n  üîß If you want to try weighted training again:\")\n",
    "    print(\"     ‚Ä¢ Try smaller weight differences (e.g., [1.0, 1.1, 1.0, 1.0, 1.2, 1.3, 1.5])\")\n",
    "    print(\"     ‚Ä¢ Or use dynamic weighting based on validation performance\")\n",
    "else:\n",
    "    print(\"\\n  ‚úì CONCLUSION: Loss weights are HELPING performance\")\n",
    "    print(\"     Recommendation: Use WEIGHTED_TRAINING = True for 100k training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
