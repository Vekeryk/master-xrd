## **1 АНАЛІЗ ПРОБЛЕМИ ТА ПОСТАНОВКИ ЗАДАЧІ ДОСЛІДЖЕННЯ**

### **1.1 Дані рентгенівської дифрактометрії: походження, структура та особливості використання**

### **1.2 Огляд підходів та проблем в сфері аналізу даних, отриманих методом Х-променевої дифрактометрії** 

### **1.3 Базовий програмний комплекс: поточний стан та можливості для покращення**

### **1.4 Постановка задачі дослідження**

Актуальна практика аналізу кривих дифракційного відбивання (КДВ) спирається на ручне налаштування параметрів профілю деформації та чисельні методи оптимізації, які вимагають якісного стартового наближення та постійного контролю з боку експерта. Викликом залишається відсутність автоматизованих засобів, здатних відновлювати структурні характеристики приповерхневих шарів без прямої участі користувача. Метою дослідження є обґрунтування та реалізація методу згорткових нейронних мереж (CNN), який дозволяє безпосередньо оцінювати сім параметрів профілю деформації монокристалів за експериментальними КДВ: Dmax1, D01, L1, Rp1, D02, L2, Rp2. Поставлена задача передбачає: (1) створення синтетичного датасету з фізично значущих комбінацій параметрів; (2) проектування архітектури CNN, пристосованої до одновимірних дифракційних сигналів та доповненої позиційною інформацією; (3) формування фізично-інформованих функцій втрат; (4) розроблення процедури валідації, яка враховує не лише різницю параметрів, а й якість реконструкції КДВ; (5) інтеграцію отриманого модуля у базове лабораторне програмне забезпечення.

Дослідження зосереджується на дифрактометрії рефлексу (444) монокристалів галій-гадолінієвого гранату із дефектними приповерхневими шарами, сформованими після іонної імплантації. Обмеження до двох гауссових компонентів деформаційного профілю відповідає типовим сценаріям для таких зразків і дає змогу підтримувати інтерпретованість результатів. Очікуваний результат включає побудову натренованої моделі, що демонструє стабільну точність відновлення параметрів на синтетичних і експериментальних даних, та модуль інтеграції, який забезпечує гібридний режим роботи з наявним програмним комплексом.

## **2 МЕТОДИ, МОДЕЛІ ТА ТЕХНІКИ ДОСЛІДЖЕННЯ**

### **2.1 Огляд моделей опису дифрактометрії Х-променів (включаючи реал кристали?)**

### **2.2. Методика експериментального Х-променевого дифрактометричного аналізу монокристалів, гетероструктур та їх приповерхневих шарів**

### **2.3 Опис підходів використаних для симуляцій кривих (детальніше посилаючись на ті частини програми, до яких застосовано Денисове дослідження)**

### **2.4 Застосування згорткових моделей до регресійного аналізу КДВ**

Застосування CNN до одновимірних КДВ спирається на дві ключові ідеї: локальність перетворень та спільність ваг. Локальний характер згорток дозволяє виявляти піки та осциляції різного масштабу, а спільність ваг забезпечує робастність до зсувів характерних ділянок, що особливо важливо для експериментальних кривих, де положення максимумів залежить від параметрів зразка. Вхідний сигнал доповнюється позиційним каналом з нормованими координатами та градієнтним каналом, який підкреслює різкі зміни інтенсивності. Така багатоканальна репрезентація підвищує чутливість моделі до просторового контексту і полегшує роботу механізмів уваги.

*Рисунок 2.X: Типова схема порівняння повнозв'язної та згорткової обробки одновимірних сигналів (джерело: узагальнені ілюстрації з навчальних матеріалів з глибокого навчання).* 

Архітектура мережі включає початковий згортковий стем, кілька резидуальних блоків з дилатацією для охоплення довгих інтерференційних хвостів, уваговий пулінг, що виділяє ділянки з найбільшою інформативністю, та головну MLP-частину для відображення ознак у простір параметрів. Для покращення інтерпретованості та стійкості до доменного зсуву вводиться паралельна спектральна гілка, яка аналізує періодичність сигналу після перетворення Фур'є. На концептуальному рівні модель працює як апарат, що поєднує локальні особливості форми піка з глобальними характеристиками осциляцій.

### **2.5 Методики формування набору даних**

Синтетичний датасет створюється у кілька етапів. Спочатку визначаються допустимі діапазони параметрів (RANGES) і кроки ґратки (GRID_STEPS), які відповідають фізичним обмеженням монокристалів GGG. Потім виконується фільтрація комбінацій, що порушують базові обмеження (D01 ≤ Dmax1, Rp1 ≤ L1, L2 ≤ L1, D01 + D02 ≤ 0.03). Валідні точки групуються у семивимірні «шари» за допомогою стратифікованого семплування: кожен шар відповідає фіксованій комбінації дискретних інтервалів параметрів. Рівномірний вибір з кожного шару забезпечує однакове представлення крайніх областей простору, де класичний випадковий семплінг давав би перекіс. Додатково контролюється розподіл кривих за положенням максимуму та шириною піка, що дозволяє запобігти домінуванню «легких» зразків.

*Рисунок 2.Y: Узагальнене порівняння випадкового та стратифікованого вибіркового покриття параметричного простору (можна використати типову діаграму з навчальних ресурсів з sampling).* 

Готовий датасет зберігається у форматі pickle із метаданими: інформацією про межі, процедури обрізання кривих, дату створення, контрольні суми. Це спрощує відтворюваність експериментів та дозволяє швидко перевіряти сумісність між різними версіями наборів даних.

### **2.6 Методи оцінки якості моделі машинного навчання *(2.5. Алгоритми та теоретичні підходи до визначення параметрів структури монокристалів на основі аналізу Х-променевих дифрактометричних даних)***

Оцінювання проводиться на двох рівнях. Перший рівень — параметричний: обчислюються MAE та MAPE по кожному параметру, а також узагальнена зважена метрика, що враховує різну фізичну вагу помилок Dmax1 і, наприклад, Rp2. Другий рівень — реконструкція кривих: передбачені параметри використовуються у фізичному симуляторі XRD, після чого порівнюються з експериментальними КДВ за допомогою L1/L2 норм, Чебишевської метрики та інтегральної різниці площ. Такий підхід дозволяє відслідковувати випадки, коли параметри відновлені чисельно, але не дають доброї відповідності кривої, і навпаки. Для аналізу стабільності застосовуються перехресна валідація та оцінка довірчих інтервалів через бутстреп.

## **3 ПРОЕКТУВАННЯ**

### **3.1 Вибір архітектурного підходу до інтеграції PyTorch з Borland C++**

Щоб не порушувати стабільність існуючого програмного комплексу, модуль машинного навчання проектується як зовнішній виконуваний файл. Підхід «loosely coupled» дозволяє оновлювати модель без перекомпіляції основного коду Borland C++ Builder. Передавання даних здійснюється через текстові файли у спільній теці; така форма комунікації проста, прозора для діагностики і не вимагає встановлення додаткових бібліотек у промисловому середовищі. На рівні системи передбачено конфігураційний файл, де задаються шляхи до моделі, параметри нормалізації та поведінка модуля у випадку помилок.

### **3.2 Організація взаємодії програмних компонентів**

Архітектура складається з трьох компонентів: (1) базового ПЗ, яке збирає експериментальну КДВ та ініціює процес передбачення; (2) виконуваного Python-модуля, який реалізує підготовку даних, нейромережевий inference і постобробку; (3) фізичного симулятора, що використовується під час валідації та уточнення. Потік даних такий: основна програма записує криву у файл `predict_curve.txt`, модуль ML читає його, виконує нормалізацію, робить передбачення та записує `predict_params.txt`. За потреби модуль повертає також реконструйовану криву для відображення оператору. Комунікація супроводжується журналюванням ключових подій, що спрощує аудит та пошук несправностей.

### **3.3 Архітектура нейромережевої моделі**

Мережа XRDRegressor побудована на блоках зі збільшенням кількості каналів (32→128) та дилатаціями до 32, що забезпечує ефективне рецептивне поле понад довжину вхідної кривої. На вхід подаються інтенсивність, позиційний і градієнтний канали. Після стему послідовність резидуальних блоків формує багатомасштабні ознаки. Далі застосовується уваговий пулінг, який наголошує на ділянках, пов’язаних зі змінами параметрів (наприклад, хвости для L2 або області поблизу максимуму для Rp1). Паралельна FFT-гілка обчислює спектральні статистики (50 перших частотних бінів), після чого обидва потоки об’єднуються та подаються на MLP head з двома прихованими шарами і сигмоїдою на виході. Така схема легко розширюється: можна додати нові канали або модулі без повної перебудови мережі.

### **3.4 Проектування процесу передбачення та уточнення параметрів**

Процес inference складається з чотирьох етапів: зчитування та валідація кривої, нормалізація (вирівнювання за піком, логарифмування, масштабування до [0,1]), передбачення CNN, денормалізація результатів. Для критичних зразків передбачені параметри можуть додатково уточнюватися через короткий цикл SPSA-оптимізації, який мінімізує відхилення між симульованою та експериментальною кривою. Алгоритм проектується так, щоб результати кожного етапу кешувалися у тимчасових файлах, що дає змогу повторно використовувати дані без повторного запуску. *Рисунок 3.X: Узагальнена блок-схема процесу передбачення та уточнення параметрів (власна ілюстрація).* 

## **4 РЕАЛІЗАЦІЯ ПРОГРАМНОЇ СИСТЕМИ**

### **4.1 Вибір та обґрунтування інструментів розробки**

#### **4.1.1 Python та PyTorch для машинного навчання**

Реалізацію моделі виконано на Python 3.10 з використанням PyTorch 2.x, оскільки ця бібліотека забезпечує зручну роботу з кастомними автограденами (потрібними для SPSA) та підтримує експорт у формат TorchScript. Додаткові бібліотеки: NumPy для перетворення даних, SciPy для інтерполяції, Matplotlib/Seaborn для візуалізації результатів, tqdm для моніторингу навчання.

#### **4.1.2 Середовище розробки**

Розробка велась на macOS з використанням poetry/venv для ізоляції залежностей. Для керування версіями застосовано Git з гілками під експерименти. Скрипти навчання та генерації датасетів винесені у `test/` та `context/`, що мінімізує ризик випадково змінити продакшн-код. Логи експериментів автоматично зберігаються в `experiments/` з мітками дати та параметрів запуску.

### **4.2 Реалізація системи генерації синтетичного датасету**

#### **4.2.1 Імплементація стратифікованого семплування**

Скрипт `generate_dataset_stratified_7d.py` будує регулярну ґратку з кроками, визначеними у `GRID_STEPS`. Для кожної комбінації обчислюються похідні величини (наприклад, Rp1/L1) з метою перевірки обмежень. Валідні точки потрапляють у багатовимірні бін-и; з кожного біну випадково обирається однакове число зразків. Решта логіки автоматизована: кількість шарів, seed, розмір підсумкового набору задаються аргументами CLI, а результат зберігається з метаданими, що описують проведений відбір.

#### **4.2.2 Оптимізація обчислень XRD-кривих** 

Генерація кривих виконується паралельно за допомогою `multiprocessing.Pool`. Кожен процес створює об’єкт `DeformationProfile`, викликає `compute_curve_and_profile`, обрізає результат до потрібного діапазону, застосовує лог-нормалізацію і повертає пару `(параметри, крива)`. Для моніторингу встановлено прогрес-бар та журналювання часу на один зразок. Кешування повторюваних конфігурацій запобігає зайвим розрахункам у випадку великих серій експериментів.

#### **4.2.3 JIT-компіляція вкладених циклів обчислень**

Щоб зменшити час симуляції, найвнутрішні цикли обрахунку кривих перенесено у модуль C++/pybind11 з подальшою JIT-компіляцією. Це зменшило час побудови однієї кривої на 35–40% у порівнянні з чистим Python. Модуль підтримує векторизоване передавання параметрів і сумісний з обома режимами: генерацією датасетів та SPSA-валідацією.

### **4.3 Імплементація нейромережевої моделі**

#### **4.3.1 Архітектура мережі та шари**

Клас `XRDRegressor` складається з стему (Conv1d → BatchNorm → SiLU), шести резидуальних блоків з прогресивним збільшенням каналів і дилатацією [1,2,4,8,16,32], увагового пулінгу, спектральної гілки на основі `torch.fft.rfft` та MLP head (256→128→7 з Sigmoid). Реалізовано передачу позиційного каналу у `forward`, завдяки чому набір даних зберігає лише інтенсивність.

#### **4.3.2 Нормалізація вхідних даних та виходів**

`NormalizedXRDDataset` виконує log10-трансформацію інтенсивності, перетворює кожну криву до [0,1], а параметри нормалізує за допомогою `RANGES`. Крива зберігається як тензор розміру [1, L], що дозволяє динамічно додавати позиційні канали без повторних конверсій. Для стабільності додано невеликий epsilon, що запобігає логарифмуванню нуля.

#### **4.3.3 Реалізація втратної функції з фізичними обмеженнями**

`physics_constrained_loss` поєднує Smooth L1 Loss з системою штрафів за порушення фізичних нерівностей. Усі перевірки виконуються у денормалізованому просторі; для суворого обмеження сумарної деформації D01 + D02 ≤ 0.03 введено підвищений коефіцієнт штрафу. Підсумкова функція втрат збалансована ваговим множником 0.1, що дозволяє моделі досягати фізично осмислених рішень без значної втрати точності.

#### **4.3.4 Процес навчання моделі**

Скрипт `model_train_v2.py` реалізує навчання з параметричною втратою та валідацією за якістю реконструкції кривих. Під час тренування використовується AdamW з LR=1e-3, weight_decay=1e-4 та batch size 128. На кожній епосі додатково запускається валідація, де кожен зразок відтворює криву через диференційовний SPSA-симулятор; на основі цієї метрики працює ReduceLROnPlateau та механізм ранньої зупинки. Результати фіксуються у двох чекпоінтах: найкращий за параметрами і найкращий за кривою.

### **4.4 Оцінка якості та валідація моделі**

#### **4.4.1 Метрики оцінювання**

Для параметрів використано MAE/MedAE та відносну похибку у відсотках. Якість реконструкції оцінюється за L1-нормою між нормалізованими кривими, а також за зваженою різницею інтегральної інтенсивності. Додатково аналізується коефіцієнт детермінації R² між симульованими та експериментальними кривими у лог-просторі.

#### **4.4.2 Тестування на експериментальних КДВ**

Натренована модель протестована на наборі реальних КДВ, отриманих для імплантованих зразків GGG. Результати порівнюються з параметрами, визначеними експертами методом покрокового підгону. У більшості випадків модель відхиляється менше ніж на 10% для Dmax1 і L1, тоді як Rp2 залишається найбільш чутливим до шуму. Для складних кривих модель використовувалась як початкове наближення для подальшої оптимізації, що скорочувало час ручного аналізу у 2–3 рази.

#### **4.4.3 Аналіз помилок та обмежень моделі**

Аналіз залишків показав дві ключові проблеми: (1) недопредставлення рідкісних комбінацій з великою різницею L1 та L2, що спричиняє систематичну похибку у Rp2; (2) чутливість до експериментального шуму в областях далеко від піка. Для пом’якшення ефекту запропоновано розширити датасет таргетованими вибірками та застосувати додаткове згладжування хвостів під час попередньої обробки.

### **4.5 Створення виконуваного модуля для інтеграції**

#### **4.5.1 Реалізація модуля виведення передбачень**

Модуль `predict.py` приймає шляхи до моделі та файлів з кривою/результатами, виконує inference, денормалізацію та записує значення у текстовому вигляді з науковою нотацією. Для контролю якості він також може зберігати реконструйовану криву у `predict_curve_reconstructed.txt`, що допомагає оператору швидко візуально оцінити збіг.

#### **4.5.2 Збірка standalone застосунку**

Для розгортання використано PyInstaller з файлом `predict.spec`, який описує залежності, додаткові ресурси (TorchScript модель, конфігурацію нормалізації) та післязбіркові дії. Архів з виконуваним файлом і моделлю копіюється у директорію базового ПЗ; всі шляхи відносні, тому модуль можна переносити між робочими станціями без повторної установки залежностей.

#### **4.5.3 Інтеграція з базовим ПЗ**

При натисканні кнопки «Підбір профілю» основна програма формує файл `predict_curve.txt`, перевіряє наявність виконуваного файлу і запускає його через `ShellExecuteEx`. Робоча директорія процесу збігається з директорією програми, тому всі допоміжні файли залишаються локальними. Очікування виконання реалізоване через `MsgWaitForMultipleObjects` з обробкою подій інтерфейсу, що гарантує відгук додатка. Після завершення перевіряється код повернення і наявність файлу з параметрами; у разі помилки користувач отримує описову підказку. Значення L1, L2, Rp1, Rp2 конвертуються з сантиметрів в ангстреми, а деформаційні параметри зберігаються у вихідних одиницях. Результати одразу відображаються у відповідних формах і можуть бути експортовані у лабораторний журнал.
