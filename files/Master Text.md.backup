## **1 АНАЛІЗ ПРОБЛЕМИ ТА ПОСТАНОВКИ ЗАДАЧІ ДОСЛІДЖЕННЯ**

### **1.1 Дані рентгенівської дифрактометрії: походження, структура та особливості використання**

### **1.2 Огляд підходів та проблем в сфері аналізу даних, отриманих методом Х-променевої дифрактометрії** 

### **1.3 Базовий програмний комплекс: поточний стан та можливості для покращення**

### **1.4 Постановка задачі дослідження**

Аналіз сучасного стану методів визначення структурних параметрів дефектних приповерхневих шарів монокристалів виявляє фундаментальну проблему обернених задач дифрактометрії: відсутність ефективних алгоритмів автоматизованого визначення параметрів профілю деформації з експериментальних КДВ. Традиційні методи ітераційної оптимізації вимагають якісного початкового наближення та експертного втручання для уникнення локальних мінімумів функції відхилення між експериментальними та теоретичними кривими. Це робить процес аналізу трудомістким, суб'єктивним та залежним від досвіду оператора. Основною метою дослідження є розробка та верифікація методології застосування згорткових нейронних мереж для автоматизованого визначення семи параметрів профілю деформації приповерхневих шарів монокристалів безпосередньо з експериментальних КДВ.   
Параметри профілю включають максимальну деформацію асиметричного гауссового компонента (Dmax1), поверхневу деформацію асиметричного компонента (D01), товщину порушеного шару асиметричного компонента (L1), положення максимуму деформації асиметричного компонента (Rp1), поверхневу деформацію спадного гауссового компонента (D02), товщину порушеного шару спадного компонента (L2) та положення максимуму деформації спадного компонента (Rp2). Для досягнення поставленої мети необхідно розв'язати наступні задачі. По-перше, розробити методику генерації синтетичного навчального датасету на основі фізичного моделювання дифракції рентгенівських променів у кристалах з неідеальною структурою, що забезпечує рівномірне покриття семивимірного параметричного простору з дотриманням фізичних обмежень на комбінації параметрів. По-друге, спроектувати та реалізувати архітектуру згорткової нейронної мережі, адаптовану для регресійного аналізу одновимірних дифракційних сигналів з урахуванням специфіки КДВ: наявності осциляційних паттернів різної періодичності та критичної важливості просторової локалізації особливостей кривої. По-третє, імплементувати фізично-інформовану функцію втрат, що забезпечує дотримання нейромережевою моделлю фізичних обмежень на параметри деформації кристалічної гратки. По-четверте, розробити методику валідації натренованої моделі не лише через порівняння передбачених параметрів зі справжніми значеннями, але й через оцінку якості реконструкції експериментальних КДВ з передбачених параметрів. По-п'яте, реалізувати інтеграцію натренованої моделі у існуюче програмне забезпечення для аналізу дифракційних даних з можливістю гібридного використання: нейромережеве передбачення як початкове наближення для подальшого уточнення традиційними методами оптимізації.   
Дослідження обмежується аналізом бездисперсійних КДВ від рефлексу (444) монокристалів галій-гадолінієвого ґранату з дефектними приповерхневими шарами, утворених внаслідок іонної імплантації. Розглядається випадок симетричної геометрії дифракції з використанням монохроматизованого CuKα₁ випромінювання. Параметризація профілю деформації обмежена двокомпонентною моделлю з асиметричним та спадним гауссовими профілями, що адекватно описує типові розподіли дефектів у імплантованих шарах.   
Очікуваними результатами дослідження є натренована нейромережева модель, здатна визначати параметри профілю деформації з точністю, достатньою для використання як стартового наближення у традиційних методах оптимізації; програмна реалізація модуля машинного навчання, інтегрованого у базове програмне забезпечення через механізм міжпроцесної взаємодії; методологія оцінки якості передбачень через фізичну валідацію на основі реконструкції КДВ; та кількісна оцінка переваг гібридного підходу порівняно з суто традиційними методами аналізу.

## **2 МЕТОДИ, МОДЕЛІ ТА ТЕХНІКИ ДОСЛІДЖЕННЯ**

### **2.1 Огляд моделей опису дифрактометрії Х-променів (включаючи реал кристали?)**

### **2.2. Методика експериментального Х-променевого дифрактометричного аналізу монокристалів, гетероструктур та їх приповерхневих шарів**

### **2.3 Опис підходів використаних для симуляцій кривих (детальніше посилаючись на ті частини програми, до яких застосовано Денисове дослідження)**

### **2.4 Застосування згорткових моделей до регресійного аналізу КДВ**

Визначення структурних параметрів монокристалічних зразків з дефектними приповерхневими шарами на основі експериментальних кривих дифракційного відбивання є складною обернененою задачею, розв'язання якої традиційними методами вимагає значних обчислювальних ресурсів та експертного втручання. Застосування методів глибокого навчання, зокрема згорткових нейронних мереж, відкриває нові можливості для автоматизованого аналізу рентгенівських дифракційних даних. У даному розділі розглядаються теоретичні основи використання CNN-архітектур для регресійного визначення параметрів профілю деформації безпосередньо з форми експериментальних КДВ.  
Криві дифракційного відбивання є одновимірними сигналами, що характеризуються наявністю осциляційних патернів, положення та амплітуда яких визначаються структурними параметрами приповерхневого шару. Згорткові нейронні мережі, традиційно застосовувані для обробки зображень, природним чином адаптуються до аналізу одновимірних сигналів завдяки фундаментальним властивостям згорткової операції. Ключова перевага згортки полягає у локальній рецептивності та інваріантності до зсувів. Локальна рецептивність означає, що кожен нейрон обробляє лише обмежену ділянку вхідного сигналу, що дозволяє виявляти локальні особливості КДВ: піки, осциляції, зміни нахилу інтенсивності. Інваріантність до зсувів забезпечує здатність мережі розпізнавати ті самі паттерни незалежно від їхнього положення на кривій, що критично важливо для КДВ, де характерні особливості можуть знаходитись у різних кутових діапазонах залежно від параметрів зразка. На відміну від повнозв'язних мереж (MLP), де кожен параметр незалежно з'єднаний з усіма точками вхідної кривої, згорткові шари використовують механізм розподілу ваг (weight sharing). Це радикально зменшує кількість параметрів моделі та забезпечує можливість навчання на обмежених обсягах даних, що особливо актуально для задач фізичного моделювання, де генерація навчальних прикладів є обчислювально затратною. 

*\[Рисунок 2.X: Порівняння архітектур MLP та 1D CNN для обробки КДВ. Ілюстрація демонструє різницю у зв'язності шарів та розмірі параметричного простору\]*  
Типово одновимірний сигнал подається у мережу як одноканальний тензор, де єдиний канал містить значення інтенсивності. Однак для задач, де критично важливою є позиційна інформація, доцільно збагатити вхідне представлення додатковими каналами. Позиційний канал, що містить нормалізовані координати вздовж осі сигналу, надає мережі явну інформацію про "де" знаходиться кожна точка кривої. Це особливо корисно для attention механізмів, які можуть навчитись асоціювати певні діапазони позицій з певними значеннями параметрів. Градієнтний канал (похідна інтенсивності) виявляє ділянки різких змін на кривій, що часто відповідають фізичним границям шарів або зонам максимальної деформації. 

*\[Рисунок 2.W: Багатоканальне представлення КДВ. Візуалізація трьох каналів: інтенсивність (log-нормалізована), позиційна координата, градієнт інтенсивності\]*   
Нормалізація вхідних даних відіграє критичну роль у стабільності та швидкості навчання. Для КДВ, інтенсивність яких змінюється на декілька порядків величини, застосування логарифмічного перетворення з подальшою нормалізацією до діапазону \[0,1\] забезпечує рівномірне представлення як високоінтенсивних піків, так і низькоінтенсивних осциляцій у хвості кривої.

### **2.5 Методики формування набору даних**

Ефективність застосування методів машинного навчання до задач регресійного аналізу критично залежить від якості та репрезентативності навчальних даних. У випадку аналізу кривих дифракційного відбивання, де отримання великої кількості експериментальних зразків є технічно складним та часозатратним процесом, доцільним підходом є формування синтетичного навчального датасету на основі фізичного моделювання дифракції. У даному розділі розглядаються теоретичні аспекти генерації синтетичних КДВ, методи семплування багатовимірного параметричного простору та стратегії забезпечення репрезентативності навчальної множини.  
Генерація синтетичних кривих дифракційного відбивання базується на розв'язанні прямої задачі дифракції рентгенівського випромінювання у кристалах з неідеальною структурою. Пряма задача полягає у розрахунку інтенсивності когерентного та дифузного розсіяння для заданого профілю деформації кристалічної гратки у приповерхневому шарі. На відміну від експериментальних вимірювань, де параметри деформації є невідомими величинами, що підлягають визначенню, у синтетичній генерації параметри задаються явно, а відповідна КДВ розраховується числовими методами. Ключовою перевагою синтетичних даних є повний контроль над розподілом параметрів та можливість генерації великих обсягів навчальних прикладів, що покривають весь діапазон фізично можливих конфігурацій. Водночас виникає проблема sim-to-real gap — розбіжності між синтетичними кривими та реальними експериментальними даними, зумовленої неповнотою фізичної моделі, інструментальними ефектами та шумами вимірювання. Мінімізація цього розриву вимагає включення реалістичних шумових компонент та врахування особливостей експериментальної установки у процесі генерації.  
Профіль деформації у приповерхневому шарі монокристала параметризується через комбінацію асиметричної та спадної гауссових компонент, що разом визначаються сімома параметрами: Dmax1, D01, L1, Rp1, D02, L2, Rp2. Кожен параметр має фізично обґрунтований діапазон допустимих значень, визначений з врахуванням граничних деформацій кристалічної гратки та типових характеристик дефектних шарів, утворених іонною імплантацією. Повна сітка параметрів у семивимірному просторі призводить до комбінаторного вибуху: навіть при скромній дискретизації кожної осі на 10 значень отримуємо 10^7 \= 10 мільйонів комбінацій. Це є непрактичним як з обчислювальної точки зору (генерація однієї КДВ займає \~0.1-1 секунду), так і з точки зору зберігання даних (датасет з мільйонів кривих по 1000 точок кожна вимагає терабайтів пам'яті). Альтернативним підходом є випадкове семплування (random sampling), але воно не гарантує рівномірного покриття параметричного простору та може залишити "порожні" регіони, критичні для узагальнення моделі. Стратифіковане семплування (stratified sampling) забезпечує компроміс між обчислювальною ефективністю та повнотою покриття: простір параметрів розбивається на стратифіковані регіони, з кожного з яких вибираються представники з гарантованою мінімальною густиною.   
*\[Рисунок 2.X: Порівняння стратегій семплування у двовимірному проєкційному просторі (L1, Rp1). Візуалізація grid sampling, random sampling та stratified sampling\]*  
Діапазони варіювання параметрів визначаються з урахуванням фізичних обмежень та типових характеристик експериментальних зразків. Максимальна деформація Dmax1 обмежена граничними значеннями пружної деформації кристалічної гратки (0.001-0.030), за межами яких відбувається пластична релаксація або утворення дислокацій. Товщини порушених шарів L1 та L2 типово знаходяться у діапазоні 1000-7000 Å, що відповідає характерним глибинам проникнення іонів при імплантації з енергіями 10-100 keV. Критичним аспектом є врахування взаємозалежностей між параметрами. Фізично некоректні комбінації, такі як положення максимуму деформації поза межами порушеного шару (Rp1 \> L1) або поверхнева деформація, що перевищує максимальну (D01 \> Dmax1), мають бути виключені з навчальної множини. Включення таких зразків призводить до навчання моделі на нефізичних конфігураціях, що погіршує її здатність коректно передбачувати параметри реальних експериментальних кривих. Крок дискретизації для кожного параметра визначається компромісом між щільністю покриття та обчислювальною складністю. Занадто великий крок призводить до розрідженої сітки, де мережа не навчається розрізняти близькі значення параметрів. Занадто малий крок генерує надлишкові приклади з мінімальними відмінностями у КДВ, що неефективно використовує обчислювальні ресурси. Типово крок вибирається таким, щоб різниця між сусідніми значеннями параметра була помітною у формі відповідних КДВ — наприклад, зміна товщини шару на 500 Å призводить до зсуву періоду осциляцій, що чітко ідентифікується візуально та алгоритмічно.  
Стратифіковане семплування забезпечує рівномірний розподіл навчальних прикладів по всьому параметричному простору при фіксованому обсязі датасету. Простір параметрів розбивається на гіперкуби (strata) фіксованого розміру, і з кожного гіперкуба випадково вибирається один представник. Це гарантує, що кожен регіон простору представлений у навчальній множині, уникаючи як кластеризації прикладів (характерної для випадкового семплування), так і надмірної регулярності (характерної для grid sampling). Розмір страт визначає баланс між різноманітністю (малі страти — більше різних комбінацій) та щільністю покриття (великі страти — менше "пропусків"). Для семивимірного простору типовий розмір страти становить \~5-10% діапазону кожного параметра, що при загальному обсязі 100 000 зразків забезпечує достатню щільність для навчання глибоких мереж.   
*\[Рисунок 2.Y: Стратифіковане розбиття параметричного простору. Схематичне зображення 3D проєкції на простір (Dmax1, L1, L2) з виділеними стратами\]*   
Важливим аспектом є обробка краєвих ефектів біля меж фізично допустимих регіонів. Наприклад, комбінації з D01 близьким до Dmax1 формують вузьку "діагональну" область у просторі (D01, Dmax1), тоді як комбінації з D01 \<\< Dmax1 заповнюють значно більший обсяг. Наївне рівномірне семплування призведе до недопредставлення граничних випадків, тоді як стратифікований підхід гарантує їхню присутність у тренувальній вибірці.  
Поряд з основним навчальним датасетом доцільно формувати спеціалізовані target datasets — невеликі множини синтетичних КДВ з відомими "складними" комбінаціями параметрів, що представляють граничні випадки або типові експериментальні конфігурації. Такі датасети слугують для цілеспрямованої оцінки здатності моделі узагальнювати на специфічні сценарії. Наприклад, target датасет може містити виключно конфігурації з максимальною деформацією (Dmax1 близьке до 0.030), що зустрічаються у сильно пошкоджених зразках після високодозової імплантації. Або конфігурації з глибоким заляганням максимуму деформації (Rp2 близьке до \-6000 Å), що відповідають імплантації високоенергетичними іонами. Систематичне тестування на таких target множинах дозволяє виявити слабкі місця моделі ще на етапі розробки та цілеспрямовано збільшити представленість проблемних регіонів у тренувальній множині.   
Додатково target датасети використовуються для налагодження pipeline обробки даних: перевірки коректності нормалізації, виявлення артефактів у згенерованих кривих, валідації denormalization процедур. Оскільки параметри target зразків відомі точно (на відміну від експериментальних даних), можна безпосередньо порівнювати передбачення моделі з ground truth та обчислювати детальну статистику помилок по кожному параметру окремо.  
Ідеалізовані КДВ, розраховані чисто з фізичної моделі, не містять шумів та інструментальних артефактів, характерних для реальних експериментальних вимірювань. Навчання моделі виключно на таких "чистих" кривих призводить до зниження точності при роботі з реальними даними через sim-to-real gap. Для підвищення робасності моделі у синтетичні криві вносяться штучні шумові компоненти, що імітують основні типи експериментальних шумів. Фотонний шум (shot noise) моделюється як пуассонівський процес з інтенсивністю, пропорційною до значення сигналу. Фоновий шум додається як адитивна гауссова компонента з постійною дисперсією. У низькоінтенсивній ділянці кривої (хвіст) застосовується процедура згладжування до порогового рівня шуму, що відповідає межі детекції експериментального дифрактометра.   
*\[Рисунок 2.Z: Порівняння ідеалізованої та зашумленої синтетичної КДВ. Демонстрація впливу різних типів шумів на форму кривої\]*   
Важливим є баланс між реалістичністю шумів та інформативністю сигналу. Надмірний шум маскує тонкі особливості КДВ, критичні для визначення деяких параметрів, тоді як недостатній шум не готує модель до роботи з реальними експериментальними даними. Рівень шумів калібрується на основі характеристик конкретної експериментальної установки та типових умов вимірювання.  
Визначення достатнього обсягу навчальної множини є нетривіальною задачею, що залежить від складності архітектури моделі, розмірності параметричного простору та бажаної точності передбачень. Емпіричні дослідження у суміжних областях (регресія параметрів з спектральних даних) вказують на необхідність 10^3-10^5 прикладів для навчання глибоких CNN на задачах з 5-10 вихідними параметрами. Для семивимірного простору параметрів профілю деформації типовий обсяг навчальної множини становить 50 000-100 000 синтетичних КДВ. Це забезпечує достатню щільність покриття (\~10-20 прикладів на кожну комірку стратифікованої сітки) для навчання моделей з 10^5-10^6 параметрів без критичного перенавчання. Валідаційна множина типово становить 10-20% загального обсягу, що дає 10 000-20 000 зразків для надійної оцінки узагальнювальної здатності моделі. Генерація такого обсягу даних вимагає значних обчислювальних ресурсів: при \~0.5 секунди на одну криву, 100 000 зразків потребують \~14 годин обчислень на одному ядрі процесора. Це робить актуальною паралелізацію процесу генерації на багатоядерних системах або кластерах. Зберігання датасету також потребує оптимізації: 100 000 кривих по 1000 точок у форматі float32 займають \~400 MB у стислому бінарному форматі, що є прийнятним для сучасних систем.

### **2.6 Методи оцінки якості моделі машинного навчання *(2.5. Алгоритми та теоретичні підходи до визначення параметрів структури монокристалів на основі аналізу Х-променевих дифрактометричних даних)***

## **3 ПРОЕКТУВАННЯ**

### **3.1 Вибір архітектурного підходу до інтеграції PyTorch з Borland C++**

Одним із ключових завдань проектування системи є забезпечення інтеграції сучасної нейромережевої моделі, розробленої з використанням фреймворку PyTorch, з існуючим програмним комплексом для аналізу рентгенівських кривих дифракційного відбивання, реалізованим на платформі Borland C++. Дана інтеграція має забезпечити ефективну взаємодію між компонентами системи при збереженні функціональності базового програмного забезпечення та можливості його подальшого розвитку. Враховуючи технологічну різнорідність компонентів та специфічні обмеження застарілого середовища розробки, необхідно здійснити обґрунтований вибір архітектурного підходу серед наявних альтернатив.   
Першим варіантом інтеграції є використання бібліотеки LibTorch – C++ API фреймворку PyTorch, яка дозволяє безпосередньо завантажувати та виконувати натреновані моделі у C++ додатках. Цей підхід передбачає експорт моделі у формат TorchScript та її завантаження через відповідні бібліотеки безпосередньо в код на C++. Однак даний варіант вимагає сумісності компілятора, наявності сучасних стандартів C++ (C++14 або новіше) та можливості лінкування зовнішніх бібліотек, що є проблематичним для застарілого середовища Borland C++, яке підтримує лише застарілі стандарти мови.   
Другим можливим підходом є організація міжпроцесної взаємодії (IPC) через механізми операційної системи, такі як named pipes, shared memory або socket-з'єднання. У цьому випадку ML-компонента виконується як окремий процес на Python, а взаємодія відбувається через обмін даними у реальному часі. Даний підхід забезпечує повну ізоляцію компонентів та незалежність від особливостей компіляторів, проте потребує складної синхронізації процесів, обробки помилок комунікації та може призводити до зниження продуктивності через накладні витрати на серіалізацію/десеріалізацію даних.   
Третім варіантом є створення автономного виконуваного модуля, який інкапсулює Python-інтерпретатор, PyTorch та натреновану модель в єдиний exe-файл за допомогою інструментів для статичного пакування на кшталт PyInstaller. Взаємодія з базовим ПЗ здійснюється через файлову систему: основна програма записує вхідні дані (експериментальну криву) у файл, викликає ML-модуль як зовнішній процес, який зчитує дані, виконує передбачення та записує результати у вихідний файл. Цей підхід забезпечує максимальну незалежність компонентів, простоту розгортання та відсутність вимог до модифікації існуючого коду на Borland C++, хоча й вносить певні затримки через операції вводу-виводу. 

### **3.2 Організація взаємодії програмних компонентів**

Зважаючи на технічні обмеження існуючого програмного комплексу, розробленого у застарілому середовищі Borland C++, та необхідність мінімізації втручання в його кодову базу, обрано архітектурний підхід на основі автономного виконуваного модуля з файловою взаємодією. Даний вибір обґрунтовується простотою імплементації, відсутністю потреби у складних механізмах синхронізації, можливістю незалежного тестування та оновлення ML-компоненти, а також сумісністю з будь-якими версіями компіляторів C++. Незначні накладні витрати на операції читання/запису файлів є прийнятними у контексті часу, необхідного для виконання передбачення нейромережевою моделлю, що зазвичай становить порядок десятків мілісекунд.  
Архітектура програмного комплексу базується на принципі слабкого зв'язування компонентів через механізм міжпроцесної взаємодії. Головна програма, розроблена в середовищі C++ Builder, виступає координуючим компонентом, що управляє процесом аналізу та забезпечує інтерфейс користувача. Компонент машинного навчання реалізується як автономний модуль з чітко визначеним інтерфейсом вхідних та вихідних даних. Взаємодія між компонентами здійснюється через файловий обмін даними.   
Головна програма формує структуроване представлення експериментальної кривої дифракційного відбивання у текстовому файлі, що містить послідовність значень інтенсивності. Модуль машинного навчання запускається як окремий процес з передачею шляхів до натренованої моделі, файлу вхідних даних та файлу результатів. Синхронізація здійснюється через очікування завершення дочірнього процесу з періодичною обробкою подій для підтримки відгуку користувацького інтерфейсу.   
	Результати роботи нейромережевого модуля представляються у вигляді текстового файлу з семи параметрів профілю деформації (Dmax1, D01, L1, Rp1, D02, L2, Rp2). Головна програма здійснює читання цих значень з подальшою конвертацією одиниць вимірювання та передачею в модуль розрахунку теоретичних кривих дифракційного відбивання. Модульна структура забезпечує можливість незалежного вдосконалення алгоритмів машинного навчання та інтеграції додаткових аналітичних модулів без модифікації основного програмного коду.

### **3.3 Архітектура нейромережевої моделі**

Для ефективного вилучення інформації з КДВ архітектура згорткової мережі має забезпечувати достатнє рецептивне поле, здатність виявляти як локальні, так і глобальні особливості сигналу, та стабільність навчання глибоких моделей. Residual connections (залишкові з'єднання) дозволяють будувати глибші мережі без деградації градієнтів. Замість навчання прямого відображення F(x), residual блок навчає залишкову функцію F(x) \= H(x) \- x, де H(x) є бажаним відображенням. Це забезпечує можливість градієнтам проходити крізь багато шарів без затухання, що критично важливо для навчання моделей з десятками шарів. У контексті аналізу КДВ, residual з'єднання дозволяють мережі поступово уточнювати представлення сигналу, виявляючи дедалі більш абстрактні фізичні залежності між формою кривої та параметрами деформації. Dilated convolutions (дилатовані згортки) розширюють рецептивне поле без збільшення кількості параметрів. Дилатація d означає, що між елементами згорткового ядра вставляються d-1 пропусків, що дозволяє одному шару "бачити" більшу область сигналу. Прогресивне збільшення дилатації (1, 2, 4, 8, ...) створює експоненційно зростаюче рецептивне поле, забезпечуючи здатність мережі виявляти як короткоперіодичні осциляції (пов'язані з товщиною шару), так і довгоперіодичні зміни інтенсивності (пов'язані з глибиною залягання дефектів).   
*\[Рисунок 2.Y: Принцип роботи дилатованих згорток. Візуалізація рецептивного поля для стандартних та дилатованих згорток з різними коефіцієнтами дилатації\]*   
Attention mechanisms (механізми уваги) дозволяють мережі динамічно визначати найбільш інформативні ділянки вхідного сигналу. Традиційний global average pooling усереднює всі просторові позиції з однаковими вагами, втрачаючи інформацію про те, які саме ділянки КДВ є найбільш значущими для визначення конкретного параметра. Attention-based pooling навчається обчислювати важливість кожної позиції, присвоюючи більші ваги тим регіонам кривої, що містять найбільше інформації про параметри профілю деформації. Це особливо критично для позиційних параметрів (Rp1, Rp2), значення яких безпосередньо пов'язані з розташуванням певних особливостей на кривій.  
Криві дифракційного відбивання від багатошарових структур містять осциляційні паттерни, період яких фізично пов'язаний з товщиною шарів. Ця залежність описується співвідношенням Δθ ∝ λ/L, де Δθ — період осциляцій, λ — довжина хвилі рентгенівського випромінювання, L — товщина шару. Перетворення Фур'є дозволяє безпосередньо виявити ці періодичності у частотній області, що комплементарно до просторового аналізу згортковими шарами. Комбінування просторових (CNN) та спектральних (FFT) ознак у гібридній архітектурі дозволяє мережі одночасно використовувати обидва типи інформації: просторові шари виявляють локальні особливості форми кривої, тоді як спектральна гілка ідентифікує глобальні осциляційні паттерни. Така multi-view архітектура забезпечує більш повне представлення фізичних взаємозв'язків між структурою зразка та формою КДВ.   
*\[Рисунок 2.Z: Гібридна архітектура з просторовою (CNN) та спектральною (FFT) гілками. Схематичне зображення потоку даних від вхідної КДВ до фінального регресійного шару\]*  
Для задачі регресії параметрів з одновимірних сигналів існує декілька альтернативних архітектурних підходів, кожен з яких має певні переваги та обмеження у контексті аналізу КДВ. Повнозв'язні мережі (MLP) забезпечують максимальну гнучкість відображення, але вимагають великої кількості параметрів та схильні до перенавчання на обмежених датасетах. Відсутність індуктивного bias щодо структури сигналу означає, що MLP має навчитись усім просторовим залежностям з даних, що вимагає експоненційно більших обсягів навчальних прикладів. Рекурентні мережі (RNN, LSTM) природним чином обробляють послідовності, але їхня каузальна природа (обробка зліва направо) не оптимальна для КДВ, де інформація про параметри розподілена по всій кривій симетрично. Крім того, RNN важче паралелізуються та мають проблеми з навчанням на довгих послідовностях через gradient vanishing. Transformer архітектури забезпечують глобальну увагу та паралелізацію, але вимагають значно більших обсягів даних для навчання через відсутність індуктивного bias. Квадратична складність механізму self-attention робить їх обчислювально затратними для довгих послідовностей (типова КДВ містить 700-1000 точок). Згорткові мережі забезпечують оптимальний баланс між виразністю моделі, ефективністю навчання та обчислювальною складністю для задач аналізу одновимірних сигналів фіксованої довжини з локальними та глобальними просторовими залежностями.  
Стандартні функції втрат для регресії (MSE, MAE) не враховують фізичних обмежень на параметри профілю деформації. Однак з фізичної природи задачі випливають жорсткі constraints: деформація на поверхні не може перевищувати максимальну деформацію (D01 ≤ Dmax1), положення максимуму деформації має знаходитись в межах порушеного шару (Rp1 ≤ L1), сумарна деформація обмежена граничними значеннями для монокристалів (D01 \+ D02 ≤ 0.03). Включення цих обмежень безпосередньо у функцію втрат через penalty-терми забезпечує physics-informed навчання, де мережа одночасно мінімізує похибку передбачення та порушення фізичних законів. Математично це реалізується як L \= L\_base \+ λ · Σ ReLU(constraint\_violation), де L\_base — стандартна регресійна втрата, λ — ваговий коефіцієнт, ReLU забезпечує штраф лише за порушення обмежень. Такий підхід гарантує, що навіть у випадках, коли мережа зустрічає КДВ поза межами навчальної множини, її передбачення залишаються фізично коректними.

### **3.4 Проектування процесу передбачення та уточнення параметрів**

Процес автоматизованого визначення параметрів профілю деформації потребує інтеграції програмного комплексу на Borland C++ Builder з модулем машинного навчання на Python/PyTorch. Розроблена архітектура передбачає створення автономного виконуваного модуля, який здійснює всі етапи обробки від зчитування експериментальної кривої до формування фізично коректних значень параметрів деформації.  
Модуль передбачення проектується як окремий виконуваний файл, що інкапсулює всі необхідні компоненти для роботи нейромережевої моделі: архітектуру мережі, процедури завантаження натренованих ваг, алгоритми попередньої обробки вхідних даних та постобробки результатів. Така архітектура забезпечує повну ізоляцію машинного навчання від основної програми — взаємодія відбувається виключно через файловий інтерфейс, що гарантує сумісність та можливість незалежного оновлення компонентів. При запуску модуль отримує три параметри командного рядка: шлях до файлу натренованої моделі, шлях до файлу з експериментальною кривою та шлях для збереження результатів. Таке параметризоване проектування дозволяє використовувати один і той самий виконуваний файл з різними версіями натренованих моделей без необхідності перекомпіляції.  
Експериментальна крива надходить у вигляді текстового файлу, що містить послідовність значень інтенсивності розсіяння, виміряних у дискретних кутових точках. Використання текстового формату забезпечує платформо-незалежність, зручність налагодження та можливість візуальної інспекції даних. Кожен рядок файлу містить одне числове значення інтенсивності у експоненційній нотації для коректної обробки широкого діапазону величин (від \~10^-6 до \~10^0). Результати передбачення зберігаються у аналогічному текстовому форматі: сім рядків, що відповідають семи параметрам профілю деформації у фізичних одиницях. Порядок параметрів фіксований та документований: Dmax1, D01, L1, Rp1, D02, L2, Rp2. Такий структурований формат спрощує парсинг результатів головною програмою та забезпечує однозначну інтерпретацію кожного значення. 

*\[Рисунок 3.X: Схема взаємодії компонентів системи. Потік даних від експериментальної КДВ через модуль передбачення до отримання параметрів\]*

Процес передбачення складається з послідовних етапів трансформації вхідних даних. На першому етапі здійснюється зчитування та валідація експериментальної кривої: перевірка коректності формату, відсутності некоректних значень (NaN, Inf), відповідності очікуваній довжині сигналу. Якщо довжина вхідної кривої відрізняється від очікуваної моделлю, застосовуються процедури інтерполяції або обрізання для приведення до стандартизованої довжини. Другий етап передбачає попередню обробку кривої відповідно до процедур, застосованих при навчанні моделі. Це включає логарифмічне перетворення інтенсивності, нормалізацію до діапазону \[0,1\] та формування багатоканального представлення з додаванням позиційного каналу. Критично важливою є точна відповідність процедур попередньої обробки тим, що використовувались при генерації навчальних даних — будь-яка розбіжність призводить до зниження точності передбачень. Третій етап — безпосередньо inference (виведення) нейромережевої моделі. Підготовлена крива подається на вхід мережі, яка генерує сім нормалізованих значень у діапазоні \[0,1\], що відповідають передбаченим параметрам профілю деформації. На цьому етапі критичною є правильна ініціалізація моделі: завантаження архітектури, відновлення натренованих ваг, переведення мережі у режим оцінювання (evaluation mode), що вимикає специфічні для навчання компоненти як dropout та batch normalization. Четвертий етап — денормалізація передбачених значень до фізичних одиниць вимірювання. Кожне нормалізоване значення перетворюється за формулою p\_phys \= p\_min \+ (p\_max \- p\_min) × p\_norm, де p\_min та p\_max визначають фізичний діапазон відповідного параметра. Для параметрів довжини (L1, Rp1, L2, Rp2) додатково здійснюється конвертація одиниць вимірювання з сантиметрів (внутрішнє представлення моделі) в ангстреми (прийняті у фізиці твердого тіла).  
Передбачені нейромережевою моделлю параметри розглядаються як стартове наближення для подальшої оптимізації. Головна програма може використати ці значення як початкову точку для традиційних методів підбору — градієнтної оптимізації або еволюційних алгоритмів — які мінімізують відхилення між експериментальною та розрахованою теоретичною КДВ. Така гібридна стратегія поєднує переваги обох підходів: нейромережеве передбачення забезпечує швидке наближення до оптимальної області параметричного простору, уникаючи проблеми локальних мінімумів, характерної для градієнтних методів при випадковій ініціалізації. Подальша локальна оптимізація дозволяє уточнити параметри з урахуванням специфічних особливостей конкретної експериментальної кривої, які можуть не бути повністю враховані узагальненою моделлю. Проектування інтерфейсу модуля передбачає можливість повернення не тільки найбільш ймовірних значень параметрів, але й оцінок їхньої невизначеності. Це може бути реалізовано через ансамблеві методи або байєсівські підходи, коли замість одного детерміністичного передбачення модель генерує розподіл ймовірностей для кожного параметра. Така інформація про невизначеність критично важлива для визначення необхідності та доцільності подальшого уточнення.  
Проектування модуля передбачення передбачає систематичну обробку потенційних помилок на кожному етапі роботи. Помилки зчитування файлів (відсутність файлу, некоректний формат, недостатні права доступу) обробляються з генерацією інформативних повідомлень та ненульових кодів завершення, що дозволяють головній програмі ідентифікувати причину збою. Помилки ініціалізації моделі (некоректний формат файлу моделі, невідповідність версій бібліотек, відсутність GPU-драйверів при спробі використання апаратного прискорення) також систематично обробляються з fallback стратегіями: якщо GPU недоступний, модуль автоматично перемикається на CPU-режим; якщо формат моделі застарів, робиться спроба конвертації або видається чітке повідомлення про необхідність оновлення. Усі помилкові ситуації документуються у виведенні модуля, що дозволяє головній програмі інформувати користувача про проблему та запропонувати шляхи її розв'язання. Коди завершення процесу використовуються для розрізнення типів помилок: 0 — успішне завершення, 1 — помилка зчитування даних, 2 — помилка ініціалізації моделі, 3 — помилка обчислень.

## **4 РЕАЛІЗАЦІЯ ПРОГРАМНОЇ СИСТЕМИ**

### **4.1 Вибір та обґрунтування інструментів розробки**

Проєкт автоматизованого визначення параметрів профілю деформації базується на інтеграції класичного C++-програмного комплексу з модулем машинного навчання на Python, доповненим сучасними інструментами для розробки, тестування та розгортання. Тому вибір технологій має критичне значення як для точності моделі, так і для стабільної міжпроцесної взаємодії двох різних програмних середовищ. Розглянемо обґрунтування вибору інструментів, що використовуються для реалізації системи, а також середовищ розробки й допоміжних засобів, необхідних для забезпечення відтворюваності експериментів, зручності налагодження та подальшої експлуатації модуля.

#### **4.1.1 Python та PyTorch для машинного навчання**

Для реалізації компонента машинного навчання обрано мову програмування Python версії 3.11 та фреймворк PyTorch версії 2.9.0. Python забезпечує високу продуктивність розробки завдяки лаконічному синтаксису, динамічній типізації та багатому екосистемі наукових бібліотек (NumPy, SciPy, Matplotlib), що є критичним для швидкого прототипування та експериментування з архітектурами моделей.   
Вибір PyTorch як основного фреймворку машинного навчання обґрунтовано наступними факторами. По-перше, PyTorch надає природний императивний стиль програмування з динамічними обчислювальними графами, що спрощує налагодження моделей та реалізацію нестандартних архітектур, зокрема комбінованої CNN+FFT мережі з attention-based pooling.   
По-друге, PyTorch забезпечує автоматичне диференціювання через модуль torch.autograd, що дозволяє реалізувати складну втратну функцію з фізичними обмеженнями без ручного виведення градієнтів.   
По-третє, фреймворк підтримує прискорення обчислень на GPU (CUDA) та Apple Silicon (MPS), що критично для навчання моделі на датасетах обсягом 10000+ зразків.   
По-четверте, PyTorch надає вбудовані інструменти для серіалізації моделей у формат checkpoint, що забезпечує портативність навчених моделей між різними середовищами.   
Додатково використано бібліотеку Numba версії 0.62.1 для JIT-компіляції критичних обчислювальних циклів у модулі фізичного моделювання XRD-кривих, що забезпечує прискорення у 2-5 разів порівняно з чистою Python-реалізацією без втрати читабельності коду.

#### **4.1.2 Середовище розробки**

Для розробки та налагодження компонентів системи використано інтегроване середовище Visual Studio Code версії 1.95 з розширеннями Python, Pylance та Jupyter. Вибір VSCode обґрунтовано його кросплатформністю, вбудованою підтримкою Git для контролю версій, інтелектуальним автодоповненням коду через language server protocol та інтегрованим терміналом для швидкого виконання скриптів.   
Для інтерактивного експериментування з моделями та візуалізації результатів використано Jupyter Notebook, що дозволяє поєднувати код, результати обчислень та графіки в єдиному документі. Це забезпечує ітеративний підхід до розробки: швидке тестування гіпотез щодо архітектури моделі, аналіз помилок на валідаційній вибірці та налаштування гіперпараметрів.   
Для збірки standalone виконуваного файлу predict.exe використано PyInstaller версії 6.16.0, що автоматизує процес пакування Python-інтерпретатора, всіх залежностей та моделі в єдиний артефакт розміром близько 150 МБ.   
	Управління залежностями проекту здійснюється через requirements.txt з фіксованими версіями бібліотек для забезпечення відтворюваності середовища розробки.

### **4.2 Реалізація системи генерації синтетичного датасету**

#### **4.2.1 Імплементація стратифікованого семплування**

Ключовим викликом під час формування навчального датасету є забезпечення рівномірного покриття семивимірного простору параметрів при одночасному дотриманні фізичних обмежень та обчислювальних лімітів на його розмір.  
Для реалізації пошарового вибірання семивимірний параметричний простір розбивається на гіперкубічні регіони-шари (bins). Кількість шарів вздовж кожної осі задається параметром n\_bins\_per\_param, що визначає баланс між детальністю стратифікації та обчислювальною складністю. При значенні n\_bins \= 3 отримуємо 3^7 \= 2187 потенційних шарів у повному просторі, хоча через фізичні обмеження лише частина з них містить допустимі комбінації параметрів. Межі шарів для кожного параметра визначаються рівномірним розбиттям його фізичного діапазону: bins \= linspace(p\_min, p\_max, n\_bins \+ 1), де результуючий масив містить n\_bins \+ 1 граничних точок, що визначають n\_bins інтервалів. Для параметрів довжини розбиття виконується у шкалі ангстрем, що забезпечує інтуїтивну інтерпретацію меж шарів. Кожна комбінація параметрів з дискретної сітки отримує унікальний семивимірний ключ шару (bin key), що є кортежем з семи індексів: bin\_key \= (idx\_Dmax1, idx\_D01, idx\_L1, idx\_Rp1, idx\_D02, idx\_L2, idx\_Rp2). Індекс для кожного параметра визначається функцією digitize, що знаходить позицію значення у послідовності граничних точок. Застосування обмежувачів idx \= max(0, min(idx, n\_bins \- 1)) запобігає виходу за межі масиву для граничних значень параметрів.  
*\[Рисунок 4.X: Схема семивимірного розбиття на шари. Тривимірна проєкція простору (Dmax1, L1, L2) з візуалізацією меж шарів та розподілу валідних комбінацій\]*  
Після групування валідних комбінацій у семивимірні шари виконується рівномірне вибірання для формування фінального датасету заданого обсягу. Базова стратегія полягає у виділенні однакової кількості зразків з кожного непорожнього шару: samples\_per\_bin \= n\_total / n\_nonempty\_bins. Якщо загальна кількість зразків не ділиться націло на кількість шарів, залишок (remainder) розподіляється поміж першими remainder шарами, додаючи їм по одному додатковому зразку. З кожного шару, що містить достатню кількість комбінацій, випадково вибираються samples\_per\_bin представників без повторень через функцію random choice з параметром replace=False.   
Це забезпечує, що жоден конкретний зразок не дублюється у фінальному датасеті. Якщо певний шар містить менше комбінацій, ніж потрібно вибрати (що може статись для шарів на межах допустимої області), використовуються всі наявні комбінації з цього шару. Така стратегія гарантує, що фінальний датасет має приблизно рівномірний розподіл по всіх регіонах семивимірного простору: кожен шар представлений однаковою кількістю зразків незалежно від того, скільки валідних комбінацій він містить. Це принципово відрізняється від наївного випадкового вибірання, де ймовірність вибору пропорційна до кількості валідних точок, що призводить до переповнення деяких регіонів та недопредставлення інших.   
*\[Рисунок 4.Y: Порівняння розподілу зразків при випадковому та пошаровому вибіранні. Гістограми розподілу по параметрах L1 та Rp1 демонструють рівномірність стратифікованого підходу\]*

Датасет зберігається у бінарному форматі pickle з compression protocol 4, що забезпечує компактність (100 000 кривих по 1000 точок займають \~400 MB) та швидке завантаження. Разом з масивами X та Y зберігаються метадані: параметри генерації (кількість шарів, кроки дискретизації, фізичні обмеження), параметри обрізання кривих для подальшого навчання, часова мітка створення. Це забезпечує повну відтворюваність та можливість трекінгу версій датасетів.

#### **4.2.2 Оптимізація обчислень XRD-кривих** 

Після формування списку відібраних комбінацій параметрів здійснюється генерація відповідних кривих дифракційного відбивання. Оскільки розрахунок однієї КДВ займає \~0.1-1 секунду та не залежить від інших зразків, процес ідеально підходить для паралелізації на багатоядерних системах. Реалізація використовує модуль multiprocessing з пулом процесів (Pool), розмір якого відповідає кількості доступних ядер процесора. Кожен процес у пулі отримує комбінацію параметрів, виконує конвертацію одиниць вимірювання, створює об'єкт профілю деформації, розраховує КДВ через фізичну модель та повертає пару (параметри, крива). Функція imap забезпечує ледачу генерацію результатів з можливістю відстеження прогресу через обгортку tqdm. Критичним аспектом паралелізації є мінімізація часу на міжпроцесну комунікацію. Замість передачі складних об'єктів між процесами, кожен робочий процес отримує лише кортеж числових параметрів (7 float значень), самостійно імпортує необхідні модулі та виконує обчислення. Результати збираються у головному процесі та агрегуються у масиви NumPy для подальшого зберігання. Результати бенчмаркінгу демонструють прискорення у 5-10 разів порівняно з чистою Python-реалізацією. Для генерації 100 000 зразків на із товщиною підшару dl \= 100 Å на восьмиядерній системі час скорочується з декількох годин до менше десяти хвилин.

#### **4.2.3 JIT-компіляція вкладених циклів обчислень**	 

Розрахунок теоретичних кривих дифракційного відбивання на основі динамічної теорії розсіяння рентгенівських променів є обчислювально інтенсивною задачею, особливо при великій кількості підшарів у дефектній приповерхневій області. Критичним вузьким місцем алгоритму є вкладені цикли обчислення амплітуди розсіяння від послідовних підшарів, що виконуються для кожної точки кутового сканування. Для підвищення обчислювальної ефективності генерації синтетичних датасетів застосовано технологію JIT-компіляції (Just-In-Time compilation, компіляція на льоту) критичних обчислювальних ядер.  
Профілювання коду розрахунку дифракційних кривих виявило, що понад 70-80% загального часу виконання припадає на функцію обчислення когерентної складової розсіяння, конкретно — на внутрішній цикл по підшарах. Типова КДВ розраховується для \~700 кутових точок, а кожна точка вимагає обчислення послідовного розсіювання від \~200-400 підшарів (залежно від параметрів L1, L2 та кроку дискретизації dl). Це призводить до виконання \~140 000 \- 280 000 ітерацій внутрішнього циклу на одну криву. Кожна ітерація циклу виконує складні фізичні обчислення: обчислення комплексних параметрів поляризованості, розв'язання дисперсійного рівняння з квадратними коренями комплексних чисел, експоненціювання, множення матриць передачі. У чистому Python-коді ці операції виконуються через інтерпретатор з суттєвими накладними витратами на динамічну типізацію, перевірку меж масивів та керування пам'яттю.  
Для усунення інтерпретаційних накладних витрат застосовано бібліотеку Numba — транслятор Python коду у машинний код через проміжне представлення LLVM (Low Level Virtual Machine). Numba аналізує підмножину Python коду з числовими обчисленнями, визначає типи всіх змінних статично та генерує оптимізований машинний код, еквівалентний за швидкістю до C або Fortran реалізацій. Ключовою особливістю Numba є режим JIT-компіляції: код компілюється не заздалегідь, а при першому виклику функції. Під час першого виклику Numba аналізує типи вхідних аргументів, генерує спеціалізовану версію функції для цих типів та кешує скомпільований машинний код. Всі наступні виклики з тими самими типами аргументів виконуються через вже скомпільовану версію без інтерпретації. Декоратор @njit (no-python JIT) вказує на режим компіляції без fallback до інтерпретатора: якщо частина коду не може бути скомпільована, генерується помилка замість повернення до інтерпретації. Це гарантує максимальну швидкість виконання, але вимагає уникнення деяких Python конструкцій (словників, списків, об'єктно-орієнтованих викликів) всередині JIT-функцій.  
Внутрішній цикл по підшарах було виокремлено з основної функції розрахунку у окрему JIT-компільовану функцію \_compute\_sublayer\_loop\_jit. Ця функція приймає всі необхідні фізичні параметри як числові аргументи: кількість підшарів km, кутове відхилення для поточної точки DeltaTeta\_i, фізичні константи (b\_as, tb, Lambda, gamma0, gammah), масиви параметрів підшарів (eta0\_a, xhp\_a\_n, xhn\_a\_n, DDpd, Dl, Esum) та початкове значення амплітуди As\_in. Всередині JIT-функції реалізовано послідовний цикл по індексу підшару k від 1 до km. Перший виклик JIT-функції супроводжується затримкою на компіляцію: Numba аналізує код, виводить типи змінних, будує граф потоку даних, оптимізує та генерує машинний код через LLVM backend. Для функції з \~50 рядками фізичних обчислень цей процес займає \~0.5-2 секунди залежно від потужності процесора. Однак після першої компіляції скомпільована версія кешується у пам'яті, і всі наступні виклики (у межах одного процесу Python) виконуються миттєво.  
*\[Рисунок 4.X: Порівняння часу виконання одного циклу по підшарах у чистому Python та з JIT-компіляцією. Демонстрація зменшення накладних витрат інтерпретації\]*  
Це призводить до середнього прискорення 2-5 разів порівняно з чистою Python реалізацією. Фактичне прискорення залежить від параметра dl: менші значення dl призводять до більшої кількості підшарів, відповідно до більшої кількості ітерацій внутрішнього циклу та більшого ефекту від JIT-компіляції. Це прискорення безпосередньо транслюється у зменшення часу генерації датасету. Для генерації датасету це означає прискорення 2-5 разів порівняно з чистою Python реалізацією. Така оптимізація робить генерацію великих датасетів практично здійсненною на стандартному робочому обладнанні без необхідності доступу до обчислювальних кластерів. 

### **4.3 Імплементація нейромережевої моделі**

Реалізація архітектури згорткової нейронної мережі для регресії параметрів профілю деформації здійснюється засобами фреймворку PyTorch. Імплементація охоплює три основні компоненти: клас моделі з визначенням архітектури шарів, клас датасету для завантаження та нормалізації навчальних даних, та функцію втрат з фізичними обмеженнями. Кожен компонент реалізовано як окремий програмний модуль з чітко визначеним інтерфейсом, що забезпечує можливість незалежного тестування та повторного використання у різних контекстах.

#### **4.3.1 Архітектура мережі та шари**

Центральним компонентом реалізації є клас XRDRegressor, що інкапсулює повну архітектуру згорткової нейронної мережі для визначення семи параметрів профілю деформації з КДВ. Клас успадковується від базового nn.Module фреймворку PyTorch, що забезпечує автоматичне управління параметрами та обчислення градієнтів.  
Вхідний згортковий шар (stem) обробляє двоканальне представлення КДВ: перший канал містить нормалізовану в логарифмічному просторі інтенсивність розсіяння, другий — позиційну координату вздовж кутової осі. Початкове перетворення з 2 каналів у 32 реалізовано згортковим шаром з ядром розміром 9, що забезпечує вилучення низькорівневих ознак при збереженні просторової розмірності. Застосування BatchNorm1d та функції активації SiLU завершує формування початкового представлення. Позиційний канал генерується динамічно у методі forward() через лінійний простір від 0 до 1 довжиною L точок та конкатенується з каналом інтенсивності. Це зменшує обсяг пам'яті датасету та уникає дублювання ідентичних даних для кожного зразка.  
Основа мережі складається з шести блоків класу ResidualBlock, де кожен блок містить два послідовних згорткових шари Conv1d з пакетною нормалізацією та активацією SiLU, доповнені residual з'єднанням. Така архітектура дозволяет навчати залишкову функцію, що полегшує оптимізацію глибоких структур та запобігає деградації градієнтів. Ключовою особливістю є прогресивне збільшення коефіцієнта дилатації (dilation) через блоки: 1, 2, 4, 8, 16, 32\. Це створює експоненційно зростаюче рецептивне поле без збільшення параметрів: ранні блоки виявляють локальні особливості КДВ, пізніші — довгоперіодичні осциляції. При розмірі ядра kernel\_size=15 та максимальній дилатації 32 каскад блоків охоплює понад 900 точок сигналу, що перевищує довжину типової КДВ (\~700 точок). Між residual блоками розміщено transition шари — згортки 1×1 для зміни кількості каналів. Прогресивне розширення реалізується послідовністю 32→48→64→96→128→128, де останні два блоки працюють зі сталою кількістю каналів. Такий підхід збалансовує виразність моделі та обчислювальну ефективність: ранні шари з меншою кількістю каналів обробляють низькорівневі ознаки, пізніші — абстрактні паттерни.   
\[Рисунок 4.X: Архітектура residual блоку з дилатованою згорткою. Схема проходження даних через два згорткових шари з residual з'єднанням\]  
Замість стандартного global average pooling реалізовано клас AttentionPool1d для зваженої агрегації просторових ознак. Мережа attention складається з двох згорток 1×1, що генерують скалярні логіти для кожної позиції вздовж КДВ. Ці логіти нормалізуються через softmax до розподілу ймовірностей та використовуються для обчислення зваженої суми ознак по всіх позиціях. Така стратегія дозволяє мережі самостійно виявляти найбільш інформативні регіони КДВ для визначення кожного параметра. Для позиційних параметрів увага може концентруватись на специфічних ділянках кривої, де прояви деформації найбільш виражені. Навчання ваг attention відбувається автоматично через backpropagation разом з іншими параметрами мережі.  
Паралельно до згорткової гілки функціонує спектральний аналізатор. У методі forward() зберігається канал інтенсивності, до якого застосовується вікно Ганна (hann\_window) для зменшення спектральних витоків. Виконується швидке перетворення Фур'є через torch.fft.rfft з ортонормованою нормалізацією, що повертає комплексні коефіцієнти для позитивних частот. Обчислюється амплітудний спектр через abs(fft), відкидається DC-компонента (нульова частота) та береться перші 50 частотних компонент. Ці компоненти після логарифмічної нормалізації обробляються через двошарову MLP (Multi-Layer Perceptron, багатошаровий перцептрон): 50→64→32 з активацією SiLU, що генерує 32-вимірний вектор спектральних ознак. Такий вектор відображає періодичності КДВ.  
Фінальний MLP-блок (head) об'єднує 128-вимірний просторовий вектор з AttentionPool1d та 32-вимірний спектральний вектор у загальне 160-вимірне представлення через конкатенацію. Це представлення обробляється через тришарову повнозв'язну мережу з архітектурою 160→256→128→7, де між шарами застосовується активація SiLU та dropout з ймовірністю 0.2. Шари Dropout застосовуються лише у режимі навчання (автоматично вимикаються при model.eval()), що запобігає перенавчанню через випадкове обнулення 20% нейронів. Фінальний шар з сигмоїдною активацією генерує 7 виходів у діапазоні \[0, 1\], що відповідають нормалізованим параметрам у фіксованому порядку: Dmax1, D01, L1, Rp1, D02, L2, Rp2.   
\[Рисунок 4.Y: Повна архітектура XRDRegressor. Діаграма потоку даних від вхідної КДВ через CNN та FFT гілки до фінального передбачення параметрів\]

#### **4.3.2 Нормалізація вхідних даних та виходів**

Нормалізація вхідних та вихідних даних є критичною для стабільності навчання та якості конвергенції. Реалізовано двоетапну схему нормалізації через клас NormalizedXRDDataset. Вхідні криві спочатку проходять log₁₀-перетворення для роботи в логарифмічному просторі інтенсивностей: y\_log \= log₁₀(y \+ ε), де ε \= 10⁻¹⁰ — регуляризаційна константа для запобігання log(0). Логарифмічне представлення критично для XRD-кривих через широкий динамічний діапазон інтенсивностей (5-6 порядків від піку до хвоста кривої). Після log-перетворення виконується покомпонентна нормалізація кожної кривої до діапазону \[0, 1\] через y\_norm \= (y\_log \- min(y\_log)) / (max(y\_log) \- min(y\_log) \+ ε). Така нормалізація забезпечує інваріантність до абсолютної шкали інтенсивності, що може варіюватися між експериментами через різні умови вимірювання. Цільові параметри нормалізуються незалежно для кожного з семи параметрів через лінійне масштабування до \[0, 1\] з використанням заздалегідь визначених діапазонів RANGES: t\_norm\[i\] \= (t\[i\] \- min\[i\]) / (max\[i\] \- min\[i\]). Це забезпечує рівномірну вагу всіх параметрів у втратній функції незалежно від їх фізичних одиниць та порядків величини. Денормалізація передбачень виконується через зворотне перетворення: p\[i\] \= min\[i\] \+ (max\[i\] \- min\[i\]) × p\_norm\[i\], де p\_norm\[i\] ∈ \[0, 1\] — вихід моделі після sigmoid активації.

#### **4.3.3 Реалізація втратної функції з фізичними обмеженнями**

Втратна функція поєднує стандартну регресійну похибку з штрафами за порушення фізичних обмежень через функцію physics\_constrained\_loss. Базова втрата обчислюється як smooth L1 loss (Huber loss) між передбаченими та істинними параметрами в нормалізованому просторі \[0, 1\], що забезпечує стійкість до викидів порівняно з MSE. Система фізичних обмежень включає чотири нерівності, що перевіряються у денормалізованому фізичному просторі. Обмеження D₀₁ ≤ Dmax₁ забезпечує коректність асиметричної гаусіани, де деформація на поверхні не може перевищувати максимальну деформацію. Обмеження D₀₁ \+ D₀₂ ≤ 0.03 відображає фізичну межу сумарної деформації для монокристалів GGG. Обмеження Rp₁ ≤ L₁ гарантує, що позиція максимуму деформації знаходиться в межах товщини шару. Обмеження L₂ ≤ L₁ забезпечує монотонність товщин шарів. Штраф за порушення обмежень обчислюється як сума ReLU від різниць: penalty \= Σ ReLU(constraint\_violation). Використання ReLU забезпечує нульовий градієнт для задоволених обмежень та лінійне зростання штрафу для порушених. Фінальна втрата формується як loss\_total \= loss\_base \+ 0.1 × penalty\_constraint, де коефіцієнт 0.1 збалансовує точність передбачення з фізичною коректністю розв'язку.

#### **4.3.4 Процес навчання моделі**

Навчання нейромережевої моделі здійснюється через функцію train\_with\_curve\_validation, що реалізує стратегію швидкого навчання на параметричній втраті з валідацією якості через реконструкцію КДВ. Така гібридна стратегія поєднує обчислювальну ефективність регресійного навчання з фізичною осмисленістю перевірки через порівняння кривих.  
Після завантаження датасету через функцію load\_dataset виконується випадкове розділення на навчальну та валідаційну множини у співвідношенні 80/20. Випадкова перестановка індексів забезпечує незалежність вибірок, а фіксований seed гарантує відтворюваність експериментів. З кожної множини створюється екземпляр класу NormalizedXRDDataset з параметром log\_space=True для логарифмічної нормалізації КДВ та train=True/False для позначення режиму. Завантажувачі даних (DataLoader) створюються з розміром пакету (batch size) 128 для навчання та меншим розміром 4 для валідації на кривих. Менший розмір пакету при валідації обумовлений необхідністю виконання двох симуляцій КДВ для кожного зразка при обчисленні втрати реконструкції, що вимагає значних обчислювальних ресурсів. Параметр shuffle=True для навчального завантажувача забезпечує випадковий порядок пакетів на кожній епосі.  
Для оптимізації параметрів моделі використовується алгоритм AdamW — варіант Adam з роз'єднаним decay ваг (decoupled weight decay). Початкова швидкість навчання встановлюється на рівні 1e-3, параметр weight\_decay дорівнює 1e-4 для регуляризації через штрафування великих ваг. AdamW зберігає експоненційно згладжені перші та другі моменти градієнтів, що забезпечує адаптивну швидкість навчання для кожного параметра окремо. Планувальник швидкості навчання (learning rate scheduler) типу ReduceLROnPlateau відстежує валідаційну втрату реконструкції кривих та зменшує швидкість навчання у 2 рази (factor=0.5), якщо метрика не покращується протягом 5 епох (patience=5). Мінімальна швидкість обмежена значенням 1e-6, після досягнення якого подальше зменшення не відбувається. Така стратегія дозволяє спочатку швидко наближатись до оптимуму з великою швидкістю, а потім уточнювати параметри з меншими кроками.  
На кожній епосі виконується повний прохід через навчальну множину пакетами. Для кожного пакету модель переводиться у режим навчання через model.train(), виконується прямий прохід для отримання передбачених нормалізованих параметрів, обчислюється фізично-обмежена втрата через функцію physics\_constrained\_loss та виконується зворотне поширення градієнтів з оновленням параметрів через оптимізатор. Функція втрат поєднує базову smooth L1 loss для різниці між передбаченими та справжніми параметрами з penalty-термами за порушення фізичних обмежень. Для кожного пакету повертається загальна втрата та окрема величина штрафу за обмеження, що дозволяє контролювати, наскільки модель порушує фізичні закони. Накопичена втрата по всій епосі нормалізується на кількість зразків для отримання середньої втрати навчання.  
Після кожної епохи навчання виконується валідація у двох режимах. Спочатку модель переводиться у режим оцінювання через model.eval(), що вимикає dropout та фіксує статистики batch normalization. Перша валідація обчислює параметричну втрату на валідаційній множині аналогічно до навчання, але без обчислення градієнтів (у контексті torch.no\_grad()). Це забезпечує швидку оцінку точності передбачення параметрів. Друга валідація обчислює втрату реконструкції кривих через клас CurveReconstructionLoss. Для кожного зразка у валідаційному пакеті модель передбачає параметри, які денормалізуються до фізичних значень. З цими параметрами виконується дві симуляції КДВ з різними випадковими зсувами параметрів (SPSA — Simultaneous Perturbation Stochastic Approximation), що апроксимує градієнт втрати по кривій без явного диференціювання фізичного симулятора. Втрата обчислюється як різниця між вхідною експериментальною кривою та симульованими кривими.  
Під час навчання відстежуються дві метрики оптимальності: найкраща валідаційна втрата на параметрах та найкраща валідаційна втрата на реконструкції кривих. Для кожної метрики зберігається окрема копія моделі: model\_path\_params для найкращої параметричної точності та model\_path\_curve для найкращої якості реконструкції. Файли моделей містять не лише ваги мережі через model.state\_dict(), але й метадані: номер епохи, обидві валідаційні втрати, довжину кривої. Критерієм ранньої зупинки (early stopping) є валідаційна втрата на кривих, а не на параметрах. Це ключова особливість реалізації: модель оптимізується для якості реконструкції КДВ, а не для точності параметрів як таких. Планувальник швидкості навчання також використовує втрату на кривих для прийняття рішень про зменшення швидкості. Така стратегія забезпечує, що фінальна модель генерує параметри, які дійсно відтворюють експериментальні криві, а не просто близькі числово до справжніх значень.  
Під час навчання на кожній епосі виводиться інформація про поточні метрики: втрата на навчанні, валідаційна втрата на параметрах, валідаційна втрата на кривих, поточна швидкість навчання. Обчислення валідації на кривих супроводжується повідомленням про тривалість (типово 30-60 секунд для валідаційної множини з 20 000 зразків при пакеті розміром 4), що дозволяє користувачу розуміти прогрес навчання. При покращенні метрик виводяться повідомлення про збереження моделей з вказанням обох втрат, що дозволяє відстежувати trade-off між параметричною точністю та якістю реконструкції. Часто спостерігається ситуація, коли модель з найкращою параметричною втратою не є найкращою за втратою на кривих, що підтверджує важливість валідації через фізичне моделювання. По завершенню навчання виводиться рекомендація використовувати модель, збережену за критерієм втрати на кривих, для практичного застосування.

### **4.4 Оцінка якості та валідація моделі**

#### **4.4.1 Метрики оцінювання**

#### **4.4.2 Тестування на експериментальних КДВ**

#### **4.4.3 Аналіз помилок та обмежень моделі**

### **4.5 Створення виконуваного модуля для інтеграції**

Для інтеграції натренованої нейромережевої моделі у існуюче програмне забезпечення, розроблене в середовищі C++ Builder, необхідне створення автономного виконуваного модуля, що інкапсулює всі залежності Python та PyTorch і може бути викликаний як зовнішній процес. Такий підхід забезпечує повну ізоляцію компонента машинного навчання від основної програми, уникаючи складнощів прямої інтеграції Python інтерпретатора в C++ додаток та забезпечуючи платформо-незалежність взаємодії через файловий інтерфейс. У даному розділі розглядаються практичні аспекти створення standalone застосунку для inference (виведення передбачень), що включає експорт натренованої моделі, підготовку скрипту передбачення та збірку виконуваного файлу з усіма необхідними бібліотеками.

#### **4.5.1 Реалізація модуля виведення передбачень**

Модуль виведення передбачень реалізовано як автономний Python скрипт predict.py, що виконує всі етапи від зчитування експериментальної КДВ до формування фізичних значень параметрів профілю деформації. Скрипт призначений для виклику як зовнішнього процесу з передачею параметрів через командний рядок.  
Модуль приймає три аргументи командного рядка: шлях до натренованої моделі, шлях до вхідної кривої та шлях для збереження результатів. Перевіряється існування файлів моделі та вхідних даних, при відсутності формується повідомлення про помилку у файл predict\_error.log та завершення з кодом 1\. Завантаження моделі здійснюється через torch.load з параметром map\_location='cpu' для роботи без GPU. З checkpoint зчитується словник параметрів та метадані, зокрема очікувана довжина кривої L. Створюється екземпляр XRDRegressor, завантажуються параметри через load\_state\_dict, та критично важливо викликається model.eval() для переведення у режим оцінювання.  
	Експериментальна КДВ зчитується з текстового файлу як послідовність значень інтенсивності. Крива проходить попередню обробку через preprocess\_curve: обрізання шумового хвоста, crop від піку та нормалізація до фіксованої довжини. Якщо вхідна крива відрізняється за довжиною, вона доповнюється або обрізається до очікуваної розмірності. Оброблена крива подається у NormalizedXRDDataset з параметром log\_space=True для застосування логарифмічної нормалізації. Нормалізована крива з розмірністю batch=1 подається на вхід моделі у контексті torch.no\_grad(). Модель повертає тензор нормалізованих передбачень \[1, 7\] у діапазоні \[0, 1\].  
Кожен нормалізований параметр перетворюється до фізичних одиниць через формулу p\_phys \= p\_min \+ (p\_max \- p\_min) \* p\_norm з використанням діапазонів зі словника RANGES. Порядок параметрів визначається списком PARAM\_NAMES: Dmax1, D01, L1, Rp1, D02, L2, Rp2. Результати записуються у вихідний файл по одному значенню на рядок у науковій нотації з точністю 6 знаків. Використання експоненційної форми гарантує коректне представлення як великих, так і малих значень. Весь код обгорнутий у блок обробки винятків: при помилці формується детальний звіт у predict\_error.log та завершення з кодом 1, при успіху — код 0\.

#### **4.5.2 Збірка standalone застосунку**

Створення автономного виконуваного файлу з Python скрипту вимагає упакування інтерпретатора, всіх залежних бібліотек та коду застосунку у єдину структуру, що може виконуватись на системах без встановленого Python. Для цього застосовується інструмент PyInstaller — утиліта, що аналізує залежності Python програми та створює самодостатній пакет з усіма необхідними компонентами.  
PyInstaller аналізує вхідний Python скрипт, виявляє всі імпортовані модулі (як стандартної бібліотеки, так і сторонніх пакетів), збирає бінарні залежності (динамічні бібліотеки для NumPy, PyTorch та інших пакетів) та створює виконуваний файл з вбудованим Python інтерпретатором. При запуску такого файлу відбувається розпакування необхідних компонентів у тимчасову директорію та виконання основного скрипту у ізольованому середовищі. Для проекту з PyTorch критичною є підтримка динамічних залежностей глибокого навчання: бібліотек лінійної алгебри (BLAS, LAPACK), драйверів апаратних прискорювачів (хоча для CPU-версії вони не потрібні), власних розширень PyTorch написаних на C++. PyInstaller автоматично виявляє більшість цих залежностей через аналіз імпортів та динамічного завантаження бібліотек.  
Базова команда PyInstaller приймає вхідний скрипт та параметри збірки через командний рядок, але для складних проектів доцільно створити специфікаційний файл (spec file) — Python скрипт, що описує процес збірки декларативно. Цей файл генерується автоматично при першому запуску PyInstaller з параметром \--onefile (один виконуваний файл) або \--onedir (директорія з файлами), після чого може бути модифікований вручну для додавання специфічних залежностей. У специфікаційному файлі визначаються шляхи до додаткових файлів даних (наприклад, натренована модель може бути вбудована безпосередньо у виконуваний файл), приховані імпорти (модулі, що завантажуються динамічно і не виявляються автоматичним аналізом), виключення непотрібних модулів для зменшення розміру. Для моделі машинного навчання типово додаються hidden imports для модулів PyTorch, що завантажуються через reflection.  
Виконуваний файл з повним PyTorch та залежностями типово має розмір 200-400 MB, що зумовлено включенням всього фреймворку глибокого навчання. Для зменшення розміру можливе виключення непотрібних компонентів: підтримки CUDA (якщо застосунок працює лише на CPU), візуалізаційних бібліотек, тестових модулів. Параметр \--exclude-module дозволяє явно виключити модулі на кшталт matplotlib, pandas, jupyter, що не використовуються у inference. Альтернативним підходом є використання CPU-версії PyTorch замість повної версії з підтримкою GPU, що зменшує розмір на 50-100 MB через відсутність CUDA бібліотек. Для задачі передбачення параметрів з однієї КДВ (що займає \~0.1-0.5 секунди на CPU) апаратне прискорення не є критичним, тому CPU-версія є оптимальною для інтеграції у основне програмне забезпечення.  
Фінальний пакет для розгортання складається з виконуваного файлу predict.exe та файлу моделі predict\_target\_model.pt, що розміщуються у тій самій директорії, що й основна програма C++ Builder. Основна програма викликає predict.exe з аргументами командного рядка, що вказують шляхи до вхідних та вихідних файлів. Така організація дозволяє легко оновлювати як виконуваний модуль (при зміні логіки передбачення), так і модель (при перенавчанні на нових даних) незалежно одне від одного. Версійність виконуваного модуля підтримується через включення номера версії у вихідні дані або окремий файл метаданих. Основна програма може перевіряти сумісність версій перед викликом модуля та попереджати користувача про необхідність оновлення у разі невідповідності. Це особливо важливо при зміні формату вхідних/вихідних файлів або додаванні нових параметрів.

#### **4.5.3 Інтеграція з базовим ПЗ**

	При активації функції автоматичного підбору параметрів основна програма формує файл з експериментальною КДВ, що зберігається у директорії виконуваного файлу під фіксованою назвою predict\_curve.txt. Експериментальна крива, що знаходиться у пам'яті основної програми як масив значень інтенсивності, записується у текстовий файл по одному значенню на рядок у експоненційній нотації. Запис виконується для всіх точок експериментального сканування від початку до кінця масиву, без попередньої обробки або фільтрації. Модуль передбачення самостійно виконує всі необхідні процедури crop та нормалізації. Перевіряється успішність створення файлу перед запуском модуля передбачення.  
Запуск виконуваного модуля predict.exe здійснюється через системний API ShellExecuteEx з параметрами, що включають шляхи до моделі, вхідного та вихідного файлів. Виконуваний файл та модель розташовані у тій самій директорії, що й основна програма, що спрощує управління шляхами. Робоча директорія процесу встановлюється у директорію основної програми. Процес запускається у прихованому режимі без відображення консольного вікна для уникнення відволікання користувача. Основна програма не блокується під час виконання передбачення: реалізовано цикл очікування з періодичною обробкою подій інтерфейсу через Application-\>ProcessMessages(), що зберігає відгук програми та дозволяє відображати індикатор прогресу.  
Очікування завершення дочірнього процесу реалізоване через функцію MsgWaitForMultipleObjects з коротким timeout 50 мілісекунд, що забезпечує баланс між швидкістю реакції та навантаженням на процесор. Після завершення процесу зчитується код завершення: значення 0 вказує на успішне виконання, ненульове — на помилку. При ненульовому коді завершення користувачу відображається повідомлення про помилку без спроби читання файлу результатів. При успішному завершенні перевіряється існування файлу predict\_params.txt перед спробою читання. Якщо файл відсутній, формується повідомлення про помилку з вказанням можливих причин.  
Файл результатів зчитується як послідовність текстових рядків, де кожен рядок містить одне числове значення у науковій нотації. Парсинг виконується через стандартну функцію atof, що коректно обробляє експоненційну форму запису. Зчитуються рівно сім значень у фіксованому порядку відповідно до списку параметрів. Для параметрів довжини (L1, Rp1, L2, Rp2) виконується конвертація з сантиметрів у ангстреми через множення на коефіцієнт 10^8 та округлення до цілого значення. Параметри деформації (Dmax1, D01, D02) зберігаються як значення з плаваючою комою без конвертації одиниць.  
