======================================================================
XRD CNN TRAINING
======================================================================
âœ“ Using MPS (Apple Silicon)
âœ“ Loaded dataset from datasets/dataset_100000_dl400.pkl
X shape: (100000, 7) Y shape: (100000, 650)

ðŸ“Š Data split:
Train: 99000 samples
Val: 1000 samples

ðŸ§  Model: XRDRegressor
Parameters to predict: ['Dmax1', 'D01', 'L1', 'Rp1', 'D02', 'L2', 'Rp2']
Output dim: 7

ðŸš€ Starting training for 80 epochs...
Batch size: 128
Learning rate: 0.0015
Weight decay: 0.0005

---

Epoch 001/80 | train: 0.01521 | val: 0.01319 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.01319)
Epoch 002/80 | train: 0.01212 | val: 0.01613 | lr: 1.50e-03
Epoch 003/80 | train: 0.01123 | val: 0.01339 | lr: 1.50e-03
Epoch 004/80 | train: 0.01071 | val: 0.01226 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.01226)
Epoch 005/80 | train: 0.01035 | val: 0.01052 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.01052)
Epoch 006/80 | train: 0.01004 | val: 0.00965 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00965)
Epoch 007/80 | train: 0.00981 | val: 0.01062 | lr: 1.50e-03
Epoch 008/80 | train: 0.00963 | val: 0.00947 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00947)
Epoch 009/80 | train: 0.00945 | val: 0.01127 | lr: 1.50e-03
Epoch 010/80 | train: 0.00933 | val: 0.01561 | lr: 1.50e-03
Epoch 011/80 | train: 0.00922 | val: 0.01754 | lr: 1.50e-03
Epoch 012/80 | train: 0.00908 | val: 0.00975 | lr: 1.50e-03
Epoch 013/80 | train: 0.00898 | val: 0.00982 | lr: 1.50e-03
Epoch 014/80 | train: 0.00890 | val: 0.00867 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00867)
Epoch 015/80 | train: 0.00879 | val: 0.00906 | lr: 1.50e-03
Epoch 016/80 | train: 0.00874 | val: 0.00949 | lr: 1.50e-03
Epoch 017/80 | train: 0.00866 | val: 0.00961 | lr: 1.50e-03
Epoch 018/80 | train: 0.00860 | val: 0.00878 | lr: 1.50e-03
Epoch 019/80 | train: 0.00854 | val: 0.00865 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00865)
Epoch 020/80 | train: 0.00846 | val: 0.01007 | lr: 1.50e-03
Epoch 021/80 | train: 0.00838 | val: 0.00935 | lr: 1.50e-03
Epoch 022/80 | train: 0.00835 | val: 0.00998 | lr: 1.50e-03
Epoch 023/80 | train: 0.00829 | val: 0.00863 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00863)
Epoch 024/80 | train: 0.00824 | val: 0.00875 | lr: 1.50e-03
Epoch 025/80 | train: 0.00819 | val: 0.00860 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00860)
Epoch 026/80 | train: 0.00815 | val: 0.00850 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00850)
Epoch 027/80 | train: 0.00811 | val: 0.01009 | lr: 1.50e-03
Epoch 028/80 | train: 0.00807 | val: 0.00841 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00841)
Epoch 029/80 | train: 0.00801 | val: 0.00945 | lr: 1.50e-03
Epoch 030/80 | train: 0.00800 | val: 0.00874 | lr: 1.50e-03
Epoch 031/80 | train: 0.00793 | val: 0.00865 | lr: 1.50e-03
Epoch 032/80 | train: 0.00792 | val: 0.00889 | lr: 1.50e-03
Epoch 033/80 | train: 0.00787 | val: 0.00840 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00840)
Epoch 034/80 | train: 0.00784 | val: 0.00818 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00818)
Epoch 035/80 | train: 0.00781 | val: 0.00814 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00814)
Epoch 036/80 | train: 0.00779 | val: 0.01100 | lr: 1.50e-03
Epoch 037/80 | train: 0.00776 | val: 0.01101 | lr: 1.50e-03
Epoch 038/80 | train: 0.00770 | val: 0.01150 | lr: 1.50e-03
Epoch 039/80 | train: 0.00770 | val: 0.00793 | lr: 1.50e-03
â†’ Saved best model (val_loss: 0.00793)
Epoch 040/80 | train: 0.00766 | val: 0.01024 | lr: 1.50e-03
Epoch 041/80 | train: 0.00763 | val: 0.00903 | lr: 1.50e-03
Epoch 042/80 | train: 0.00762 | val: 0.00952 | lr: 1.50e-03
Epoch 043/80 | train: 0.00758 | val: 0.00808 | lr: 1.50e-03
Epoch 044/80 | train: 0.00752 | val: 0.00812 | lr: 1.50e-03
Epoch 045/80 | train: 0.00750 | val: 0.00837 | lr: 7.50e-04
Epoch 046/80 | train: 0.00715 | val: 0.00743 | lr: 7.50e-04
â†’ Saved best model (val_loss: 0.00743)
Epoch 047/80 | train: 0.00710 | val: 0.00857 | lr: 7.50e-04
Epoch 048/80 | train: 0.00707 | val: 0.00762 | lr: 7.50e-04
Epoch 049/80 | train: 0.00704 | val: 0.00807 | lr: 7.50e-04
Epoch 050/80 | train: 0.00703 | val: 0.00772 | lr: 7.50e-04
Epoch 051/80 | train: 0.00699 | val: 0.00764 | lr: 7.50e-04
Epoch 052/80 | train: 0.00699 | val: 0.00906 | lr: 3.75e-04
Epoch 053/80 | train: 0.00677 | val: 0.00750 | lr: 3.75e-04
Epoch 054/80 | train: 0.00673 | val: 0.00770 | lr: 3.75e-04
Epoch 055/80 | train: 0.00672 | val: 0.00739 | lr: 3.75e-04
â†’ Saved best model (val_loss: 0.00739)
Epoch 056/80 | train: 0.00671 | val: 0.00763 | lr: 3.75e-04
Epoch 057/80 | train: 0.00671 | val: 0.00754 | lr: 3.75e-04
Epoch 058/80 | train: 0.00669 | val: 0.00759 | lr: 3.75e-04
Epoch 059/80 | train: 0.00668 | val: 0.00730 | lr: 3.75e-04
â†’ Saved best model (val_loss: 0.00730)
Epoch 060/80 | train: 0.00666 | val: 0.00763 | lr: 3.75e-04
Epoch 061/80 | train: 0.00666 | val: 0.00727 | lr: 3.75e-04
â†’ Saved best model (val_loss: 0.00727)
Epoch 062/80 | train: 0.00666 | val: 0.00765 | lr: 3.75e-04
Epoch 063/80 | train: 0.00662 | val: 0.00730 | lr: 3.75e-04
Epoch 064/80 | train: 0.00662 | val: 0.00753 | lr: 3.75e-04
Epoch 065/80 | train: 0.00661 | val: 0.00720 | lr: 3.75e-04
â†’ Saved best model (val_loss: 0.00720)
Epoch 066/80 | train: 0.00660 | val: 0.00730 | lr: 3.75e-04
Epoch 067/80 | train: 0.00659 | val: 0.00725 | lr: 3.75e-04
Epoch 068/80 | train: 0.00658 | val: 0.00745 | lr: 3.75e-04
Epoch 069/80 | train: 0.00658 | val: 0.00733 | lr: 3.75e-04
Epoch 070/80 | train: 0.00656 | val: 0.00752 | lr: 3.75e-04
Epoch 071/80 | train: 0.00656 | val: 0.00737 | lr: 1.88e-04
Epoch 072/80 | train: 0.00642 | val: 0.00763 | lr: 1.88e-04
Epoch 073/80 | train: 0.00640 | val: 0.00731 | lr: 1.88e-04
Epoch 074/80 | train: 0.00640 | val: 0.00724 | lr: 1.88e-04
Epoch 075/80 | train: 0.00641 | val: 0.00727 | lr: 1.88e-04
Epoch 076/80 | train: 0.00639 | val: 0.00719 | lr: 1.88e-04
â†’ Saved best model (val_loss: 0.00719)
Epoch 077/80 | train: 0.00638 | val: 0.00717 | lr: 1.88e-04
â†’ Saved best model (val_loss: 0.00717)
Epoch 078/80 | train: 0.00639 | val: 0.00718 | lr: 1.88e-04
Epoch 079/80 | train: 0.00638 | val: 0.00723 | lr: 1.88e-04
Epoch 080/80 | train: 0.00636 | val: 0.00717 | lr: 1.88e-04
â†’ Saved best model (val_loss: 0.00717)

---

âœ… Training completed!
Total time: 24.26 minutes
Best val loss: 0.00717
Model saved to: checkpoints/dataset_100000_dl400.pt
======================================================================

======================================================================
XRD CNN MODEL EVALUATION
======================================================================
âœ“ Using MPS (Apple Silicon)
âœ“ Loaded dataset from datasets/dataset_100000_dl400.pkl
X shape: (100000, 7) Y shape: (100000, 650)

ðŸ“¦ Loading model from: checkpoints/dataset_100000_dl400.pt
Checkpoint: epoch 61, val_loss 0.0072700782269239424

ðŸ”® Running predictions on 100000 samples...

======================================================================
ðŸ“Š MAE BY PARAMETER
======================================================================
Parameter MAE (abs) % of range % of mean

---

Dmax1 7.766990e-04 2.77 4.69  
D01 1.435909e-03 4.95 17.63  
L1 1.440649e-06 2.40 2.85  
Rp1 1.317709e-06 1.88 5.36  
D02 2.091594e-03 7.47 18.27  
L2 5.173846e-06 8.62 22.72  
Rp2 1.182109e-05 19.70 -39.25  
======================================================================

# ðŸ“‹ RANDOM PREDICTION EXAMPLES (n=10)

Sample #25370:
True: [0.012500, 0.002500, 6.000e-05, 1.490e-05, 0.022500, 3.500e-05, -4.010e-05]
Predicted: [0.016198, 0.011690, 6.151e-05, 8.696e-06, 0.014385, 1.685e-05, -2.533e-05]
Error: [+3.698e-03, +9.190e-03, +1.514e-06, -6.204e-06, -8.115e-03, -1.815e-05, +1.477e-05]

Sample #80893:
True: [0.020000, 0.012500, 6.500e-05, 2.490e-05, 0.007500, 4.500e-05, -1.010e-05]
Predicted: [0.020940, 0.015387, 6.663e-05, 2.458e-05, 0.006172, 4.247e-05, -1.382e-06]
Error: [+9.402e-04, +2.887e-03, +1.627e-06, -3.168e-07, -1.328e-03, -2.528e-06, +8.718e-06]

Sample #76163:
True: [0.020000, 0.005000, 6.000e-05, 1.990e-05, 0.017500, 3.500e-05, -1.000e-07]
Predicted: [0.021563, 0.012493, 6.096e-05, 1.723e-05, 0.009084, 4.160e-05, -1.016e-05]
Error: [+1.563e-03, +7.493e-03, +9.615e-07, -2.667e-06, -8.416e-03, +6.595e-06, -1.006e-05]

Sample #48384:
True: [0.002500, 0.002500, 6.500e-05, 4.990e-05, 0.007500, 1.500e-05, -1.010e-05]
Predicted: [0.003098, 0.002302, 6.398e-05, 4.360e-05, 0.008981, 2.018e-05, -2.866e-05]
Error: [+5.982e-04, -1.985e-04, -1.018e-06, -6.299e-06, +1.481e-03, +5.179e-06, -1.856e-05]

Sample #43696:
True: [0.025000, 0.015000, 2.000e-05, 9.900e-06, 0.007500, 5.000e-06, -6.010e-05]
Predicted: [0.024084, 0.011572, 2.130e-05, 1.071e-05, 0.004128, 1.770e-05, -3.736e-05]
Error: [-9.162e-04, -3.428e-03, +1.301e-06, +8.103e-07, -3.372e-03, +1.270e-05, +2.274e-05]

Sample #19914:
True: [0.002500, 0.002500, 6.000e-05, 3.990e-05, 0.022500, 5.000e-06, -4.010e-05]
Predicted: [0.002970, 0.002426, 6.314e-05, 3.845e-05, 0.019830, 1.004e-05, -3.244e-05]
Error: [+4.698e-04, -7.361e-05, +3.142e-06, -1.448e-06, -2.670e-03, +5.042e-06, +7.659e-06]

Sample #29410:
True: [0.017500, 0.005000, 6.000e-05, 1.490e-05, 0.022500, 4.500e-05, -4.010e-05]
Predicted: [0.021116, 0.010033, 6.115e-05, 1.210e-05, 0.018891, 3.070e-05, -3.780e-05]
Error: [+3.616e-03, +5.033e-03, +1.146e-06, -2.803e-06, -3.609e-03, -1.430e-05, +2.301e-06]

Sample #64208:
True: [0.025000, 0.022500, 3.500e-05, 9.900e-06, 0.007500, 3.500e-05, -5.010e-05]
Predicted: [0.023061, 0.020981, 3.531e-05, 9.887e-06, 0.008286, 2.852e-05, -3.571e-05]
Error: [-1.939e-03, -1.519e-03, +3.058e-07, -1.338e-08, +7.857e-04, -6.482e-06, +1.439e-05]

Sample #98376:
True: [0.002500, 0.002500, 5.500e-05, 3.990e-05, 0.010000, 5.000e-06, -1.000e-07]
Predicted: [0.003240, 0.002835, 4.979e-05, 3.413e-05, 0.013593, 1.051e-05, -5.402e-07]
Error: [+7.396e-04, +3.350e-04, -5.205e-06, -5.774e-06, +3.593e-03, +5.512e-06, -4.402e-07]

Sample #66672:
True: [0.020000, 0.010000, 6.000e-05, 2.990e-05, 0.020000, 5.000e-06, -2.010e-05]
Predicted: [0.020770, 0.009468, 6.154e-05, 3.090e-05, 0.016652, 1.006e-05, -2.507e-05]
Error: [+7.696e-04, -5.317e-04, +1.544e-06, +1.003e-06, -3.348e-03, +5.061e-06, -4.973e-06]

======================================================================
ðŸ“‹ COPYABLE OUTPUT (for error analysis)
======================================================================
([0.012500, 0.002500, 6.000e-05, 1.490e-05, 0.022500, 3.500e-05, -4.010e-05], [0.016198, 0.011690, 6.151e-05, 8.696e-06, 0.014385, 1.685e-05, -2.533e-05]),
([0.020000, 0.012500, 6.500e-05, 2.490e-05, 0.007500, 4.500e-05, -1.010e-05], [0.020940, 0.015387, 6.663e-05, 2.458e-05, 0.006172, 4.247e-05, -1.382e-06]),
([0.020000, 0.005000, 6.000e-05, 1.990e-05, 0.017500, 3.500e-05, -1.000e-07], [0.021563, 0.012493, 6.096e-05, 1.723e-05, 0.009084, 4.160e-05, -1.016e-05]),
([0.002500, 0.002500, 6.500e-05, 4.990e-05, 0.007500, 1.500e-05, -1.010e-05], [0.003098, 0.002302, 6.398e-05, 4.360e-05, 0.008981, 2.018e-05, -2.866e-05]),
([0.025000, 0.015000, 2.000e-05, 9.900e-06, 0.007500, 5.000e-06, -6.010e-05], [0.024084, 0.011572, 2.130e-05, 1.071e-05, 0.004128, 1.770e-05, -3.736e-05]),
([0.002500, 0.002500, 6.000e-05, 3.990e-05, 0.022500, 5.000e-06, -4.010e-05], [0.002970, 0.002426, 6.314e-05, 3.845e-05, 0.019830, 1.004e-05, -3.244e-05]),
([0.017500, 0.005000, 6.000e-05, 1.490e-05, 0.022500, 4.500e-05, -4.010e-05], [0.021116, 0.010033, 6.115e-05, 1.210e-05, 0.018891, 3.070e-05, -3.780e-05]),
([0.025000, 0.022500, 3.500e-05, 9.900e-06, 0.007500, 3.500e-05, -5.010e-05], [0.023061, 0.020981, 3.531e-05, 9.887e-06, 0.008286, 2.852e-05, -3.571e-05]),
([0.002500, 0.002500, 5.500e-05, 3.990e-05, 0.010000, 5.000e-06, -1.000e-07], [0.003240, 0.002835, 4.979e-05, 3.413e-05, 0.013593, 1.051e-05, -5.402e-07]),
([0.020000, 0.010000, 6.000e-05, 2.990e-05, 0.020000, 5.000e-06, -2.010e-05], [0.020770, 0.009468, 6.154e-05, 3.090e-05, 0.016652, 1.006e-05, -2.507e-05]),

======================================================================
âœ… Evaluation completed!
======================================================================
