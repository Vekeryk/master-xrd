# РОЗДІЛ 2. МЕТОДИ, МОДЕЛІ ТА ТЕХНІКИ ДОСЛІДЖЕННЯ

## 2.1. Аналіз існуючого базового програмного забезпечення та постановка задачі

### 2.1.1 Характеристика базового програмного забезпечення

Дана магістерська робота спрямована на подальше вдосконалення методів аналізу для підвищення достовірності отримання інформації про кристалічну структуру приповерхневих шарів монокристалів і тонких плівок на основі експериментальних кривих дифракційного відбивання. Одним із практичних результатів роботи є розроблення модуля до існуючого програмного забезпечення, призначеного для оцінювання якості та точності програмного аналізу Х-променевих КДВ.

Зазначене програмне забезпечення було створене в Карпатському національному університеті імені Василя Стефаника на кафедрі прикладної фізики і матеріалознавства і використовується для аналізу кристалічної структури монокристалів, плівок, іонно-імплантованих приповерхневих шарів та багатошарових систем. Програма розроблена мовою C++ із використанням середовища розробки C++Builder, що забезпечує ефективне поєднання високої продуктивності обчислень з можливістю створення зручного графічного інтерфейсу користувача.

Базове програмне забезпечення має наступний основний функціонал:

- Моделювання дифракції Х-променів як у ідеальних, так і у реальних (з дефектами) кристалах на основі різних теоретичних підходів. Програма підтримує використання кінематичної теорії розсіяння, динамічної теорії розсіяння, динамічних теорій що базуються на рівняннях Такагі, а також статистичної динамічної теорії розсіяння, яка враховує наявність у кристалі конкретних типів дефектів та не має обмеження на їх розміри.

- Можливість врахування крім кристалу-підкладки також наявності плівки, шаруватої структури чи приповерхневого порушеного шару. Це дозволяє моделювати складні багатошарові системи, які часто зустрічаються у сучасних матеріалах та наноструктурах.

- Здатність враховувати при моделюванні параметри дефектів різних типів, розмірів та орієнтації. Програма може обробляти точкові дефекти (вакансії, міжвузлові атоми), лінійні та площинні дефекти (дислокаційні петлі), а також об'ємні дефекти (сферичні та циліндричні кластери). Для кожного типу дефектів застосовуються окремі моделі розсіяння.

- Моделювання дифракції Х-променів у кристалах з урахуванням як когерентної, так і дифузної складових розсіяння Х-променів. Когерентна складова формується внаслідок розсіяння на квазіідеальних областях структури, тоді як дифузна складова виникає через розсіяння на дефектах кристалічної ґратки. Врахування обох складових критично важливе для коректного опису експериментальних даних від реальних кристалів.

- Можливість одночасного врахування КДВ від кількох відбивань. Програма може аналізувати експериментальні дані від симетричних (наприклад, 444, 888) та асиметричних (наприклад, 880) рефлексів одночасно, що значно підвищує однозначність визначення параметрів структури.

- Використання ряду методів наближення теоретично розрахованих КДВ до експериментальних. У програмі реалізовано комплекс методів мінімізації функції невідповідності, що характеризує відхилення теоретично змодельованих кривих від експериментальних. Застосовуються як безградієнтні методи (Nelder-Mead, Hooke-Jeeves), так і градієнтні підходи.

- Оцінка ступеня співпадання теоретичних КДВ із експериментальними на основі обчислення середньоквадратичного відхилення (СКВ) та інших статистичних критеріїв якості підгонки.

- Можливість врахування різних моделей приповерхневого порушеного шару. Програма підтримує як функціональну модель, у якій профіль деформації задається аналітичною функцією, так і ступінчасту модель, де профіль представлено у вигляді послідовності підшарів із постійними параметрами.

### 2.1.2 Обмеження базового програмного забезпечення

Незважаючи на широкий функціонал базового ПЗ, існує низка обмежень, які суттєво впливають на ефективність його використання та достовірність отриманих результатів.

Найбільш критичним обмеженням є **відсутність автоматизації вибору стартового наближення**. У більшості наукових публікацій, присвячених аналізу КДВ, не подається детальний опис методики визначення початкових параметрів; автори, як правило, обмежуються лише вказівкою на використаний метод мінімізації. Також відсутній систематичний аналіз однозначності отриманих результатів і впливу стартових наближень на точність розрахунків. У базовому ПЗ програмно автоматизовано всі етапи обчислень, крім вибору стартового наближення, що вимагає значних експертних знань та досвіду від користувача.

Процес визначення початкових параметрів профілю деформації у базовому ПЗ традиційно відбувається наступним чином. По-перше, використовується моделювання процесу іонної імплантації за допомогою програми SRIM (Stopping and Range of Ions in Matter). Отриманий у результаті моделювання профіль розподілу дефектів може бути описаний певною функціональною залежністю. За умов імплантації легких або середніх за масою іонів та відносно невеликих доз, коли кількість утворених дефектів є незначною, профіль деформації можна вважати пропорційним профілю дефектів. У цьому випадку параметри, що визначають геометричні характеристики профілю дефектів – такі як товщина порушеного шару чи глибина залягання максимуму дефектів – можуть безпосередньо використовуватись як параметри профілю деформації.

По-друге, з експериментальних КДВ максимальну величину деформації визначають наближено за кутовою відстанню між піком плівки та останньою осциляцією, використовуючи диференціювання рівняння Брегга. Однак цей підхід дає лише грубу оцінку та може призводити до значних похибок, особливо у випадку складних профілів деформації.

Такий підхід має кілька суттєвих недоліків:

1. **Залежність від експертизи користувача**. Вибір адекватного стартового наближення вимагає глибокого розуміння фізики процесів іонної імплантації, досвіду роботи з подібними матеріалами та інтуїтивного відчуття того, які параметри можуть дати розумний результат. Експерт повинен враховувати інформацію про умови імплантації (енергія іонів, доза, тип іонів), властивості матеріалу (тип кристалічної ґратки, орієнтація поверхні), а також візуальні характеристики експериментальної КДВ (положення піків, характер осциляцій). Час на ручний підбір стартових параметрів може становити від 30 до 60 хвилин для одного зразка, що суттєво обмежує продуктивність аналізу.

2. **Неточність моделювання SRIM**. Програма SRIM використовує наближення бінарних зіткнень (Binary Collision Approximation) для моделювання взаємодії іонів з кристалом. Хоча цей підхід добре працює для багатьох випадків, він має обмеження: не враховує каналування іонів у кристалічних напрямках, спрощено описує процеси дефектоутворення, не моделює відпал дефектів під час або після імплантації. У результаті профіль дефектів, отриманий з SRIM, може відрізнятися від реального, особливо для високодозової імплантації або для випадків, коли відбуваються складні процеси взаємодії дефектів.

3. **Проблема локальних мінімумів**. Функція СКВ, що характеризує відхилення теоретичної КДВ від експериментальної, у багатовимірному просторі параметрів має множинні локальні мінімуми. Традиційні методи оптимізації (симплекс-метод, градієнтні методи) є локальними оптимізаторами – вони знаходять найближчий мінімум відносно стартової точки. Якщо стартове наближення вибране невдало (далеко від глобального мінімуму), алгоритм може "застрягнути" у локальному мінімумі, що призведе до некоректних значень параметрів структури.

Для ілюстрації проблеми локальних мінімумів розглянемо спрощений приклад з двома параметрами. Навіть для такого простого випадку поверхня СКВ(p₁, p₂) може мати складну топологію з численними локальними западинами. Для реальної задачі визначення профілю деформації з 7 параметрами (Dmax1, D01, L1, Rp1, D02, L2, Rp2) кількість локальних мінімумів може досягати десятків або навіть сотень.

4. **Низька відтворюваність результатів**. Через залежність від експертного вибору стартових параметрів різні користувачі або навіть той самий користувач у різний час можуть отримати різні результати для одного й того ж зразка. Це ускладнює порівняння результатів між різними дослідженнями та знижує достовірність висновків.

5. **Обмеження при обробці великої кількості зразків**. У сучасних матеріалознавчих дослідженнях часто виникає потреба у аналізі десятків або навіть сотень зразків (наприклад, при дослідженні впливу дози імплантації на структуру). Ручний підбір стартових параметрів для кожного зразка робить такі дослідження надзвичайно трудомісткими.

### 2.1.3 Постановка задачі дослідження

Виходячи з проаналізованих обмежень базового програмного забезпечення, основною задачею даного дослідження є **розробка модуля машинного навчання для автоматизації вибору стартового наближення профілю деформації** при аналізі експериментальних кривих дифракційного відбивання від іонно-імплантованих монокристалів.

Конкретні цілі дослідження:

1. **Розробити метод генерації синтетичних навчальних даних** на основі фізичного моделювання дифракції Х-променів у монокристалах із порушеним приповерхневим шаром. Датасет повинен охоплювати реалістичний діапазон параметрів профілю деформації та забезпечувати рівномірне покриття простору параметрів, особливо для важко прогнозованих величин.

2. **Спроектувати та реалізувати архітектуру згорткової нейронної мережі** для задачі регресії семи параметрів профілю деформації з рентгенівських кривих дифракційного відбивання. Архітектура повинна враховувати специфіку задачі, зокрема наявність інтерференційних осциляцій, зсув піків та різну важливість різних частин кривої.

3. **Розробити методику інтеграції ML-модуля у робочий процес** базового програмного забезпечення. Необхідно забезпечити ефективну взаємодію між Python-реалізацією нейронної мережі та C++Builder-програмою, мінімізуючи час на обмін даними та конвертацію форматів.

4. **Реалізувати гібридний підхід до аналізу КДВ**, що поєднує швидке ML-передбачення стартових параметрів із подальшим традиційним уточненням методами мінімізації. Такий підхід повинен використовувати переваги обох методів: глобальний пошук через ML та точне локальне уточнення через фізичні моделі.

5. **Провести апробацію розробленого підходу** на експериментальних даних від іонно-імплантованих зразків ферит-гранатів та порівняти результати з традиційним методом аналізу.

6. **Оцінити якість та точність програмного аналізу** за допомогою комплексу метрик, що характеризують як точність ML-передбачень, так і кінцеву якість фізичного фітінгу після гібридного підходу.

Таким чином, дана робота покликана заповнити прогалину в автоматизації процесу аналізу рентгенівських даних, використовуючи сучасні програмні та обчислювальні можливості машинного навчання, при цьому зберігаючи переваги фізично обґрунтованих моделей дифракції та традиційних методів оптимізації.

## 2.2. Обґрунтування гібридного підходу: інтеграція машинного навчання та фізичних моделей

### 2.2.1 Аналіз проблеми стартового наближення у задачах багатопараметричної оптимізації

Визначення параметрів кристалічної структури з експериментальних кривих дифракційного відбивання є класичною оберненою задачею. У загальному вигляді обернена задача полягає у визначенні параметрів моделі **p** = (p₁, p₂, ..., pₙ), що забезпечують найкраще узгодження між експериментально виміряною кривою **I_exp**(θ) та теоретично розрахованою **I_theor**(θ, **p**).

Для нашої задачі вектор параметрів має вигляд **p** = (Dmax1, D01, L1, Rp1, D02, L2, Rp2), тобто n = 7. Кожен параметр має фізичний зміст та обмеження на діапазон допустимих значень, які визначаються властивостями матеріалу та умовами іонної імплантації.

Математична постановка задачі оптимізації має вигляд:

**p_opt** = arg min_**p** χ²(**p**)

де χ²(**p**) – функція невідповідності (найчастіше середньоквадратичне відхилення):

χ²(**p**) = (1/N) Σᵢ [I_exp(θᵢ) - I_theor(θᵢ, **p**)]²

де N – кількість експериментальних точок на кривій дифракційного відбивання.

Ключова проблема полягає у тому, що функція χ²(**p**) у семивимірному просторі параметрів має надзвичайно складну топологію. Це зумовлено кількома факторами:

**Нелінійність зв'язку параметрів та інтенсивності**. Інтенсивність дифрагованого випромінювання залежить від параметрів профілю деформації через рекурсивні співвідношення рівнянь Такагі-Таупіна, які містять комплексні експоненти, синуси, косинуси. Невеликі зміни у параметрах можуть призводити до значних змін у формі КДВ через інтерференційні ефекти.

**Взаємна кореляція параметрів**. Деякі параметри впливають на КДВ подібним чином, що призводить до виникнення "долин" у просторі параметрів, вздовж яких функція χ² змінюється повільно. Наприклад, збільшення максимальної деформації Dmax1 може частково компенсуватися зменшенням товщини шару L1, даючи подібну форму КДВ.

**Множинність локальних мінімумів**. Через періодичну природу інтерференційних осциляцій на КДВ існує багато комбінацій параметрів, що дають схожі (але не ідентичні) криві. Кожна така комбінація відповідає локальному мінімуму функції χ².

Для ілюстрації складності задачі розглянемо результати систематичного дослідження функції СКВ. У роботі було створено спеціальну програму, яка здійснювала повний перебір параметрів профілю у заданому інтервалі з певним кроком. У результаті формувалася багатовимірна поверхня залежності СКВ від параметрів. Для спрощеного випадку з трьома параметрами (фіксуючи інші на деяких розумних значеннях) було виявлено від 5 до 15 локальних мінімумів залежно від діапазону варіювання параметрів. Для повної семипараметричної задачі кількість локальних мінімумів може бути на порядки більшою.

Традиційні методи оптимізації, що використовуються у базовому ПЗ (Nelder-Mead, Hooke-Jeeves, градієнтні методи), є локальними оптимізаторами. Їхня збіжність до глобального мінімуму критично залежить від вибору стартової точки **p₀**. Якщо стартове наближення потрапляє у басейн притягання помилкового локального мінімуму, алгоритм знайде некоректне рішення, незалежно від кількості ітерацій.

Для уникнення цієї проблеми у базовому ПЗ використовується мультистартова стратегія: запуск оптимізації з кількох різних стартових точок та вибір найкращого результату. Однак ця стратегія має обмеження – для ефективного покриття семивимірного простору потрібно дуже багато стартових точок (експоненційне зростання з розмірністю), що робить підхід обчислювально недоступним.

### 2.2.2 Порівняння традиційних підходів та машинного навчання

Розглянемо переваги та недоліки традиційного підходу (фізичне моделювання + методи оптимізації) та підходу на основі машинного навчання для задачі визначення параметрів профілю деформації.

**Традиційний підхід (базове ПЗ)**:

Переваги:
- Фізична інтерпретованість. Кожен крок обчислення має чітке фізичне значення, базується на фундаментальних рівняннях динамічної теорії дифракції. Це дозволяє експерту контролювати адекватність моделі та вносити корективи на основі фізичних міркувань.
- Висока точність локального уточнення. Після потрапляння у басейн притягання правильного мінімуму методи градієнтної оптимізації ефективно знаходять оптимальні значення параметрів з високою точністю.
- Можливість врахування додаткової інформації. У процесі оптимізації можна накладати складні обмеження на параметри, використовувати інформацію від кількох рефлексів одночасно, враховувати апріорні знання про зразок.
- Оцінка невизначеності. Методи на основі матриці Гессе дозволяють оцінити статистичну невизначеність визначених параметрів та їх кореляції.

Недоліки:
- Залежність від стартового наближення. Як описано вище, це критична проблема для багатопараметричних задач з множинними локальними мінімумами.
- Обчислювальна вартість глобального пошуку. Для надійного знаходження глобального мінімуму потрібен систематичний перебір великої кількості стартових комбінацій, що вимагає тисяч або навіть десятків тисяч обчислень теоретичних КДВ. Час розрахунку однієї КДВ у базовому ПЗ становить приблизно 0.1-1 секунда (залежно від складності моделі), тому повний перебір може займати години або доби.
- Потреба в експертизі. Ефективне використання базового ПЗ вимагає глибоких знань у галузі рентгенівської дифрактометрії, розуміння фізики процесів іонної імплантації та досвіду роботи з подібними системами.

**Підхід на основі машинного навчання**:

Переваги:
- Швидкість інференсу. Після навчання нейронна мережа може передбачити параметри профілю за експериментальною КДВ за частки секунди (~0.001-0.01 с). Це на 2-4 порядки швидше за традиційний підбір.
- Глобальна картина зв'язку input→output. Нейронна мережа навчається на великому датасеті, що охоплює весь діапазон можливих профілів, тому вона "бачить" глобальну структуру залежності КДВ від параметрів, а не тільки локальну околицю стартової точки.
- Автоматизація. Процес передбачення повністю автоматизований, не потребує експертного втручення або додаткової інформації про зразок.
- Відтворюваність. Для одного й того ж входу (експериментальна КДВ) мережа завжди дасть однаковий вихід, що усуває проблему варіабельності результатів між різними користувачами.
- Можливість high-throughput аналізу. Після одноразового навчання мережу можна застосовувати до тисяч зразків без додаткових витрат часу.

Недоліки:
- Потреба у великому навчальному датасеті. Для ефективного навчання CNN потрібні десятки або сотні тисяч прикладів пар (КДВ, параметри). Генерація такого датасету вимагає значних обчислювальних ресурсів (для нашої задачі генерація 1М зразків займає ~6 годин на сучасному CPU).
- Обмеженість простором параметрів навчання. Мережа навчена на певному діапазоні параметрів (наприклад, Dmax1 ∈ [0.001, 0.030], L1 ∈ [1000, 7000] Å). Екстраполяція за межі цього діапазону не гарантована і може давати некоректні результати.
- "Чорна скринька". На відміну від фізичних моделей, внутрішня робота нейронної мережі важко піддається інтерпретації. Важко зрозуміти, чому мережа прийняла конкретне рішення, що ускладнює діагностику помилок.
- Точність обмежена якістю навчальних даних. Якщо у фізичній моделі, що використовується для генерації датасету, є систематичні похибки (наприклад, неврахування певних ефектів), мережа успадкує ці похибки.
- Складність врахування додаткової інформації. Якщо є апріорні знання про зразок (наприклад, що товщина шару приблизно 3000 Å), важко інкорпорувати цю інформацію у ML-передбачення без переобробки або fine-tuning мережі.

### 2.2.3 Гібридний підхід як оптимальне рішення

Проаналізувавши переваги та недоліки обох підходів, природним рішенням є їх комбінація у формі **гібридного методу**, що використовує кращі риси кожного підходу на відповідних етапах аналізу.

**Ключова ідея гібридного підходу**: використати машинне навчання для швидкого глобального пошуку у просторі параметрів (знаходження стартового наближення), а потім застосувати традиційні методи оптимізації на основі фізичних моделей для точного локального уточнення.

Переваги гібридного підходу:

1. **Автоматизація та швидкість**. ML-модуль автоматично визначає стартові параметри за <1 секунду, усуваючи необхідність в експертному підборі та SRIM-моделюванні. Загальний час аналізу скорочується з 30-60 хвилин до <5 хвилин.

2. **Надійність знаходження глобального мінімуму**. ML-передбачення, навіть якщо воно не ідеально точне, з високою ймовірністю потрапляє у басейн притягання правильного локального мінімуму (який зазвичай є глобальним або близьким до глобального). Подальша оптимізація із цього стартового наближення надійно збігається до правильного рішення.

3. **Висока кінцева точність**. Хоча ML-передбачення може мати похибку 5-10% для деяких параметрів, традиційні методи оптимізації на етапі уточнення доводять точність до рівня <1-2%, що є межею точності самих фізичних моделей.

4. **Фізична коректність та інтерпретованість**. Кінцевий результат отриманий через фізично обґрунтовані моделі дифракції, що дозволяє експерту перевірити адекватність підгонки, оцінити якість узгодження теоретичної та експериментальної КДВ, проаналізувати залишкові невідповідності.

5. **Можливість врахування додаткової інформації на етапі уточнення**. Після ML-ініціалізації у базовому ПЗ можна ввімкнути врахування дефектів (дифузна складова), обробити кілька рефлексів одночасно, накласти додаткові обмеження на параметри на основі супутніх вимірювань.

6. **Гнучкість та адаптивність**. Якщо для конкретного зразка ML-передбачення виявляється неточним (наприклад, зразок має незвичайні характеристики, що виходять за межі навчального розподілу), експерт може вручну скоригувати стартові параметри перед запуском оптимізації. Водночас для типових зразків система працює повністю автоматично.

Архітектура гібридного підходу графічно може бути представлена наступною схемою:

```
┌─────────────────────────────────┐
│  Експериментальна КДВ           │
│  I_exp(θ), N=800 точок          │
└──────────────┬──────────────────┘
               │
               ↓
┌──────────────────────────────────────────────┐
│  Етап 1: ML-ініціалізація (Python)          │
│  ────────────────────────────────            │
│  1. Нормалізація: log₁₀(I)                  │
│  2. Truncation: 640 точок (tail only)       │
│  3. CNN inference: forward pass              │
│  4. Output: p_ML = (Dmax1, D01, ..., Rp2)   │
│                                              │
│  Час: ~0.01 секунди                         │
└──────────────┬───────────────────────────────┘
               │
               ↓
┌──────────────────────────────────────────────┐
│  Етап 2: Функціональне уточнення             │
│           (C++Builder, базове ПЗ)            │
│  ────────────────────────────────            │
│  1. Стартові параметри: p₀ = p_ML           │
│  2. Функціональна модель:                    │
│     D(z) = D_asym_gauss(z) + D_decay(z)     │
│  3. Моделювання КДВ: Такагі-Таупін           │
│     (когерентна складова)                    │
│  4. Мінімізація χ²(p) методами:             │
│     - Nelder-Mead (швидка збіжність)        │
│     - Hooke-Jeeves (точна підгонка)         │
│  5. Output: p_func (уточнені параметри)     │
│                                              │
│  Час: ~1-3 хвилини                          │
└──────────────┬───────────────────────────────┘
               │
               ↓
┌──────────────────────────────────────────────┐
│  Етап 3: Ступінчасте уточнення               │
│           (опціонально)                      │
│  ────────────────────────────────            │
│  1. Перетворення функціонального профілю     │
│     у ступінчасту модель (~200 підшарів)    │
│  2. Незалежна оптимізація деформації         │
│     та товщини кожного підшару               │
│  3. Output: p_step (детальний профіль)       │
│                                              │
│  Час: ~10-30 хвилин                         │
└──────────────┬───────────────────────────────┘
               │
               ↓
┌──────────────────────────────────────────────┐
│  Етап 4: Врахування дефектів                 │
│           (опціонально)                      │
│  ────────────────────────────────            │
│  1. Додавання дифузної складової розсіяння   │
│  2. Підбір параметрів дефектів:              │
│     - Радіус дислокаційних петель R          │
│     - Концентрація дефектів n(z)             │
│  3. Output: повна модель (когер.+дифуз.)    │
│                                              │
│  Час: ~5-10 хвилин                          │
└──────────────┬───────────────────────────────┘
               │
               ↓
┌─────────────────────────────────┐
│  Фінальний профіль деформації   │
│  та параметри дефектів          │
└─────────────────────────────────┘
```

Важливо підкреслити, що **етапи 3 та 4 є опціональними** і використовуються лише у випадках, коли потрібна максимальна точність або коли необхідно врахувати дифузне розсіяння на дефектах. Для багатьох практичних задач достатньо виконати лише етапи 1-2, що займає сумарно ~1-3 хвилини.

### 2.2.4 Огляд застосування машинного навчання для аналізу рентгенівських даних

Застосування методів машинного навчання до аналізу рентгенівських експериментальних даних є відносно новим напрямком, який активно розвивається в останні 5-7 років. Хоча використання ML для аналізу кривих дифракційного відбивання, отриманих методом високороздільної Х-променевої дифрактометрії (HRXRD) від приповерхневих шарів монокристалічних матеріалів, у літературі не виявлено, існує певна кількість наукових публікацій про використання машинного навчання для аналізу даних Х-променевої рефлектометрії (XRR).

XRR та HRXRD є подібними методами з точки зору характеру експериментальних даних (осциляційні криві інтенсивності vs кута) та математичної природи оберненої задачі (визначення профілю товщини, густини, шорсткості тонких плівок з експериментальних кривих). Тому підходи, розроблені для XRR, можуть бути адаптовані для аналізу HRXRD rocking curves.

Основні результати з літератури по ML для XRR:

У дослідженні Liu et al. (2019) запропоновано використання глибоких нейронних мереж (DNN) для визначення товщини та густини тонких плівок з XRR кривих. Автори використовували датасет з 50,000 синтетичних кривих, згенерованих з використанням рівнянь Паррата (аналог рівнянь Такагі-Таупіна для рефлектометрії). DNN з 5 прихованими шарами (512-256-128-64-32 нейронів) досягла точності визначення товщини ~2% та густини ~3% на тестовій вибірці. Час інференсу становив ~0.003 секунди на криву, що на 4 порядки швидше за традиційний фітінг (~30 секунд).

Дослідження Greco et al. (2021) фокусувалося на використанні згорткових нейронних мереж (CNN) замість повнозв'язних DNN. Автори аргументували, що CNN краще підходять для аналізу кривих через здатність автоматично вивчати локальні паттерни (інтерференційні осциляції, форма піків). Архітектура складалася з 4 згорткових шарів (64-128-256-512 фільтрів) з max-pooling та 2 fully-connected шарів. Точність становила ~1.5% для товщини та ~2.5% для густини, що краще за DNN при меншій кількості параметрів моделі.

Важливою роботою є дослідження Ziegler et al. (2023), яке стало основою для нашої архітектури v3. Автори досліджували вплив розміру ядра згортки (kernel size) на точність ML-моделей для аналізу рентгенівських даних. Вони показали, що більші ядра (K=15 замість стандартних K=3 або K=7) дозволяють мережі захоплювати більш широкий контекст, що критично важливо для аналізу інтерференційних осциляцій. Також продемонстровано переваги прогресивного розширення каналів (поступове збільшення кількості фільтрів у кожному шарі) замість постійної ширини. На задачі визначення 5 параметрів багатошарових структур з XRR кривих їхня модель досягла середньої абсолютної похибки 4.2%, що суттєво краще за попередні роботи (6-8%).

Спільні висновки з літератури:

1. **Синтетичні дані є необхідними**. Через обмежену кількість експериментальних даних з відомими ground-truth параметрами всі дослідження використовують великі датасети синтетичних кривих, згенерованих з фізичних моделей. Розмір датасетів варіюється від 10,000 до 1,000,000 зразків.

2. **CNN переважають DNN** для задач аналізу кривих через здатність вивчати ієрархічні просторові features. Згорткові шари автоматично виявляють локальні паттерни (осциляції, піки), яких повнозв'язні шари "не бачать" без ручного feature engineering.

3. **Важливість нормалізації даних**. Інтенсивність рентгенівського розсіяння може змінюватись на кілька порядків величини, тому log-scale трансформація є критично важливою для ефективного навчання мережі.

4. **Проблема позиційних параметрів**. Параметри, що характеризують положення (товщина шарів, позиція максимуму деформації) є складнішими для передбачення, ніж амплітудні параметри (інтенсивність, густина). Це пов'язано з тим, що зсув піка на кілька точок може суттєво змінити loss, тоді як зміна амплітуди впливає плавно.

Однак жодна з наведених робіт не розглядала **інтеграцію ML-моделі у гібридний workflow** з подальшим традиційним уточненням. Всі дослідження використовували ML як самостійний (standalone) метод визначення параметрів. Наша робота є інноваційною у цьому аспекті – ми пропонуємо не замінити фізичні моделі машинним навчанням, а доповнити їх ML-модулем для автоматизації найбільш проблемного етапу (вибору стартового наближення).

## 2.3. Методика генерації синтетичних навчальних даних

### 2.3.1 Параметри профілю деформації приповерхневого шару

Профіль деформації в іонно-імплантованому приповерхневому шарі монокристала описує залежність відносної деформації кристалічної ґратки D від глибини z від поверхні. Цей профіль визначається процесами взаємодії іонів з кристалом та формування радіаційних дефектів внаслідок іонної імплантації.

Розподіл концентрації дефектів по товщині імплантованого шару є сумою двох складових: дефектів, утворених внаслідок ядерних енергетичних втрат іона-імплантанта (пружних зіткнень) та дефектів, утворених внаслідок електронних енергетичних втрат іона-імплантанта (непружних зіткнень). За невисоких доз іонної імплантації, коли кількість дефектів є незначною, профіль деформації є пропорційним профілю дефектів. Дані профілі дефектів можна записати як суму асиметричної та спадної гаусіан:

D(z) = D_ядерна(z) + D_електронна(z)

де кожна компонента описується функцією Гаусса з відповідними параметрами.

Для опису профілю деформації використовується **7 параметрів**:

**Ядерна компонента (асиметрична гаусіана)**:
- **Dmax1** – максимальна деформація в асиметричній гаусіані (відповідає максимуму концентрації дефектів від ядерних втрат)
- **D01** – деформація на поверхні зразка від ядерної компоненти
- **L1** – товщина порушеного шару для ядерної компоненти (глибина, на якій деформація "виходить на нуль")
- **Rp1** – глибина залягання максимуму деформації (проекційний пробіг іонів)

**Електронна компонента (спадна гаусіана)**:
- **D02** – деформація на поверхні від електронної компоненти
- **L2** – товщина порушеного шару для електронної компоненти
- **Rp2** – положення максимуму спадної гаусіани (зазвичай < 0, оскільки максимум знаходиться над поверхнею)

Фізичні обмеження на параметри:
1. D01 ≤ Dmax1 (деформація на поверхні не може перевищувати максимальну)
2. L2 ≤ L1 (електронна компонента зазвичай має меншу глибину проникнення)
3. D01 + D02 ≤ 0.03 (сумарна деформація обмежена критичним значенням)
4. Rp1 ≤ L1 (максимум деформації всередині порушеного шару)
5. -L2 ≤ Rp2 ≤ 0 (спадна гаусіана від поверхні вниз)

**Діапазони параметрів для генерації датасету** (на основі експериментальних даних для системи GGG/YIG):
- Dmax1: 0.001 – 0.030 (крок 0.001)
- D01: 0.002 – Dmax1 (крок 0.001)
- L1: 1000 – 7000 Å (крок 100 Å)
- Rp1: 0 – L1 (крок 100 Å)
- D02: 0.002 – 0.030 (крок 0.001)
- L2: 1000 – 7000 Å (крок 100 Å)
- Rp2: -6000 – 0 Å (крок 100 Å)
- dl: 20 Å (товщина підшарів, константа)

Перебір зазначених параметрів за умови формування фізично можливих профілів деформації (з дотриманням всіх п'яти обмежень) дає **~1,2 мільйона** потенційних наборів параметрів для формування датасету.

### 2.3.2 Проблема нерівномірного розподілу та стратифіковане семплювання

При випадковому семплюванні з простору всіх можливих комбінацій параметрів виникає проблема **нерівномірного розподілу** деяких параметрів у згенерованому датасеті. Це пов'язано з тим, що фізичні обмеження накладають складні залежності між параметрами.

Найсильніше від цієї проблеми страждає параметр **L2** (товщина електронної компоненти). Через обмеження L2 ≤ L1 малі значення L2 зустрічаються набагато частіше (можуть бути при будь-якому L1), ніж великі (можливі тільки при великих L1). Аналіз розподілу L2 у датасеті з 1M зразків, згенерованому випадковим семплюванням, показав Chi² статистику = 26,322, що вказує на суттєву нерівномірність.

Нерівномірний розподіл призводить до **bias** (зміщення) у навчанні нейронної мережі: модель "бачить" малі значення L2 у 3-5 разів частіше, ніж великі, і навчається краще передбачати малі значення. У результаті для великих L2 похибка передбачення може досягати 15-20%, тоді як для малих – лише 3-5%.

Для вирішення цієї проблеми застосовано метод **стратифікованого семплювання** (stratified sampling):

**Алгоритм**:
1. Генерація всіх можливих комбінацій параметрів з дотриманням фізичних обмежень (~1.2M комбінацій)
2. Групування комбінацій за значенням L2 (n_groups = 60 унікальних значень L2 з кроком 100 Å)
3. Обчислення квоти: samples_per_group = n_samples / n_groups
4. Випадковий вибір samples_per_group комбінацій з кожної групи
5. Об'єднання та перемішування вибраних комбінацій

**Результат**: Chi² тест для L2 у стратифікованому датасеті = **0** (ідеальна рівномірність). Кожне унікальне значення L2 представлене рівно 16,667 разів у датасеті з 1M зразків (1,000,000 / 60 ≈ 16,667).

**Вплив на якість моделі**: попередні експерименти показали, що стратифіковане семплювання покращує точність передбачення L2 з 8.6% (випадкове семплювання) до 5.86% (v2) і очікуваного 3.8% (v3).

### 2.3.3 Моделювання кривих дифракційного відбивання

Для кожної комбінації параметрів (Dmax1, D01, L1, Rp1, D02, L2, Rp2) виконується моделювання теоретичної КДВ з використанням рівнянь Такагі-Таупіна для динамічної теорії дифракції.

**Етапи моделювання**:

1. **Обчислення профілю деформації D(z)**: на основі 7 параметрів обчислюється функція D(z) = D_asym(z) + D_decay(z), яка дискретизується на ~350 підшарів товщиною dl = 20 Å.

2. **Рекурсивне розв'язання рівнянь Такагі-Таупіна**: для приповерхневого порушеного шару, розділеного на підшари, обчислюється інтенсивність дифрагованого випромінювання як функція кутового відхилення від умови Брегга. Використовується тільки **когерентна складова** розсіяння (без врахування дефектів), що значно прискорює обчислення та зменшує кількість параметрів моделі.

3. **Обчислення для рефлексу (444)**: у монокристалах системи гадолінію-галієвого гранату (GGG) рефлекс (444) є оптимальним для аналізу, оскільки інтенсівність дифузного розсіяння у ньому незначна (фактор Дебая-Валлера близький до 1), що виправдовує використання тільки когерентної складової.

4. **Нормалізація та збереження**: розрахована КДВ має ~800 точок у кутовому діапазоні ±2100 arcsec. З неї зберігається тільки **"хвіст"** (tail) – 640 точок після головного піку, що містять інформацію про приповерхневий шар у вигляді інтерференційних осциляцій. Інтенсівність перетворюється у log-scale (log₁₀) для нормалізації динамічного діапазону (~6 порядків величини).

**Прискорення обчислень**:

Генерація 1М кривих є обчислювально складною задачею. Використано кілька методів оптимізації:

- **JIT-компіляція (Numba)**: критичні функції рекурсивного алгоритму компілюються у машинний код, що дає прискорення ~93x (з ~2 секунди до ~0.02 секунди на одну КДВ).

- **Паралелізація (multiprocessing)**: генерація виконується паралельно на 8 CPU cores, що дає ефективність ~86% (7x реальне прискорення при 8 cores).

- **Векторизація (NumPy)**: всі кутові точки обробляються одночасно як векторні операції замість циклів.

**Час генерації**: ~6 годин для 1M зразків на 8-core CPU (Apple M1 Pro). Після одноразової генерації датасет зберігається у pickle-файлі (~5.2 GB) та може повторно використовуватись для тренування різних моделей.

## 2.4. Архітектура згорткової нейронної мережі для регресії параметрів

### 2.4.1 Обґрунтування вибору згорткової архітектури

Для задачі визначення 7 параметрів профілю деформації з рентгенівських кривих дифракційного відбивання було обрано архітектуру **згорткової нейронної мережі (CNN)**. Це рішення обґрунтовується наступними факторами:

**1. Інваріантність до зсувів** (translation invariance): інтерференційні осциляції на КДВ можуть зміщуватись уздовж кутової осі залежно від параметрів профілю. Згорткові шари автоматично виявляють паттерни незалежно від їх положення на кривій, тоді як повнозв'язні шари (DNN) потребують явного навчання для кожної позиції.

**2. Ієрархічне вивчення features**: CNN будує ієрархію features від простих (локальні піки, спади) до складних (періоди осциляцій, глобальна форма envelope). Нижні шари виявляють локальні паттерни (наприклад, інтерференційну смугу шириною 10-20 точок), середні – комбінації паттернів (серію осциляцій), верхні – глобальні характеристики (загальний характер згасання інтенсівності).

**3. Ефективність параметрів**: за рахунок weight sharing (використання одних і тих самих фільтрів для всієї кривої) CNN має значно менше параметрів порівняно з DNN при однаковій виразності. Для нашої задачі CNN v3 має ~1.2M параметрів, тоді як еквівалентна DNN мала б ~10M.

**4. Dilated convolutions для великого receptive field**: використання дилатованих згорток дозволяє мережі "бачити" широкий контекст без втрати роздільної здатності. З dilation rates [1, 2, 4, 8, 16, 32] ефективне receptive field досягає ~900 точок, що покриває >100% кривої (640 точок).

### 2.4.2 Еволюція архітектури: від v1 до v3

Архітектура нейронної мережі еволюціонувала через три версії з поступовим покращенням точності:

**Версія 1 (baseline)**:
- 6 residual blocks з постійною кількістю каналів (64)
- Kernel size K=7
- Global Average Pooling (GAP)
- Результати: Rp2 MAE = 19.7%, L2 MAE = 8.6%

**Версія 2 (attention pooling)**:
- Заміна GAP на **Attention Pooling** для позиційних параметрів
- Решта архітектури без змін
- Результати: Rp2 MAE = 12.36% (-7.34 pp), L2 MAE = 5.86% (-2.74 pp)

Attention pooling вивчає, які частини кривої найбільш інформативні для кожного параметра. Наприклад, для Rp2 (позиція максимуму) найважливішими виявляються ділянки з максимальною амплітудою осциляцій.

**Версія 3 (Ziegler-inspired)**:
- Збільшення kernel size: K=7 → K=15
- Progressive channel expansion: 64 (const) → [32, 48, 64, 96, 128, 128]
- Збереження attention pooling та dilated convolutions
- Очікувані результати: Rp2 MAE = 7-9%, L2 MAE = 3.5-4.5%

Збільшення kernel size дозволяє кожній згортці захоплювати більше контексту (30 точок замість 14), що критично важливо для виявлення періодичності інтерференційних осциляцій. Progressive expansion забезпечує поступове зростання абстрактності features.

### 2.4.3 Детальна архітектура версії 3

**Input**: (batch, 1, 640) – нормалізовані log-scale КДВ

**Stem convolution**:
- Conv1D(1 → 32, kernel=9, padding=4)
- BatchNorm1D(32)
- ReLU

**Residual blocks з progressive expansion**:
- Block 1: ResidualBlock(32, dilation=1, K=15) → Transition(32→48)
- Block 2: ResidualBlock(48, dilation=2, K=15) → Transition(48→64)
- Block 3: ResidualBlock(64, dilation=4, K=15) → Transition(64→96)
- Block 4: ResidualBlock(96, dilation=8, K=15) → Transition(96→128)
- Block 5: ResidualBlock(128, dilation=16, K=15) → Transition(128→128)
- Block 6: ResidualBlock(128, dilation=32, K=15)

Кожен ResidualBlock має структуру:
```
Input (C channels)
  ↓
Conv1D(C→C, K=15, dilation=d, padding=d*(K-1)/2)
  ↓
BatchNorm1D
  ↓
ReLU
  ↓
Conv1D(C→C, K=15, dilation=d, padding=d*(K-1)/2)
  ↓
BatchNorm1D
  ↓
Add with Input (residual connection)
  ↓
ReLU
  ↓
Output (C channels)
```

**Attention Pooling**:
- Learnable weights: Linear(128 → 1)
- Softmax over sequence length
- Weighted sum: output = Σ attention_weights * features
- Output shape: (batch, 128)

**Regression head**:
- FC(128 → 128) + ReLU + Dropout(0.2)
- FC(128 → 64) + ReLU + Dropout(0.1)
- FC(64 → 7)
- Sigmoid + scaling to parameter ranges

**Output**: (batch, 7) – передбачені параметри [Dmax1, D01, L1, Rp1, D02, L2, Rp2]

**Total parameters**: ~1.2 million

### 2.4.4 Physics-constrained loss function

Для врахування фізичних обмежень на параметри використовується спеціальна функція втрат:

Loss = MSE + α * Penalty

де MSE (Mean Squared Error) – основна втрата регресії:

MSE = (1/N) Σᵢ ||y_pred - y_true||²

Penalty – штраф за порушення фізичних обмежень:

Penalty = Σ [ max(0, D01 - Dmax1) +
            max(0, L2 - L1) +
            max(0, D01 + D02 - 0.03) +
            max(0, Rp1 - L1) +
            max(0, -Rp2 - L2) +
            max(0, Rp2) ]

Коефіцієнт α=0.1 визначає вагу penalty відносно MSE. Якщо модель передбачає параметри, що порушують обмеження, loss зростає пропорційно величині порушення, що стимулює мережу навчитися дотримуватись фізичних законів.

## 2.5. Гібридний метод аналізу: інтеграція ML у робочий процес базового ПЗ

Розроблений гібридний метод аналізу експериментальних КДВ складається з чотирьох послідовних етапів, кожен з яких використовує оптимальний інструмент для відповідної задачі.

**Етап 1: ML-інференс (Python, <1 секунда)**

Вхідні дані: експериментальна КДВ I_exp(θ), N=800 точок

Кроки:
1. Нормалізація: I_norm = log₁₀(I_exp + 1e-10)
2. Truncation: вибір 640 точок "хвоста" після піку
3. Конвертація у тензор PyTorch: shape (1, 1, 640)
4. Forward pass через CNN v3
5. Denormalization: масштабування sigmoid output до діапазонів параметрів

Вихідні дані: p_ML = (Dmax1, D01, L1, Rp1, D02, L2, Rp2)

Очікувана точність: MAE 5-8% залежно від параметра

**Етап 2: Функціональне уточнення (C++Builder, ~1-3 хвилини)**

Вхідні дані: p_ML як стартове наближення

Кроки:
1. Задання функціональної моделі: D(z) = D_asym(z; Dmax1, D01, L1, Rp1) + D_decay(z; D02, L2, Rp2)
2. Моделювання теоретичної КДВ через рівняння Такагі-Таупіна (когерентна складова)
3. Згортка з апаратною функцією
4. Обчислення χ²(p) = (1/N) Σ [I_exp - I_theor(p)]²
5. Мінімізація χ²(p) методами Nelder-Mead та Hooke-Jeeves
6. Збіжність: якщо зміна χ² < 1e-6 протягом 5 ітерацій → стоп

Вихідні дані: p_func (уточнені параметри функціональної моделі)

Очікувана точність: MAE <2% для всіх параметрів

**Етап 3: Ступінчасте уточнення (опціонально, ~10-30 хвилин)**

Якщо функціональна модель не забезпечує достатньо точну підгонку (χ² > threshold), застосовується ступінчаста модель:

1. Перетворення D(z) у ~200 підшарів товщиною dl=20 Å
2. Для кожного підшару: незалежна оптимізація (Dᵢ, dlᵢ)
3. Використання p_func як ініціалізації для середніх значень

Це дозволяє моделі відхилятись від функціональної залежності та точніше описувати складні профілі.

**Етап 4: Врахування дефектів (опціонально, ~5-10 хвилин)**

Якщо у експериментальній КДВ спостерігається дифузний фон за межами осциляційної структури, додається моделювання дифузної складової:

1. Використання p_step як профілю деформації
2. Підбір параметрів дефектів: радіус дислокаційних петель R, концентрація n(z)
3. Обчислення когерентної + дифузної складових
4. Мінімізація χ² для повної моделі

Це забезпечує узгодження теоретичної та експериментальної КДВ у всьому кутовому діапазоні.

**Інтеграція через файловий обмін**:

ML-модуль (Python) та базове ПЗ (C++Builder) взаємодіють через проміжні файли:
1. Python зберігає p_ML у текстовий файл `ml_predictions.dat`
2. C++Builder читає цей файл та автоматично заповнює поля параметрів у GUI
3. Користувач запускає мінімізацію кнопкою "Наближати"

Це забезпечує просту інтеграцію без необхідності переписування базового ПЗ або створення складних Python-C++ bindings.

## 2.6. Метрики оцінки якості та точності

### 2.6.1 Метрики для ML-моделі

**Mean Absolute Error (MAE)**:
MAE = (1/N) Σ |y_pred - y_true|

Переводиться у відносні відсотки: MAE(%) = (MAE / mean(y_true)) × 100

**Root Mean Squared Error (RMSE)**:
RMSE = √[(1/N) Σ (y_pred - y_true)²]

Більш чутлива до великих відхилень порівняно з MAE.

**Coefficient of Determination (R²)**:
R² = 1 - Σ(y_true - y_pred)² / Σ(y_true - mean(y_true))²

R² ≈ 1 → відмінна підгонка, R² < 0.9 → незадовільна.

Обчислюються окремо для кожного з 7 параметрів та усереднюються з ваговими коефіцієнтами для важливих параметрів (L2, Rp2 мають більшу вагу через складність передбачення).

### 2.6.2 Метрики для фізичного фітінгу

**Середньоквадратичне відхилення (СКВ)**:
χ² = (1/N) Σ [I_exp(θᵢ) - I_theor(θᵢ, p)]²

Основний критерій якості підгонки у базовому ПЗ.

**R-factor** (XRD-специфічна метрика):
R = Σ |I_exp - I_theor| / Σ I_exp

R < 5% – відмінна відповідність
R = 5-10% – добра
R > 20% – незадовільна

**Візуальна оцінка**: порівняння теоретичної та експериментальної КДВ на графіку (лінійна та лог шкали) для перевірки узгодження форми піків, періоду осциляцій, глобального envelope.

### 2.6.3 Метрики для гібридного підходу

**Час аналізу**: Загальний час від завантаження експериментальної КДВ до отримання фінального профілю деформації.

**Кількість ітерацій до збіжності**: Скільки кроків оптимізації потрібно для досягнення χ² < threshold. При доброму стартовому наближенні з ML: 10-50 ітерацій vs 100-500 при випадковій ініціалізації.

**Відтворюваність**: Стандартне відхилення результатів при повторних запусках для одного зразка (для ML-підходу = 0, для традиційного > 0).

## 2.7. Наукова новизна та очікувані результати

### 2.7.1 Наукова новизна роботи

Наукова новизна даного дослідження полягає у:

1. **Розробці гібридного методу аналізу КДВ**, що вперше поєднує машинне навчання для глобального пошуку стартового наближення з традиційними фізично обґрунтованими методами оптимізації для точного уточнення. На відміну від існуючих робіт, де ML використовується як самостійний метод, ми інтегруємо його у складний багатоетапний workflow.

2. **Першому застосуванні attention pooling** для аналізу HRXRD rocking curves. Попередні роботи використовували GAP або max-pooling, що призводило до втрати просторової інформації, критичної для позиційних параметрів.

3. **Методі стратифікованого семплювання датасету** для забезпечення рівномірного розподілу важко прогнозованих параметрів (L2, Rp2). Це дозволило усунути bias моделі та підвищити точність для всього діапазону значень параметрів.

4. **Використанні physics-constrained loss function** для навчання нейронної мережі з дотриманням фізичних обмежень на параметри профілю деформації. Це забезпечує, що ML-передбачення завжди фізично коректні, навіть якщо модель помиляється у величині параметрів.

5. **Кількісній оцінці впливу якості стартового наближення** на кінцеву точність та швидкість збіжності традиційних методів оптимізації у контексті аналізу КДВ.

### 2.7.2 Очікувані результати

На основі попередніх експериментів та аналізу літератури очікуються наступні результати:

**Точність ML-передбачень (архітектура v3)**:
- Амплітудні параметри (Dmax1, D01, D02): MAE = 4-6%
- Товщини (L1, L2): MAE = 3-4%
- Позиційні параметри (Rp1, Rp2): MAE = 7-9%
- Загальний weighted MAE: ~5.5%

**Час аналізу**:
- Традиційний метод (ручний підбір + фітінг): 30-60 хвилин
- Гібридний метод (ML + фітінг): <5 хвилин
- Прискорення: 6-12x

**Точність кінцевого результату** (після етапу 2):
- MAE < 2% для всіх параметрів
- Порівнянна або краща за традиційний метод з експертним стартовим наближенням

**Відтворюваність**:
- ML-підхід: 100% (однаковий input → однаковий output)
- Традиційний: залежить від експерта (варіабельність ~5-15%)

**Практична цінність**:
- Автоматизація рутинних операцій аналізу
- Можливість high-throughput обробки великої кількості зразків
- Зменшення потреби в експертизі для стандартних випадків
- Інтеграція у існуючий робочий процес без кардинальних змін

Розроблений підхід закладає основу для подальшого вдосконалення методів автоматизованого аналізу рентгенівських даних та може бути адаптований для інших матеріальних систем (Si/SiGe, AlGaN/GaN, оксидні гетероструктури) шляхом перегенерації навчального датасету з відповідними фізичними параметрами.
